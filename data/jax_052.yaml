- en: Named axes and easy-to-revise parallelism with xmap
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名轴和 `xmap` 轻松修改并行处理策略
- en: 原文：[`jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html`](https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html`](https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html)
- en: '***UPDATE:*** `xmap` is deprecated and will be removed in a future release.
    The recommended ways to do multi-device programming in JAX are using: 1) [`jit`
    (automatic partitioning of computation and `jax.Array` sharding)](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html);
    and/or 2) [`shard_map` (manual data sharding)](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html).
    Learn more in [Why don’t `pmap` or `xmap` already solve this?](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html#why-don-t-pmap-or-xmap-already-solve-this)
    in the [`shard_map` JEP document](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '***更新：*** `xmap` 已弃用，并将在未来版本中删除。在 JAX 中执行多设备编程的推荐方法是使用：1) [`jit`（计算自动分区和 `jax.Array`
    分片）](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html);
    和/或 2) [`shard_map`（手动数据分片）](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html)。详细了解请参阅[`shard_map`
    JEP 文档中的“为什么 `pmap` 或 `xmap` 不能解决此问题？”](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html#why-don-t-pmap-or-xmap-already-solve-this)。'
- en: This tutorial introduces JAX `xmap` (`jax.experimental.maps.xmap`) and the named-axis
    programming model that comes with it. By reading this, you’ll learn how to write
    error-avoiding, self-documenting functions using named axes, then control how
    they’re executed on hardware at any scale, from your laptop CPU to the largest
    TPU supercomputer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程介绍了 JAX `xmap` (`jax.experimental.maps.xmap`) 和随附的命名轴编程模型。通过阅读本教程，您将学习如何使用命名轴编写避免错误、自描述的函数，然后控制它们在从笔记本电脑
    CPU 到最大 TPU 超级计算机的任何规模的硬件上执行的方式。
- en: We start with a toy neural network example.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个玩具神经网络的例子开始。
- en: From positions to names in a toy neural network
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从玩具神经网络中的位置到名称
- en: 'Presentations on JAX often start with a simple neural network prediction function
    and loss, written in pure NumPy. Here’s a simple network with one hidden layer:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 的演示通常从纯 NumPy 编写的简单神经网络预测函数和损失开始。这是一个具有一个隐藏层的简单网络：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can then initialize inputs with the right shapes and compute the loss value:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以用正确的形状初始化输入并计算损失值：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here’s how we might write the same function using named axes. Don’t worry if
    you can’t follow the API details. They are not important now and we will explain
    everything step-by-step afterwards. This is just to show you what you can do with
    xmap before you learn them!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何使用命名轴写相同函数的方式。如果您无法理解 API 的细节，请不要担心。现在这些不重要，我们将逐步解释一切。这只是为了展示在学习之前您可以使用
    xmap 做些什么！
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code is simpler: we don’t need to worry about axis order when calling
    functions like `jnp.dot`, or remember which axis position to reduce over with
    `logsumexp`, `jnp.sum`, or `jnp.mean`.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码更简单：我们在调用 `jnp.dot` 等函数时不需要担心轴的顺序，也不需要记住使用 `logsumexp`、`jnp.sum` 或 `jnp.mean`
    时要减少哪个轴位置。
- en: 'But the real win is that names let us use `xmap` to control our function’s
    execution. At its simplest, `xmap` will just vectorize over all named axes, so
    that the function is executed just like its positional-axis counterpart:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但真正的优势在于，名称使我们可以使用 `xmap` 控制函数的执行。最简单的情况下，`xmap` 将仅在所有命名轴上向量化，使函数的执行方式与其位置轴的对应方式相同：
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'But on a whim we can decide to parallelize over the batch axis:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以随心所欲地决定在批处理轴上进行并行处理：
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Or we might want to perform model parallelism over the hidden axis:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可能希望在隐藏轴上执行模型并行处理：
- en: '[PRE6]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Or we might want to do both model and batch data parallelism at once:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可能希望同时进行模型和批处理数据的并行处理：
- en: '[PRE7]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With `xmap`, we can revise our parallelism strategy on a dime, without needing
    to rewrite our neural network function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `xmap`，我们可以随时修改我们的并行处理策略，而无需重写我们的神经网络函数。
- en: Preliminaries
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: '[PRE8]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To better illustrate the new programming model, we make extensive use of custom
    type annotations in this notebook. The annotations have no effect on how the code
    evaluates and will be unchecked for now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明新的编程模型，我们在本笔记本中广泛使用自定义类型注释。这些注释不会影响代码的评估方式，并且现在将不会进行检查。
- en: '[PRE9]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Tensors with named axes
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有命名轴的张量
- en: 'The NumPy programming model is based around nd-arrays. Each nd-array can be
    associated with a two-component type:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 编程模型是围绕 nd-arrays 构建的。每个 nd-array 可以与两部分类型相关联：
- en: the element type (accessible via the `.dtype` attribute)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元素类型（通过`.dtype` 属性访问）
- en: shape (a tuple of integers given by `.shape`).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状（由`.shape`给出的整数元组）。
- en: Using our little type annotation language, we will write these types as `dtype[shape_tuple]`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的小型类型注释语言，我们将这些类型写成`dtype[shape_tuple]`。
- en: For example, a 5x7x4 array of 32-bit floating point numbers will be denoted
    as `f32[(5, 7, 4)]`.
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，一个由32位浮点数构成的5x7x4数组将被表示为`f32[(5, 7, 4)]`。
- en: 'Here is a small example that shows how the annotations can demonstrate the
    way shapes propagate through a simple NumPy program:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个小例子，展示了注释如何演示形状在简单NumPy程序中传播的方式：
- en: '[PRE10]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The extension we propose is to add another component of array type: a `named_shape`,
    mapping axis names (arbitrary hashable objects, with strings being a common choice)
    to integer sizes. Most importantly, because each axis has a name, their order
    has no meaning. That is, a named shape of `{''a'': 2, ''b'': 5}` is indistinguishable
    from a named shape of `{''b'': 5, ''a'': 2}`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '我们建议的扩展是添加另一个数组类型的组成部分：一个`named_shape`，将轴名称（任意可散列对象，字符串是常见选择）映射到整数大小。最重要的是，因为每个轴都有一个名称，它们的顺序没有意义。也就是说，`{''a'':
    2, ''b'': 5}`的命名形状与`{''b'': 5, ''a'': 2}`的命名形状是无法区分的。'
- en: 'This is not an entirely new idea. Some good examples of where using named axes
    has been proposed in the past are: [Mesh TensorFlow](https://github.com/tensorflow/mesh),
    [Tensor Considered Harmful](http://nlp.seas.harvard.edu/NamedTensor) manifesto
    as well as the [xarray](http://xarray.pydata.org/en/stable/) and [einops](http://einops.rocks/)
    packages. Keep in mind that many of those are slightly different in that they
    do assign an order to the named axes, but they are unordered in JAX.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这并不是一个全新的想法。过去提出使用命名轴的一些好例子包括：[Mesh TensorFlow](https://github.com/tensorflow/mesh)、[Tensor
    Considered Harmful](http://nlp.seas.harvard.edu/NamedTensor)宣言，以及[xarray](http://xarray.pydata.org/en/stable/)和[einops](http://einops.rocks/)包。请记住，这些中许多在于它们在JAX中无序，尽管它们会为命名轴分配顺序。
- en: From now on we will allow the type annotations to have two components, the first
    one still being the value’s `.shape`, while the second one will be the `.named_shape`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将允许类型注释具有两个组件，第一个仍然是值的`.shape`，而第二个将是`.named_shape`。
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While we don’t modify the meaning of `.ndim` (which is always equal to `len(shape)`)
    and `.size` (equal to the product of `shape`), we do so solely for backward-compatibility
    reasons. The true rank of an array that has non-empty named axes is `len(shape)
    + len(named_shape)`. The true number of elements stored in such an array is equal
    to the product of sizes of all dimensions, both positional and named.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不修改`.ndim`的含义（始终等于`len(shape)`）和`.size`的含义（等于`shape`的乘积），但我们仅出于向后兼容性的原因而这样做。具有非空命名轴的数组的真实秩为`len(shape)
    + len(named_shape)`。存储在这种数组中的元素的真实数量等于所有维度的大小的乘积，包括位置和命名维度。
- en: Introducing and eliminating named axes
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入和消除命名轴
- en: But how does one create such arrays, if all top-level JAX operations work in
    the NumPy model with purely positional axes? While this constraint could be lifted
    at some point, for the time being the only way to introduce named axes is to use
    `xmap`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果所有顶级JAX操作都使用纯位置轴的NumPy模型，那么如何创建这样的数组呢？尽管在某些时候可以解除此约束，但目前引入命名轴的唯一方式是使用`xmap`。
- en: '`xmap` can be thought of as an adapter that takes in arrays with positional
    axes, makes some of them named (as specified by `in_axes`), and calls the function
    that it wraps. Once the wrapped function returns arrays, all named axes appearing
    in those are converted back to positional axes (as specified by `out_axes`).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`xmap`可以被视为一种适配器，接受具有位置轴的数组，将其中一些命名（由`in_axes`指定），并调用其包装的函数。一旦包装函数返回数组，所有出现在其中的命名轴都会转换回位置轴（由`out_axes`指定）。'
- en: '`in_axes` should have a structure that matches the signature of the `xmap`ped
    function arguments, except with all places where array arguments would be replaced
    by an *axis mapping*. There are two ways in which axis mappings can be specified:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`in_axes`的结构应该与`xmap`函数参数的签名匹配，但所有数组参数的位置都应被*轴映射*替换。有两种指定轴映射的方式：'
- en: 'as dictionaries mapping positional axes to axis names (e.g. `{0: ''x'', 2:
    ''y''}`); and'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '作为将位置轴映射到轴名称的字典（例如`{0: ''x'', 2: ''y''}`）；以及'
- en: as lists of axis names terminated by the ellipsis object (e.g. `['a', 'b', ...]`),
    indicating that a prefix of positional dimensions are to be mapped to given names.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为以省略号对象结尾的轴名称列表（例如`['a', 'b', ...]`），指示要将一组位置维度映射到给定名称。
- en: '`out_axes` are similar, except that their structure has to match the return
    signature of the `xmap`ped function (but again, with all arrays replaced by axes
    mappings).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`out_axes`类似，但其结构必须与`xmap`函数的返回签名匹配（但再次，所有数组都用轴映射替换）。'
- en: For each array argument, all positional axes mentioned in its respective `in_axes`
    axis mapping are converted to named axes. For each array result, all named axes
    are inserted in the positions indicated by its respective `out_axes`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数组参数，其各自的`in_axes`轴映射中提到的所有位置轴都会转换为命名轴。对于每个数组结果，所有命名轴都插入到其各自`out_axes`指示的位置中。
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: While this might seem like a handful at first, if you’ve seen code that uses
    `jnp.einsum` you are already familiar with this approach. The `einsum` function
    interprets an expression such as `nk,km->nm` assigning names (each letter is considered
    a separate name) to positional axes, performing necessary broadcasts and reductions,
    and finally putting back the results in positional axes, according to the order
    given by the right-hand side of the `->` separator. While `einsum` never lets
    you interact with named axes directly, they do appear naturally in its implementation.
    `xmap` is a *generalized einsum* because named axes are now first-class and you
    get to implement the function that can manipulate them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然起初可能会有些困难，但如果您见过使用`jnp.einsum`的代码，您已经熟悉这种方法。 `einsum`函数解释表达式如`nk,km->nm`，为位置轴分配名称（每个字母被视为单独的名称），执行必要的广播和约简，最后根据`->`分隔符右侧给定的顺序将结果放回位置轴。虽然`einsum`从不让您直接与命名轴交互，但它们在其实现中自然出现。
    `xmap`是*广义的einsum*，因为现在命名轴是一流的，您可以实现可以操作它们的函数。
- en: Continuing this analogy, `xmap(my_func, ...)` from the above example is equivalent
    to `jnp.einsum('bx->xb')`. But of course not every `xmap`ped function will have
    an equivalent `einsum`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 继续这个类比，上述示例中的`xmap(my_func, ...)`等同于`jnp.einsum('bx->xb')`。但当然，并非每个`xmap`的函数都有等效的`einsum`。
- en: 'One more similarity with `einsum` is that whenever a name is reused for multiple
    axes, they do have to have the same size:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个与`einsum`相似的地方是，每当一个名称被多个轴重用时，它们必须具有相同的大小：
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Named axis propagation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名轴传播
- en: We now know how named axes are introduced and eliminated, but what are they
    good for? How do they propagate throughout the program? Let’s explore a few examples.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道了命名轴是如何引入和消除的，但它们有什么好处？它们如何在整个程序中传播？让我们来探讨几个例子。
- en: Interactions with positional axes
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与位置轴的交互
- en: 'First rule: named axes never implicitly interact with positional axes. Any
    function that’s written without named axes in mind can always be invoked with
    inputs that have named dimensions. The result is the same as if `vmap` was applied
    on a per-named-axis basis.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条规则：命名轴从不隐式与位置轴交互。任何未考虑命名轴的函数总是可以使用具有命名尺寸的输入调用。结果与应用`vmap`到每个命名轴基础上时的结果相同。
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Broadcasting
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广播
- en: 'Secondly, named axes are broadcast *by name*, and every existing NumPy (and
    almost every JAX) operator implicitly broadcasts the named dimensions. Whenever
    a standard NumPy function is called with arrays with named axes, the NumPy function
    determines the positional shape of the result array, while the named shape becomes
    a union of all named shapes of its inputs. Analyze the following example to understand
    how the axes propagate:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，命名轴通过*名称*进行广播，几乎每个现有的NumPy（以及几乎每个JAX）运算符都会隐式地广播命名维度。每当使用具有命名轴的数组调用标准NumPy函数时，NumPy函数确定结果数组的位置形状，而命名形状成为其输入所有命名形状的并集。分析以下示例以了解轴如何传播：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To recap, the named shape of the result of an expression such as `i + j` with
    `i` having a named shape of `{''a'': 2, ''b'': 3}` and `j` of `{''b'': 3, ''c'':
    5}` is `{''a'': 2, ''b'': 3, ''c'': 5}`. The `''b''` axis is present in both inputs,
    so no broadcasting is necessary, while `''a''` and `''c''` occur in only one of
    the two inputs, causing the other one to get broadcast along the axis missing
    in its named shape.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '总结一下，例如表达式`i + j`的结果的命名形状，其中`i`的命名形状为`{''a'': 2, ''b'': 3}`，`j`为`{''b'': 3,
    ''c'': 5}`，则为`{''a'': 2, ''b'': 3, ''c'': 5}`。 `''b''`轴存在于两个输入中，因此不需要广播，而`''a''`和`''c''`仅出现在两个输入中的一个中，导致另一个沿其命名形状中缺少的轴进行广播。'
- en: No shape errors can occur when operating over named axes, because `xmap` enforces
    that a single name is associated with a single size inside its body.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 操作命名轴时不会出现形状错误，因为`xmap`强制其体内的单个名称与单个大小关联。
- en: While the rule for broadcasting named axes might seem like an arbitrary extension
    of the NumPy model, it is actually consistent with it.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管广播命名轴的规则可能看起来像NumPy模型的任意扩展，但实际上与其一致。
- en: ''
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Broadcasting first looks for pairs of dimensions it considers as equivalent
    in both operands. For all matched pairs, it asserts that both sizes are equal
    or one of them is 1\. All unpaired dimensions are carried over to the result.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 广播首先查找它认为在两个操作数中等效的维度对。对于所有匹配的维度对，它断言两个尺寸要么相等，要么其中一个为1。所有未配对的维度都传递到结果中。
- en: ''
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now, in the positional world the way NumPy broadcasting chooses to form the
    pairs is by right-aligning the shapes. But our axes are named, so there is a straightforward
    way of finding equivalent axes: just check their names for equality!'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，在位置世界中，NumPy广播选择形成对的方式是通过右对齐形状。但是我们的轴是有名称的，因此找到等效轴的方法非常直接：只需检查它们的名称是否相等！
- en: Reductions
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缩减
- en: But named axes are not only good for batching! In fact, our goal is that named
    axes should be equivalent to positional axes. In particular, every NumPy function
    that takes in positional axes as arguments should also accept named axes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，命名轴不仅对批处理有益！实际上，我们的目标是，命名轴应等同于位置轴。特别是，每个将位置轴作为参数的NumPy函数也应接受命名轴。
- en: 'The paragraph above is aspirational and the set of NumPy functions that do
    accept named axes is relatively limited. At the moment named axes are only supported
    in:'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 上面的段落是雄心勃勃的，接受具有命名轴的NumPy函数的集合相对有限。目前，仅支持具有命名轴的：
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`jnp.sum`, `jnp.max`, `jnp.min`'
  id: totrans-72
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.sum`、`jnp.max`、`jnp.min`'
- en: 'Reductions are a good example:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减是一个很好的例子：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`einsum`'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`einsum`'
- en: Similarly to how we have extended reductions with support for named axes, we’ve
    also made it possible to contract over named axes using `jnp.einsum`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们如何扩展支持命名轴的缩减，我们还使得可以使用`jnp.einsum`在命名轴上进行收缩成为可能。
- en: Operands and results still use a convention of one letter per positional axis,
    but now it is also possible to mention named axes in curly braces. For example,
    `n{b,k}` implies that a value will have a single positional dimension `n` and
    named dimensions `b` and `k` (their order doesn’t matter). Following the usual
    einsum semantics, any named axes that appear in inputs, but do not appear in an
    output will be contracted (summed after all multiplications are performed).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数和结果仍然使用每个位置轴的一个字母的约定，但现在也可以在花括号中提到命名轴。例如，`n{b,k}`表示一个值将具有单个位置维度`n`和命名维度`b`和`k`（它们的顺序不重要）。按照通常的einsum语义，任何出现在输入中但不出现在输出中的命名轴都将被收缩（在执行所有乘法后求和）。
- en: It is acceptable to omit a named dimension from *all arguments and the result*
    in which case it will be treated according to the usual broadcasting semantics.
    However, it is not acceptable to mention a named axis in one argument that has
    it in its named shape and skip it in another argument that also has it in its
    named shape. Of course, skipping it in the arguments that don’t have it is required.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以接受从*所有参数和结果*中省略一个命名维度，此时它将根据通常的广播语义处理。但是，在一个参数中提到具有命名形状的命名轴并跳过另一个参数中也具有它的命名形状是不可接受的。当然，在没有它的参数中跳过它是必需的。
- en: 'NOTE: This invariant is **unchecked** at the moment (it is still work-in-progress).
    Such axis skipping will result in undefined behavior.'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：目前**未经检查**（仍在进行中）。这种跳过轴将导致未定义的行为。
- en: At the moment `jnp.einsum` with named axes only supports two inputs and a single
    result.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 目前，`jnp.einsum`仅支持两个输入和单个结果的命名轴。
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The example above is admittedly no clearer than using `jnp.einsum` directly.
    But contractions over named axes are a crucial component of larger applications
    such as Transformer models and this is only meant to be an exercise to show you
    how the names propagate.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例毫不意外地比直接使用`jnp.einsum`更清晰。但是，对命名轴的收缩是更大应用（如Transformer模型）的关键组成部分，这只是一个演示如何传播名称的练习。
- en: Collectives
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集合
- en: Finally, all collectives that could have been used with `pmap`ped functions
    also work with named axes. As we’ll show later, `xmap` can be used as a drop-in
    replacement for `pmap` that makes programming for multi-dimensional hardware meshes
    much easier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有可能用于`pmap`函数的集合在命名轴上也有效。正如我们稍后将展示的，`xmap`可以作为`pmap`的替代方案，使多维硬件网格的编程变得更加容易。
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parallelism support
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行支持
- en: While the new programming paradigm can be nice at times, the killer feature
    of `xmap` is its ability to parallelize code over supercomputer-scale hardware
    meshes!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管新的编程范式有时可能很好，但 `xmap` 的杀手级特性在于其能够在超级计算机规模的硬件网格上并行化代码！
- en: Named axes are the secret sauce that makes all this possible, thanks to the
    carefully tuned rules that describe their propagation. Good support for partitioning
    in a purely positional programming model is notoriously difficult. Positional
    axes are usually disposable and it is hard to keep track of the way axis partitioning
    propagates through the program. As you’ll see below, named axes enable us to define
    a straightforward correspondence between their names and hardware resources, making
    it easy to reason about the way different values end up partitioned.
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 命名轴是使所有这一切成为可能的秘密武器，多亏了精心调整的规则来描述它们的传播方式。在纯位置编程模型中支持分区通常非常困难。位置轴通常是一次性的，很难跟踪轴分区传播方式。正如您将在下文中看到的，命名轴使我们能够定义它们的名称与硬件资源之间直接的对应关系，从而使我们能够轻松推断不同值的分区方式。
- en: 'In all the previous examples, we haven’t said a word about parallelism and
    for a good reason. By default `xmap` doesn’t perform any parallelization and vectorizes
    the computation in the same way `vmap` does (i.e. it still executes on a single
    device). To partition the computation over multiple accelerators we have to introduce
    one more concept: *resource axes*.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有先前的示例中，我们还没有提到并行性，有其原因。默认情况下，`xmap` 不执行任何并行化，而是像 `vmap` 一样向量化计算（即仍然在单个设备上执行）。要在多个加速器上分区计算，我们必须引入一个概念：*资源轴*。
- en: The basic idea is that logical axes (the ones that appear in named shapes) assume
    that we have abundant hardware and memory, but before the program is to be executed,
    they have to be placed somewhere. The default (`vmap`-like) evaluation style pays
    a high memory cost on the default JAX device. By mapping logical axes to (one
    or more) resource axes through the `axis_resources` argument, we can control how
    `xmap` evaluates the computation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是逻辑轴（出现在命名形状中的轴）假设我们拥有充足的硬件和内存，但在程序执行之前，它们必须放置在某个位置。默认的（类似 `vmap` 的）评估风格在默认的
    JAX 设备上付出了高昂的内存成本。通过通过 `axis_resources` 参数将逻辑轴映射到（一个或多个）资源轴，我们可以控制 `xmap` 如何评估计算。
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Both `local_matmul` and `distr_matmul` implement matrix multiplication, but
    `distr_matmul` will additionally partition the `left` and `right` logical axes
    over the `x` and `y` resource axes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_matmul` 和 `distr_matmul` 都实现了矩阵乘法，但 `distr_matmul` 会额外将 `left` 和 `right`
    逻辑轴分割到 `x` 和 `y` 资源轴上。'
- en: But… where do those resource names come from?
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 但是... 这些资源名称是从哪里来的呢？
- en: Well, it depends, but one good choice is… a hardware mesh!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这取决于情况，但一个很好的选择是... 硬件网格！
- en: For our purposes a mesh is an nd-array of devices with named axes. But, because
    NumPy doesn’t support named axes (that’s our extension!), the meshes are represented
    by a pair of an nd-array of JAX device objects (as obtained from `jax.devices()`
    or `jax.local_devices()`) and a tuple of resource axis names of length matching
    the rank of the array.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，网格是一个带有命名轴的设备 nd-数组。但由于 NumPy 不支持命名轴（这是我们的扩展！），网格由 JAX 设备对象的 nd-数组对（如从
    `jax.devices()` 或 `jax.local_devices()` 获得的对象）和长度与数组秩匹配的资源轴名称元组表示。
- en: '![How real hardware is represented as an abstract mesh](img/1f28a4882e87ec96f5c8629ec0e857ae.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![真实硬件如何表示为抽象网格](img/1f28a4882e87ec96f5c8629ec0e857ae.png)'
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The mesh axis names are exactly the names of resources that named axes can
    be mapped to. But just creating a mesh definition won’t make the resource names
    visible to `distr_matmul`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 网格轴名称正是可以将命名轴映射到的资源名称。但仅创建网格定义并不会使资源名称对 `distr_matmul` 可见：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To introduce the resources in a scope, use the `with Mesh` context manager:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要在范围内引入资源，请使用 `with Mesh` 上下文管理器：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Anyway, the best part of it is that specifying `axis_resources` **never changes
    program semantics**. You are free to experiment with different ways of partitioning
    your computation (just change the assignment of resources to named axes!) and
    even how the physical devices are organized in the mesh (by changing the construction
    of the NumPy array of devices). None of those things should have any significant
    influence on the results you get back (up to, for example, floating point inaccuracy),
    though of course some of them will achieve significantly better performance than
    the others.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，最好的部分在于，指定`axis_resources`**从不改变程序语义**。您可以自由尝试不同的计算分区方式（只需更改资源分配到命名轴的分配！），甚至可以更改网格中物理设备的组织方式（通过更改设备的NumPy数组构造）。这些变化不应对您获得的结果产生重大影响（例如浮点精度不准确性），尽管当然其中一些方法的性能显著优于其他方法。
- en: '`xmap` doesn’t provide any automatic scheduling options at the moment, because
    the best schedule often has to be somewhat carefully matched to your program.
    We’re considering adding support for that in the future, but it will take time.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`xmap`目前不提供任何自动调度选项，因为最佳调度通常必须与您的程序相匹配。我们正在考虑在未来添加对此的支持，但这需要时间。'
- en: Once you map a logical axis to a mesh dimension, the size of that logical axis
    has to be divisible by the mesh dimension size.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一旦您将逻辑轴映射到网格维度，该逻辑轴的大小必须可被网格维度大小整除。
- en: Is my data replicated? Or partitioned? Where is it?
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我的数据是复制的吗？还是分区的？它在哪里？
- en: Named axes also give us a neat way of reasoning about partitioning and replication.
    A value is partitioned over a mesh axis if and only if it has a named axis that
    has been mapped to that mesh axis in its shape. Otherwise, it will be replicated
    over all slices along that axis.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 命名轴还为我们提供了一种关于分区和复制的简洁方式。如果一个值在网格轴上分区，则该值的命名轴已在其形状中映射到该网格轴。否则，它将在该轴上的所有切片中复制。
- en: 'For example, assume that we’re in an `xmap` that had `axis_resources={''a'':
    ''x'', ''b'': ''y''}` specified (i.e. we are running the computation over a 2D
    mesh with `x` and `y` axes with sizes 2 and 3 respectively). Then:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，假设我们在具有`axis_resources={''a'': ''x'', ''b'': ''y''}`的`xmap`中（即在具有分别大小为2和3的`x`和`y`轴上运行计算）。那么：'
- en: An array of type `f32[(5, 5), {}]` is completely replicated over the whole mesh.
    All devices store a local copy of the value.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型为`f32[(5, 5), {}]`的数组在整个网格上完全复制。所有设备存储该值的本地副本。
- en: 'An array of type `f32[(6,), {''a'': 8}]` is partitioned over mesh axis `x`,
    because it has `''a''` in its named shape, and `''a''` is mapped to `x`. It is
    replicated over mesh axis `y`. To put it differently, all devices in a slice of
    the mesh with the same `x` coordinate will store a local copy of a chunk of this
    array. But, mesh slices with different `x` coordinates will store different chunks
    of the data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类型为`f32[(6,), {''a'': 8}]`的数组在网格轴`x`上进行分区，因为其命名形状中含有`''a''`，且`''a''`被映射到`x`。它在网格轴`y`上复制。换句话说，网格切片中具有相同`x`坐标的所有设备将存储该数组的一块本地副本。而具有不同`x`坐标的网格切片将存储数据的不同块。'
- en: 'An array of type `f32[(), {''a'': 8, ''c'': 7}]` is partitioned just like in
    the previous case: split over the `x` mesh axis and replicated over the `y` axis.
    Named dimensions with no resources specified are no different than positional
    dimensions when considering partitioning, so `''c''` has no influence on it.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类型为`f32[(), {''a'': 8, ''c'': 7}]`的数组与前一情况完全相同：在`x`网格轴上分割，在`y`轴上复制。未指定资源的命名维度在考虑分区时与位置维度没有任何不同，因此`''c''`对其没有影响。'
- en: 'An array of type `f32[(), {''a'': 8, ''b'': 12}]` is completely partitioned
    over the whole mesh. Every device holds a distinct chunk of the data.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '类型为`f32[(), {''a'': 8, ''b'': 12}]`的数组完全分区在整个网格上。每个设备持有数据的不同块。'
- en: '![An illustration for the above examples](img/afd219d6adaaded3ec4b6b9f97272976.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![上述示例的插图](img/afd219d6adaaded3ec4b6b9f97272976.png)'
- en: 'This also highlights one restriction: `xmap` won’t complain if you specify
    `axis_resources={''a'': ''x'', ''b'': ''x''}`, but consider how would an array
    with type `f32[(2, 8), {''a'': 4, ''b'': 12}]` be partitioned. If the size of
    the `x` mesh axis is 2, then we only have 2 devices, but we have 4 chunks to place
    (2 along `''a''` and 2 along `''b''`)! Now we can state it in full: **named axes
    mapped to the same resources can never both appear in the named shape of a single
    array**. But they can appear in named shapes of two distinct arrays, such as in
    this program:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '这也突显了一个限制：如果您指定`axis_resources={''a'': ''x'', ''b'': ''x''}`，`xmap`不会发出投诉，但请考虑`f32[(2,
    8), {''a'': 4, ''b'': 12}]`类型的数组如何分区。如果`x`网格轴的大小为2，则我们只有2个设备，但我们有4个要放置的块（2个沿着`''a''`和2个沿着`''b''`）！现在我们可以完整陈述：**映射到相同资源的命名轴永远不能同时出现在单个数组的命名形状中**。但它们可以出现在两个不同数组的命名形状中，例如在这个程序中：'
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This program is valid, because `jnp.sum` eliminates the axes that cannot co-occur
    before the values are added.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序是有效的，因为`jnp.sum`在值相加之前消除了不能同时出现的轴。
- en: While the final release of `xmap` will ensure that you don’t accidentally end
    up doing so, the current implementation *doesn’t verify it*. Violating this restriction
    will result in *undefined behavior*.
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管最终版本的`xmap`将确保您不会意外地这样做，但当前实现并*不验证它*。违反此限制将导致*未定义的行为*。
- en: Why `axis_resources` and not a more direct mapping to hardware?
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择`axis_resources`而不是更直接地映射到硬件？
- en: At this point you might wonder why go through the detour of introducing yet
    another concept of resource axes in the mix. For as long as you’re interested
    in partitioning your computations over hardware, there is no good reason, but
    this mental framework is more flexible than that!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此时您可能会想为什么要引入混合中的另一个资源轴的绕道。只要您对在硬件上分区您的计算感兴趣，就没有好的理由，但这种思维框架比那更灵活！
- en: 'For example, there is one additional resource we all deal with: time! Just
    like a computation can be partitioned over multiple hardware devices, e.g. to
    lower its memory usage, the same thing can be achieved with a single accelerator
    that evaluates a chunk of the computation in multiple steps.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们都在处理一个额外的资源：时间！就像计算可以分区到多个硬件设备上，例如降低其内存使用，同样的事情可以通过一个单一的加速器实现，该加速器在多个步骤中评估计算的一个块。
- en: So, while hardware meshes are the only source of resource axes in JAX programs
    at the moment, we are planning to extend the whole system with other sources.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然硬件网格目前是JAX程序中资源轴的唯一来源，但我们计划扩展整个系统以涵盖其他来源。
- en: Porting positional code to named code
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将位置代码移植到命名代码
- en: In this section we will go over a few more real examples to show how `xmap`
    can help you implement and distribute various models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将举几个实际例子，展示`xmap`如何帮助您实现和分发各种模型。
- en: '**This section is a work in progress**'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**本节内容正在完善**'
