- en: Building on JAX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在JAX之上构建
- en: 原文：[`jax.readthedocs.io/en/latest/building_on_jax.html`](https://jax.readthedocs.io/en/latest/building_on_jax.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/building_on_jax.html`](https://jax.readthedocs.io/en/latest/building_on_jax.html)
- en: A great way to learn advanced JAX usage is to see how other libraries are using
    JAX, both how they integrate the library into their API, what functionality it
    adds mathematically, and how it’s used for computational speedup in other libraries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 学习高级JAX使用的一种很好的方法是看看其他库如何使用JAX，它们如何将库集成到其API中，它在数学上添加了什么功能，并且如何在其他库中用于计算加速。
- en: Below are examples of how JAX’s features can be used to define accelerated computation
    across numerous domains and software packages.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是JAX功能如何用于跨多个领域和软件包定义加速计算的示例。
- en: Gradient Computation
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度计算
- en: Easy gradient calculation is a key feature of JAX. In the [JaxOpt library](https://github.com/google/jaxopt)
    value and grad is directly utilized for users in multiple optimization algorithms
    in [its source code](https://github.com/google/jaxopt/blob/main/jaxopt/_src/base.py#LL87C30-L87C44).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的梯度计算是JAX的一个关键特性。在[JaxOpt库](https://github.com/google/jaxopt)中值和grad直接用于用户在[其源代码](https://github.com/google/jaxopt/blob/main/jaxopt/_src/base.py#LL87C30-L87C44)中的多个优化算法中。
- en: Similarly the same Dynamax Optax pairing mentioned above is an example of gradients
    enabling estimation methods that were challenging historically [Maximum Likelihood
    Expectation using Optax](https://probml.github.io/dynamax/notebooks/linear_gaussian_ssm/lgssm_learning.html).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，上面提到的Dynamax Optax配对，是过去具有挑战性的梯度使估计方法的一个例子，[Optax的最大似然期望](https://probml.github.io/dynamax/notebooks/linear_gaussian_ssm/lgssm_learning.html)。
- en: Computational Speedup on a Single Core across Multiple Devices
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个设备上单核计算速度加快
- en: Models defined in JAX can then be compiled to enable single computation speedup
    through JIT compiling. The same compiled code can then be sent to a CPU device,
    to a GPU or TPU device for additional speedup, typically with no additional changes
    needed. This allows for a smooth workflow from development into production. In
    Dynamax the computationally expensive portion of a Linear State Space Model solver
    has been [jitted](https://github.com/probml/dynamax/blob/main/dynamax/linear_gaussian_ssm/models.py#L579).
    A more complex example comes from PyTensor which compiles a JAX function dynamically
    and then [jits the constructed function](https://github.com/pymc-devs/pytensor/blob/main/pytensor/link/jax/linker.py#L64).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在JAX中定义的模型然后可以被编译以通过JIT编译进行单次计算速度加快。相同的编译码然后可以被发送到CPU设备，GPU或TPU设备以获得额外的速度加快，通常不需要额外的更改。
    这允许平稳地从开发流程转入生产流程。在Dynamax中，线性状态空间模型求解器的计算密集型部分已[jitted](https://github.com/probml/dynamax/blob/main/dynamax/linear_gaussian_ssm/models.py#L579)。
    PyTensor的一个更复杂的例子源于动态地编译JAX函数，然后[jit构造的函数](https://github.com/pymc-devs/pytensor/blob/main/pytensor/link/jax/linker.py#L64)。
- en: Single and Multi Computer Speedup Using Parallelization
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用并行化的单台和多台计算机加速
- en: Another benefit of JAX is the simplicity of parallelizing computation using
    `pmap` and `vmap` function calls or decorators. In Dynamax state space models
    are parallelized with a [VMAP decorator](https://github.com/probml/dynamax/blob/main/dynamax/linear_gaussian_ssm/parallel_inference.py#L89)
    a practical example of this use case being multi object tracking.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: JAX的另一个好处是使用`pmap`和`vmap`函数调用或装饰器轻松并行化计算。在Dynamax中，状态空间模型使用[VMAP装饰器](https://github.com/probml/dynamax/blob/main/dynamax/linear_gaussian_ssm/parallel_inference.py#L89)进行并行化，其实际用例是多对象跟踪。
- en: Incorporating JAX code into your, or your users, workflows
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将JAX代码合并到您的工作流程中或您的用户工作流程中
- en: JAX is quite composable and can be used in multiple ways. JAX can be used with
    a standalone pattern, where the user defines all the calculations themselves.
    However other patterns, such as using libraries built on jax that provide specific
    functionality. These can be libraries that define specific types of models, such
    as Neural Networks or State Space models or others, or provide specific functionality
    such as optimization. Here are more specific examples of each pattern.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: JAX非常可组合，并且可以以多种方式使用。 JAX可以作为独立模式使用，用户自己定义所有计算。 但是其他模式，例如使用构建在jax上提供特定功能的库。
    这些可以是定义特定类型的模型的库，例如神经网络或状态空间模型或其他，或者提供特定功能，例如优化。以下是每种模式的更具体的示例。
- en: Direct Usage
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接使用
- en: Jax can be directly imported and utilized to build models “from scratch” as
    shown across this website, for example in [JAX Tutorials](https://jax.readthedocs.io/en/latest/tutorials.html)
    or [Neural Network with JAX](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html).
    This may be the best option if you are unable to find prebuilt code for your particular
    challenge, or if you’re looking to reduce the number of dependencies in your codebase.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Jax可以直接导入和利用，以便在本网站上“从零开始”构建模型，例如在[JAX教程](https://jax.readthedocs.io/en/latest/tutorials.html)或[使用JAX进行神经网络](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html)中展示的方法。如果您无法找到特定挑战的预建代码，或者希望减少代码库中的依赖项数量，这可能是最佳选择。
- en: Composable Domain Specific Libraries with JAX exposed
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用JAX暴露的可组合领域特定库
- en: Another common approach are packages that provide prebuilt functionality, whether
    it be model definition, or computation of some type. Combinations of these packages
    can then be mixed and matched for a full end to end workflow where a model is
    defined and its parameters are estimated.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见方法是提供预建功能的包，无论是模型定义还是某种类型的计算。这些包的组合可以混合使用，以实现全面的端到端工作流程，定义模型并估计其参数。
- en: One example is [Flax](https://github.com/google/flax) which simplifies the construction
    of Neural Networks. Flax is then typically paired with [Optax](https://github.com/deepmind/optax)
    where Flax defines the neural network architecture and Optax supplies the optimization
    & model-fitting capabilities.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是[Flax](https://github.com/google/flax)，它简化了神经网络的构建。通常将Flax与[Optax](https://github.com/deepmind/optax)配对使用，其中Flax定义了神经网络架构，而Optax提供了优化和模型拟合能力。
- en: Another is [Dynamax](https://github.com/probml/dynamax) which allows easy definition
    of state space models. With Dynamax parameters can be estimated using [Maximum
    Likelihood using Optax](https://probml.github.io/dynamax/notebooks/linear_gaussian_ssm/lgssm_learning.html)
    or full Bayesian Posterior can be estimating using [MCMC from Blackjax](https://probml.github.io/dynamax/notebooks/linear_gaussian_ssm/lgssm_hmc.html)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个是[Dynamax](https://github.com/probml/dynamax)，它允许轻松定义状态空间模型。使用Dynamax可以使用[Optax进行最大似然估计](https://probml.github.io/dynamax/notebooks/linear_gaussian_ssm/lgssm_learning.html)，或者使用[Blackjax进行MCMC全贝叶斯后验估计](https://probml.github.io/dynamax/notebooks/linear_gaussian_ssm/lgssm_hmc.html)。
- en: JAX Totally Hidden from Users
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户完全隐藏JAX
- en: Other libraries opt to completely wrap JAX in their model specific API. An example
    is PyMC and [Pytensor](https://github.com/pymc-devs/pytensor), in which a user
    may never “see” JAX directly but instead wrapping [JAX functions](https://pytensor.readthedocs.io/en/latest/extending/creating_a_numba_jax_op.html)
    with a PyMC specific API.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其他库选择完全包装JAX以适应其特定API。例如，PyMC和[Pytensor](https://github.com/pymc-devs/pytensor)就是一个例子，用户可能从未直接“看到”JAX，而是使用PyMC特定的API包装[JAX函数](https://pytensor.readthedocs.io/en/latest/extending/creating_a_numba_jax_op.html)。
