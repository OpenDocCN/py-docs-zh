- en: Custom operations for GPUs with C++ and CUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 C++ 和 CUDA 进行 GPU 自定义操作
- en: 原文：[`jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html`](https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html`](https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html)
- en: JAX ships with a large number of built-in operations, but users occasionally
    run into a situation where they need a new operation that is not supported by
    JAX.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 预装有大量内置操作，但用户偶尔会遇到需要新操作但 JAX 不支持的情况。
- en: To accommodate such scenarios, JAX allows users to define custom operations
    and this tutorial is to explain how we can define one for GPUs and use it in single-GPU
    and multi-GPU environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应这些情况，JAX 允许用户定义自定义操作，本教程旨在解释如何为 GPU 定义并在单 GPU 和多 GPU 环境中使用它们。
- en: This tutorial contains information from [Extending JAX with custom C++ and CUDA
    code](https://github.com/dfm/extending-jax) and supposes that you are familiar
    with [JAX primitive](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程包含来自 [使用自定义 C++ 和 CUDA 代码扩展 JAX](https://github.com/dfm/extending-jax) 的信息，并假设您熟悉
    [JAX 原语](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html)。
- en: RMS normalization
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMS 标准化
- en: For this tutorial, we are going to add the RMS normalization as a custom operation
    in JAX. Note that the RMS normalization can be expressed with [`jax.numpy`](https://jax.readthedocs.io/en/latest/jax.numpy.html)
    directly. However, we are using it as an example to show the process of creating
    a custom operation for GPUs. The CUDA code in `gpu_ops/rms_norm_kernels.cu` for
    this operation has been borrowed from [Apex](https://github.com/NVIDIA/apex/blob/master/csrc/layer_norm_cuda_kernel.cu)
    and adapted to eliminate any dependency on PyTorch.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将 RMS 标准化作为 JAX 中的自定义操作添加。请注意，可以直接使用 [`jax.numpy`](https://jax.readthedocs.io/en/latest/jax.numpy.html)
    表达 RMS 标准化。但是，我们使用它作为示例来展示如何为 GPU 创建自定义操作的过程。此操作在 `gpu_ops/rms_norm_kernels.cu`
    中的 CUDA 代码已从 [Apex](https://github.com/NVIDIA/apex/blob/master/csrc/layer_norm_cuda_kernel.cu)
    借用，并进行了修改，以消除对 PyTorch 的任何依赖。
- en: High-level steps
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级步骤
- en: This tutorial shows how to write both a custom operation and its gradient.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程展示了如何编写自定义操作及其梯度。
- en: 'In C: You need to follow these steps in C for each new JAX primitive:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C 中：每个新的 JAX 原语都需要按照以下步骤进行操作。
- en: Have CUDA kernel(s).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 CUDA 核心（核心）。
- en: Create a C function that dispatches the CUDA kernel that will be called by XLA.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建分派 CUDA 核心的 C 函数，该函数将由 XLA 调用。
- en: Create a descriptor to convey information needed for the computation.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建描述符以传达计算所需的信息。
- en: The types, the shapes and other attributes.
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型、形状和其他属性。
- en: Bind C functions to Python
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 C 函数绑定到 Python
- en: To create the descriptor and to call the primitive during execution.
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以创建描述符并在执行期间调用原语。
- en: 'In Python: You need to follow these steps in Python:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中：您需要按照以下步骤进行操作。
- en: Define a new JAX primitive (instruction/operation)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义新的 JAX 原语（指令/操作）
- en: Write Python functions to build the graph nodes with the primitive.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 Python 函数以使用原语构建图节点。
- en: Define its abstract evaluation.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义其抽象评估。
- en: Define its lowering to MLIR.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义其降低到 MLIR。
- en: '[Optional] Define the gradient.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可选] 定义梯度。'
- en: '[Optional] Use [custom_partitioning](https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html)
    or [shard_map](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html)
    functions for fast multi-GPU.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可选] 使用 [custom_partitioning](https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html)
    或 [shard_map](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html) 函数实现快速多
    GPU 支持。'
- en: C code
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C 代码
- en: See `gpu_ops` code listing for a complete code listing of C++ and CUDA files.
    `gpu_ops/rms_norm_kernels.cu` defines the following functions, which are declared
    with the XLA custom function signature. These functions are responsible for launching
    RMS normalization kernels with the given `buffers` on the specified `stream`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 `gpu_ops` 代码列表，其中包含完整的 C++ 和 CUDA 文件代码列表。`gpu_ops/rms_norm_kernels.cu` 定义了以下函数，这些函数使用给定的
    `buffers` 在指定的 `stream` 上启动 RMS 标准化核心。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`stream` is the CUDA stream to be used to execute any kernel on the GPU.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream` 是用于在 GPU 上执行任何核心的 CUDA 流。'
- en: '`buffers` has all pointers to input buffers followed by all pointers to output
    buffers.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`buffers` 包含所有指向输入缓冲区的指针，后跟所有指向输出缓冲区的指针。'
- en: '`opaque` is a buffer for any extra information that is being passed to the
    custom functions and `opaque_len` is the length of `opaque`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`opaque` 是传递给自定义函数的任何额外信息的缓冲区，而 `opaque_len` 是 `opaque` 的长度。'
- en: For this tutorial, an `RMSNormDescriptor` object will be passed to these functions
    as `opaque`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将通过`opaque`将一个`RMSNormDescriptor`对象传递给这些函数。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, we need to expose these functions as well as `ElementType` and `RMSNormDescriptor`
    as a Python module, `gpu_ops`, through `pybind11`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要通过`pybind11`将这些函数以及`ElementType`和`RMSNormDescriptor`作为Python模块`gpu_ops`公开。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Build `gpu_ops` extension module
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建`gpu_ops`扩展模块
- en: We build the `gpu_ops` Python extension module with the aforementioned code.
    (See `gpu_ops` code listing for a complete code listing of C++ and CUDA files.)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述代码构建了`gpu_ops` Python扩展模块。（请参阅C++和CUDA文件的完整代码清单，查看`gpu_ops`代码列表。）
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Add RMS normalization to JAX as custom call
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将RMS归一化添加到JAX作为自定义调用
- en: '`gpu_ops` is just a Python extension module and we need more work to plug it
    into JAX.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpu_ops`只是一个Python扩展模块，我们需要更多工作来将其插入到JAX中。'
- en: Create primitives
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建原语
- en: We first create primitives, `_rms_norm_fwd_p` and `_rms_norm_bwd_p`, which the
    custom functions can be mapped to. We set the `multiple_results` attribute to
    `True` for these operations, which means that the operation produces multiple
    outputs as a tuple. When it is set to `False`, the operation produces a single
    output without a tuple. For more details, see [How JAX primitives work](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建了原语`_rms_norm_fwd_p`和`_rms_norm_bwd_p`，这些原语可以映射到自定义函数。我们为这些操作设置了`multiple_results`属性为`True`，表示该操作作为元组产生多个输出。当设置为`False`时，该操作将产生单个输出而不是元组。有关更多详细信息，请参见[How
    JAX primitives work](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html)。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lowering to MLIR custom call
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降低到MLIR自定义调用
- en: 'To map the custom functions to the new primitives, `_rms_norm_fwd_p` and `_rms_norm_bwd_p`,
    we need to:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将自定义函数映射到新原语`_rms_norm_fwd_p`和`_rms_norm_bwd_p`，我们需要：
- en: Register custom functions as custom call targets with `xla_client.register_custom_call_target`,
    and
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`xla_client.register_custom_call_target`注册自定义函数作为自定义调用目标，并且
- en: Register lowering functions that lower the primitives to MLIR custom calls with
    the registered custom call targets.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册将原语降低为MLIR自定义调用的降低函数，并使用注册的自定义调用目标。
- en: The functions `_rms_norm_fwd_cuda_lowering` and `_rms_norm_bwd_cuda_lowering`
    below lower the primitives to MLIR custom call operations with the custom targets
    from `gpu_ops`. These functions are registered with `jax.interpreters.mlir.register_lowering`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的函数`_rms_norm_fwd_cuda_lowering`和`_rms_norm_bwd_cuda_lowering`通过`gpu_ops`中的自定义目标将原语降低为MLIR自定义调用操作。这些函数已经注册到`jax.interpreters.mlir.register_lowering`中。
- en: Note that an `RMSNormDescriptor` object is created in the lowering function,
    and passed to the custom call as `opaque`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在降低函数中创建了一个`RMSNormDescriptor`对象，并将其作为`opaque`传递给自定义调用。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s test it
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们进行测试
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Test forward function
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试前向函数
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Abstract evaluation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽象评估
- en: 'The test above failed with `NotImplementedError: Abstract evaluation for ''rms_norm_fwd''
    not implemented`. Why did the test fail? What does it mean?'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '上述测试失败，报错信息为`NotImplementedError: Abstract evaluation for ''rms_norm_fwd''
    not implemented`。为什么测试失败？这是什么意思？'
- en: As part of the execution, JAX performs abstract evaluation. As JAX has no knowledge
    about the new primitives, it doesn’t know how to compute the output shapes and
    output data types, thus can’t evaluate these operations abstractly.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作为执行的一部分，JAX执行抽象评估。由于JAX对新原语没有任何了解，因此不知道如何计算输出形状和输出数据类型，因此无法进行这些操作的抽象评估。
- en: We need to provide a function for abstract evaluation of each primitive. These
    abstract evaluation functions compute the shape and the data type of the outputs,
    but don’t compute actual values for the operations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为每个原语的抽象评估提供一个函数。这些抽象评估函数计算输出的形状和数据类型，但不计算操作的实际值。
- en: These functions are passed to `.def_abstract_eval` method to be registered with
    the corresponding primitives.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数将传递给`.def_abstract_eval`方法，以便与相应的原语进行注册。
- en: See [How JAX primitives work](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#abstract-evaluation-rules)
    for more information on abstract evaluation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于抽象评估的信息，请参见[How JAX primitives work](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#abstract-evaluation-rules)。
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s test it again
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们再次进行测试
- en: Test the forward function
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试前向函数
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Test the backward function
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试反向函数
- en: Now let’s test the backward operation using `jax.grad` and `jtu.check_grads`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`jax.grad`和`jtu.check_grads`测试反向操作。
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Differentiation rule
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差分规则
- en: 'The backward operation failed with the error `NotImplementedError: Differentiation
    rule for ''rms_norm_fwd'' not implemented`. It means that, although we have defined
    `rms_norm_fwd` and `rms_norm_bwd`, JAX doesn’t know the relationship between them.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '反向操作以 `NotImplementedError: Differentiation rule for ''rms_norm_fwd'' not implemented`
    错误失败。这意味着，尽管我们定义了 `rms_norm_fwd` 和 `rms_norm_bwd`，但 JAX 不知道它们之间的关系。'
- en: We can teach JAX that `rms_norm_bwd` is the backward operation for `rms_norm_fwd`,
    using `jax.custom_vjp` and its convention. As the first step, we need to refine
    the definition of `rms_norm_fwd` and `rms_norm_bwd`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `jax.custom_vjp` 及其约定，教给 JAX `rms_norm_bwd` 是 `rms_norm_fwd` 的反向操作。作为第一步，我们需要完善
    `rms_norm_fwd` 和 `rms_norm_bwd` 的定义。
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`rms_norm_fwd` now returns an extra output `(invvar, x, weight)` for the residual
    data and `rms_norm_bwd` takes `eps`, `res`, and `g` as the parameters.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`rms_norm_fwd` 现在返回额外的输出 `(invvar, x, weight)` 作为剩余数据，而 `rms_norm_bwd` 接受 `eps`、`res`
    和 `g` 作为参数。'
- en: Once the relationship between `rms_norm_fwd` and `rms_norm_bwd` is established
    through `jax.custom_vjp`, JAX will ensure that the residual data from `rms_norm_fwd`
    is passed to `rms_norm_bwd` as `res` for backward operation. For non-differentiable
    parameters such as `eps`, JAX ensures that they are passed to the backward operation
    before the residual data. That’s why `eps` precedes `res` in the parameter list
    of `rms_norm_bwd`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `jax.custom_vjp` 建立 `rms_norm_fwd` 和 `rms_norm_bwd` 之间的关系后，JAX 将确保从 `rms_norm_fwd`
    中传递的剩余数据作为反向操作的 `res` 传递给 `rms_norm_bwd`。对于像 `eps` 这样的不可微参数，JAX 确保它们在剩余数据之前传递给反向操作。这就是为什么
    `eps` 在 `rms_norm_bwd` 的参数列表中位于 `res` 之前。
- en: Now that `rms_norm_fwd` returns the residual data, which is not needed for simple
    RMS normalization operation, we define a wrapper around it, `rms_norm`. It simply
    calls `rms_norm_fwd` and returns only `output`. Note that `rms_norm` is annotated
    with `@partial(jax.custom_vjp, nondiff_argnums=(2,))` and we are passing `rms_norm_fwd`
    and `rms_norm_bwd` to `rms_norm.defvjp`. It teaches JAX that, when `rms_norm`
    is differentiated, `rms_norm_fwd` is to be used for forward operation, and `rms_norm_bwd`
    is to be used for backward operation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `rms_norm_fwd` 返回了不需要用于简单 RMS 标准化操作的剩余数据，我们在其周围定义了一个包装器 `rms_norm`，它简单地调用
    `rms_norm_fwd` 并仅返回 `output`。请注意，`rms_norm` 使用 `@partial(jax.custom_vjp, nondiff_argnums=(2,))`
    进行了注释，并且我们将 `rms_norm_fwd` 和 `rms_norm_bwd` 传递给 `rms_norm.defvjp`。这教会了 JAX，在对
    `rms_norm` 进行微分时，使用 `rms_norm_fwd` 进行前向操作，使用 `rms_norm_bwd` 进行反向操作。
- en: See [Custom derivative rules for JAX-transformable Python functions](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#use-jax-custom-vjp-to-define-custom-reverse-mode-only-rules)
    for more information on `jax.custom_vjp`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用 `jax.custom_vjp` 定义 JAX 可转换 Python 函数的自定义导数规则，请参阅[自定义导数规则](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#use-jax-custom-vjp-to-define-custom-reverse-mode-only-rules)。
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With the refinement we have made, the backward operation test works with a
    modification: `loss` now calls `rms_norm` instead of `rms_norm_fwd`.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 经过我们的改进，反向操作测试与修改一起正常运行：`loss` 现在调用 `rms_norm` 而不是 `rms_norm_fwd`。
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s test it on multiple devices
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们在多个设备上进行测试。
- en: We are using `jax.experimental.pjit.pjit` for parallel execution on multiple
    devices, and we produce reference values with sequential execution on a single
    device.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 `jax.experimental.pjit.pjit` 在多个设备上进行并行执行，并在单个设备上的顺序执行中生成参考值。
- en: Test the forward function
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试前向函数。
- en: Let’s first test the forward operation on multiple devices. We are creating
    a simple 1D mesh and sharding `x` on all devices.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先在多个设备上测试前向操作。我们创建了一个简单的 1D 网格，并在所有设备上分片 `x`。
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The values have been computed correctly for forward operation, however, the
    generated HLO modules show an `all-gather` operation to replicate `x` on all devices,
    incurring large communication overhead.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前向操作，值已经计算正确，然而生成的 HLO 模块显示一个 `all-gather` 操作来在所有设备上复制 `x`，导致大量的通信开销。
- en: As XLA does not have enough knowledge about the custom functions to shard input
    tensors, it decides to replicate them to produce correct values before making
    the custom call.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 XLA 对于自定义函数不具备足够的知识来分片输入张量，它决定在进行自定义调用之前将它们复制以生成正确的值。
- en: 'To avoid this duplication, we can:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种重复，我们可以：
- en: '[custom_partitioning](https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html):
    to make it behave like all native JAX operations (but more complicated)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[custom_partitioning](https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html)：使其表现得像所有本机
    JAX 操作一样（但更复杂）。'
- en: Use manual sharding
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用手动分片
- en: '[shard_map](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html):
    the new replacement for xmap'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[shard_map](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html)：xmap
    的新替代品'
- en: '[xmap](https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html) (now
    deprecated)'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[xmap](https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html)（现已弃用）'
- en: This example demonstrates the use of custom_partitioning.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了使用 custom_partitioning 的用法。
- en: Shard the forward function with custom_partitioning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 custom_partitioning 分片向前函数
- en: We first create a helper function to help with all the JAX/XLA callback registration
    required.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个辅助函数来帮助所有需要的 JAX/XLA 回调注册。
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We define 2 JAX primitives, one inner primitive that map to the real kernel
    we want to warp in JAX. And an outer primitive that will be used with the custom_partitioning
    registration and for the gradient. (And if you implement the interface to support
    vmat, it will also be on the outer primitive).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个 JAX 原语，一个内部原语映射到我们想要在 JAX 中封装的真实内核。还有一个外部原语，将与自定义分区注册一起使用，并用于梯度。（如果您实现支持
    vmat 的接口，它也将位于外部原语中）。
- en: 'JAX custom_partitioning implementation are callbacks from XLA to Python during
    XLA sharding logic. XLA sharding goes in two phases: a sharding propagation phase
    and a partition phase. The propagation phase is when XLA plan the sharding to
    be created. It is the partition phase that create the sharded graph. For XLA to
    be able to shard our custom operations, it needs us to define 2 extra functions:
    infer_sharding_from_operands() and partition(). They are used in the first and
    second phase respectively.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: JAX custom_partitioning 实现是 XLA 在 XLA 分片逻辑期间从 XLA 到 Python 的回调。XLA 分片分为两个阶段：分片传播阶段和分区阶段。传播阶段是
    XLA 规划要创建的分片的阶段。分区阶段创建分片图。为了让 XLA 能够分片我们的自定义操作，它需要我们定义两个额外的函数：infer_sharding_from_operands()
    和 partition()。它们分别在第一阶段和第二阶段中使用。
- en: 'The infer_sharding_from_operands() function must do what its name say: infer
    the output sharding from the input sharding.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: infer_sharding_from_operands() 函数必须做其名称所述的事情：从输入分片推断输出分片。
- en: 'The partition() function will do a few things:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: partition() 函数将执行几个操作：
- en: tell which input sharding will be expected. XLA will reshad if needed.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告诉预期将有哪些输入分片。如有必要，XLA 将进行重新分片。
- en: tell the final version of the output sharding.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告诉输出分片的最终版本。
- en: give a function that will create the new instruction from the sharded inputs.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给出一个函数，将从分片输入创建新指令。
- en: 'See the code comments for more explanation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 查看代码注释以获取更多解释：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next we define the primitive for the backward pass of RMSNorm
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义 RMSNorm 后向传递的原语
- en: Shard the backward function with custom_partitioning
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 custom_partitioning 分片向后函数
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Plumbing to establish the forward and backward primitives with a custom_vjp
    rule as before:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与以前相同的自定义_vjp 规则建立前向和后向原语的管道：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With that we have completely defined our custom RMS norm primitive with custom_partitioning.
    To check for correctness we define the following loss functions: ref_loss is the
    reference value to compare against, while custom_p_loss uses our new primitive
    that implements custom_partitioning.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们完全定义了我们的自定义 RMS 规范原语与自定义分区。为了检查正确性，我们定义了以下损失函数：ref_loss 是要与之比较的参考值，而
    custom_p_loss 使用了我们新实现的实现了自定义分区的原语。
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Check for correctness
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查正确性
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now there are no all-gathers in the HLO, sharding is respected and only gradients
    are accumulated via an all-reduce.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 HLO 中没有全收集操作，尊重分片，只有通过全归约累积梯度。
- en: Let’s put it together
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们把它放在一起
- en: 'The complete definition of the primitives using custom_partitioning can be
    found in Custom_Operation_for_GPUs.py and the corresponding C++ code the defines
    python bindings in addition to the kernel implementations can be found below:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 custom_partitioning 完全定义原语的完整定义可以在 Custom_Operation_for_GPUs.py 中找到，以及定义
    python 绑定的相应 C++ 代码可以在以下找到：
- en: '`gpu_ops` code listing'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`gpu_ops` 代码列表'
- en: gpu_ops/kernel_helpers.h
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: gpu_ops/kernel_helpers.h
- en: gpu_ops/kernels.h
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: gpu_ops/kernels.h
- en: gpu_ops/pybind11_kernel_helpers.h
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: gpu_ops/pybind11_kernel_helpers.h
- en: gpu_ops/gpu_ops.cpp
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: gpu_ops/gpu_ops.cpp
- en: gpu_ops/rms_norm_kernels.cu
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: gpu_ops/rms_norm_kernels.cu
