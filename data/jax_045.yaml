- en: Training a Simple Neural Network, with tensorflow/datasets Data Loading
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个简单的神经网络，使用 `tensorflow/datasets` 进行数据加载
- en: 原文：[`jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html`](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html`](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html)
- en: '![Open in Colab](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)
    ![Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![在 Colab 中打开](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)
    ![在 Kaggle 中打开](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)'
- en: '*Forked from* `neural_network_and_data_loading.ipynb`'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*从* `neural_network_and_data_loading.ipynb` *衍生*'
- en: '![JAX](img/5f620f90762a1045911438d68b694265.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![JAX](img/5f620f90762a1045911438d68b694265.png)'
- en: Let’s combine everything we showed in the [quickstart](https://jax.readthedocs.io/en/latest/quickstart.html)
    to train a simple neural network. We will first specify and train a simple MLP
    on MNIST using JAX for the computation. We will use `tensorflow/datasets` data
    loading API to load images and labels (because it’s pretty great, and the world
    doesn’t need yet another data loading library :P).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们结合我们在[快速入门](https://jax.readthedocs.io/en/latest/quickstart.html)中展示的所有内容来训练一个简单的神经网络。我们将首先使用
    JAX 在 MNIST 上指定和训练一个简单的 MLP 进行计算。我们将使用 `tensorflow/datasets` 数据加载 API 来加载图像和标签（因为它非常出色，世界上不需要再另外一种数据加载库
    :P）。
- en: Of course, you can use JAX with any API that is compatible with NumPy to make
    specifying the model a bit more plug-and-play. Here, just for explanatory purposes,
    we won’t use any neural network libraries or special APIs for building our model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以使用 JAX 与任何与 NumPy 兼容的 API，使模型的指定更加即插即用。这里，仅供解释用途，我们不会使用任何神经网络库或特殊的 API
    来构建我们的模型。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Hyperparameters
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: Let’s get a few bookkeeping items out of the way.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先处理一些簿记事项。
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Auto-batching predictions
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动批量预测
- en: Let us first define our prediction function. Note that we’re defining this for
    a *single* image example. We’re going to use JAX’s `vmap` function to automatically
    handle mini-batches, with no performance penalty.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义我们的预测函数。请注意，我们为*单个*图像示例定义了这个函数。我们将使用 JAX 的 `vmap` 函数自动处理小批量数据，而不会影响性能。
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s check that our prediction function only works on single images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们的预测函数只适用于单个图像。
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point, we have all the ingredients we need to define our neural network
    and train it. We’ve built an auto-batched version of `predict`, which we should
    be able to use in a loss function. We should be able to use `grad` to take the
    derivative of the loss with respect to the neural network parameters. Last, we
    should be able to use `jit` to speed up everything.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经具备了定义和训练神经网络所需的所有要素。我们已经构建了一个自动批处理版本的 `predict` 函数，应该可以在损失函数中使用。我们应该能够使用
    `grad` 对神经网络参数的损失函数进行求导。最后，我们应该能够使用 `jit` 加速整个过程。
- en: Utility and loss functions
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用函数和损失函数
- en: '[PRE9]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Data Loading with `tensorflow/datasets`
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `tensorflow/datasets` 进行数据加载
- en: JAX is laser-focused on program transformations and accelerator-backed NumPy,
    so we don’t include data loading or munging in the JAX library. There are already
    a lot of great data loaders out there, so let’s just use them instead of reinventing
    anything. We’ll use the `tensorflow/datasets` data loader.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 主要专注于程序转换和支持加速的 NumPy，因此我们不包括数据加载或整理在 JAX 库中。已经有很多出色的数据加载器，所以我们只需使用它们，而不是重新发明轮子。我们将使用
    `tensorflow/datasets` 数据加载器。
- en: '[PRE10]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Training Loop
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练循环
- en: '[PRE13]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We’ve now used most of the JAX API: `grad` for derivatives, `jit` for speedups
    and `vmap` for auto-vectorization. We used NumPy to specify all of our computation,
    and borrowed the great data loaders from `tensorflow/datasets`, and ran the whole
    thing on the GPU.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经使用了大部分 JAX API：`grad` 用于求导，`jit` 用于加速和 `vmap` 用于自动向量化。我们使用 NumPy 来指定所有的计算，并从
    `tensorflow/datasets` 借用了优秀的数据加载器，并在 GPU 上运行了整个过程。
