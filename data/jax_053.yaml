- en: The Autodiff Cookbook
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动微分手册
- en: 原文：[`jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html`](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html`](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)
- en: '![Open in Colab](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)
    ![Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![在Colab中打开](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)
    ![在Kaggle中打开](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)'
- en: '*alexbw@, mattjj@*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*alexbw@, mattjj@*'
- en: JAX has a pretty general automatic differentiation system. In this notebook,
    we’ll go through a whole bunch of neat autodiff ideas that you can cherry pick
    for your own work, starting with the basics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: JAX拥有非常通用的自动微分系统。在这本手册中，我们将介绍许多巧妙的自动微分思想，您可以根据自己的工作进行选择。
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Gradients
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度
- en: Starting with `grad`
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从`grad`开始
- en: 'You can differentiate a function with `grad`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`grad`对函数进行微分：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`grad` takes a function and returns a function. If you have a Python function
    `f` that evaluates the mathematical function \(f\), then `grad(f)` is a Python
    function that evaluates the mathematical function \(\nabla f\). That means `grad(f)(x)`
    represents the value \(\nabla f(x)\).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`grad`接受一个函数并返回一个函数。如果您有一个评估数学函数 \( f \) 的Python函数 `f`，那么 `grad(f)` 是一个评估数学函数
    \( \nabla f \) 的Python函数。这意味着 `grad(f)(x)` 表示值 \( \nabla f(x) \)。'
- en: 'Since `grad` operates on functions, you can apply it to its own output to differentiate
    as many times as you like:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`grad`操作函数，您可以将其应用于其自身的输出以多次进行微分：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s look at computing gradients with `grad` in a linear logistic regression
    model. First, the setup:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在线性逻辑回归模型中使用`grad`计算梯度。首先，设置：
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Use the `grad` function with its `argnums` argument to differentiate a function
    with respect to positional arguments.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`argnums`参数的`grad`函数来相对于位置参数微分函数。
- en: '[PRE6]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This `grad` API has a direct correspondence to the excellent notation in Spivak’s
    classic *Calculus on Manifolds* (1965), also used in Sussman and Wisdom’s [*Structure
    and Interpretation of Classical Mechanics*](https://mitpress.mit.edu/9780262028967/structure-and-interpretation-of-classical-mechanics)
    (2015) and their [*Functional Differential Geometry*](https://mitpress.mit.edu/9780262019347/functional-differential-geometry)
    (2013). Both books are open-access. See in particular the “Prologue” section of
    *Functional Differential Geometry* for a defense of this notation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此`grad` API直接对应于Spivak经典著作*Calculus on Manifolds*（1965）中的优秀符号，也用于Sussman和Wisdom的*Structure
    and Interpretation of Classical Mechanics*（2015）及其*Functional Differential Geometry*（2013）。这两本书都是开放获取的。特别是参见*Functional
    Differential Geometry*的“序言”部分，以了解此符号的辩护。
- en: Essentially, when using the `argnums` argument, if `f` is a Python function
    for evaluating the mathematical function \(f\), then the Python expression `grad(f,
    i)` evaluates to a Python function for evaluating \(\partial_i f\).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`argnums`参数时，如果`f`是一个用于计算数学函数 \( f \) 的Python函数，则Python表达式`grad(f, i)`用于评估
    \( \partial_i f \) 的Python函数。
- en: Differentiating with respect to nested lists, tuples, and dicts
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相对于嵌套列表、元组和字典进行微分
- en: Differentiating with respect to standard Python containers just works, so use
    tuples, lists, and dicts (and arbitrary nesting) however you like.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准的Python容器进行微分是完全有效的，因此可以随意使用元组、列表和字典（以及任意嵌套）。
- en: '[PRE8]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can [register your own container types](https://github.com/google/jax/issues/446#issuecomment-467105048)
    to work with not just `grad` but all the JAX transformations (`jit`, `vmap`, etc.).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以[注册您自己的容器类型](https://github.com/google/jax/issues/446#issuecomment-467105048)以便不仅与`grad`一起工作，还可以与所有JAX转换（`jit`、`vmap`等）一起工作。
- en: Evaluate a function and its gradient using `value_and_grad`
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`value_and_grad`评估函数及其梯度
- en: 'Another convenient function is `value_and_grad` for efficiently computing both
    a function’s value as well as its gradient’s value:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方便的函数是`value_and_grad`，可以高效地计算函数值及其梯度值：
- en: '[PRE10]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Checking against numerical differences
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与数值差分进行对比
- en: 'A great thing about derivatives is that they’re straightforward to check with
    finite differences:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 导数的一个很好的特性是它们很容易用有限差分进行检查：
- en: '[PRE12]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'JAX provides a simple convenience function that does essentially the same thing,
    but checks up to any order of differentiation that you like:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 提供了一个简单的便利函数，本质上执行相同的操作，但可以检查任何您喜欢的微分顺序：
- en: '[PRE14]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hessian-vector products with `grad`-of-`grad`
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `grad`-of-`grad` 进行 Hessian 向量乘积
- en: One thing we can do with higher-order `grad` is build a Hessian-vector product
    function. (Later on we’ll write an even more efficient implementation that mixes
    both forward- and reverse-mode, but this one will use pure reverse-mode.)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高阶 `grad` 可以构建一个 Hessian 向量乘积函数。 （稍后我们将编写一个更高效的实现，该实现混合了前向和反向模式，但这个实现将纯粹使用反向模式。）
- en: A Hessian-vector product function can be useful in a [truncated Newton Conjugate-Gradient
    algorithm](https://en.wikipedia.org/wiki/Truncated_Newton_method) for minimizing
    smooth convex functions, or for studying the curvature of neural network training
    objectives (e.g. [1](https://arxiv.org/abs/1406.2572), [2](https://arxiv.org/abs/1811.07062),
    [3](https://arxiv.org/abs/1706.04454), [4](https://arxiv.org/abs/1802.03451)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在最小化平滑凸函数的[截断牛顿共轭梯度算法](https://en.wikipedia.org/wiki/Truncated_Newton_method)或研究神经网络训练目标的曲率（例如[1](https://arxiv.org/abs/1406.2572)，[2](https://arxiv.org/abs/1811.07062)，[3](https://arxiv.org/abs/1706.04454)，[4](https://arxiv.org/abs/1802.03451)）中，Hessian
    向量乘积函数非常有用。
- en: 'For a scalar-valued function \(f : \mathbb{R}^n \to \mathbb{R}\) with continuous
    second derivatives (so that the Hessian matrix is symmetric), the Hessian at a
    point \(x \in \mathbb{R}^n\) is written as \(\partial² f(x)\). A Hessian-vector
    product function is then able to evaluate'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '对于一个标量值函数 \( f : \mathbb{R}^n \to \mathbb{R} \)，具有连续的二阶导数（因此 Hessian 矩阵是对称的），点
    \( x \in \mathbb{R}^n \) 处的 Hessian 被写为 \(\partial² f(x)\)。然后，Hessian 向量乘积函数能够评估'
- en: \(\qquad v \mapsto \partial² f(x) \cdot v\)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \(\qquad v \mapsto \partial² f(x) \cdot v\)
- en: for any \(v \in \mathbb{R}^n\).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意 \( v \in \mathbb{R}^n \)。
- en: 'The trick is not to instantiate the full Hessian matrix: if \(n\) is large,
    perhaps in the millions or billions in the context of neural networks, then that
    might be impossible to store.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 窍门在于不要实例化完整的 Hessian 矩阵：如果 \( n \) 很大，例如在神经网络的背景下可能是百万或十亿级别，那么可能无法存储。
- en: Luckily, `grad` already gives us a way to write an efficient Hessian-vector
    product function. We just have to use the identity
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`grad` 已经为我们提供了一种编写高效的 Hessian 向量乘积函数的方法。我们只需使用下面的身份证
- en: \(\qquad \partial² f (x) v = \partial [x \mapsto \partial f(x) \cdot v] = \partial
    g(x)\),
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \(\qquad \partial² f (x) v = \partial [x \mapsto \partial f(x) \cdot v] = \partial
    g(x)\)，
- en: where \(g(x) = \partial f(x) \cdot v\) is a new scalar-valued function that
    dots the gradient of \(f\) at \(x\) with the vector \(v\). Notice that we’re only
    ever differentiating scalar-valued functions of vector-valued arguments, which
    is exactly where we know `grad` is efficient.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \( g(x) = \partial f(x) \cdot v \) 是一个新的标量值函数，它将 \( f \) 在 \( x \) 处的梯度与向量
    \( v \) 点乘。请注意，我们只对向量值参数的标量值函数进行微分，这正是我们知道 `grad` 高效的地方。
- en: 'In JAX code, we can just write this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JAX 代码中，我们可以直接写成这样：
- en: '[PRE15]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This example shows that you can freely use lexical closure, and JAX will never
    get perturbed or confused.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，您可以自由使用词汇闭包，而 JAX 绝不会感到不安或困惑。
- en: We’ll check this implementation a few cells down, once we see how to compute
    dense Hessian matrices. We’ll also write an even better version that uses both
    forward-mode and reverse-mode.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们看到如何计算密集的 Hessian 矩阵，我们将在几个单元格下检查此实现。我们还将编写一个更好的版本，该版本同时使用前向模式和反向模式。
- en: Jacobians and Hessians using `jacfwd` and `jacrev`
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `jacfwd` 和 `jacrev` 计算 Jacobians 和 Hessians
- en: 'You can compute full Jacobian matrices using the `jacfwd` and `jacrev` functions:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `jacfwd` 和 `jacrev` 函数计算完整的 Jacobian 矩阵：
- en: '[PRE16]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'These two functions compute the same values (up to machine numerics), but differ
    in their implementation: `jacfwd` uses forward-mode automatic differentiation,
    which is more efficient for “tall” Jacobian matrices, while `jacrev` uses reverse-mode,
    which is more efficient for “wide” Jacobian matrices. For matrices that are near-square,
    `jacfwd` probably has an edge over `jacrev`.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数计算相同的值（直到机器数学），但它们在实现上有所不同：`jacfwd` 使用前向模式自动微分，对于“高”的 Jacobian 矩阵更有效，而
    `jacrev` 使用反向模式，对于“宽”的 Jacobian 矩阵更有效。对于接近正方形的矩阵，`jacfwd` 可能比 `jacrev` 有优势。
- en: 'You can also use `jacfwd` and `jacrev` with container types:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在容器类型中使用 `jacfwd` 和 `jacrev`：
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For more details on forward- and reverse-mode, as well as how to implement `jacfwd`
    and `jacrev` as efficiently as possible, read on!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关于前向模式和反向模式的更多细节，以及如何尽可能高效地实现 `jacfwd` 和 `jacrev`，请继续阅读！
- en: 'Using a composition of two of these functions gives us a way to compute dense
    Hessian matrices:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个这些函数的复合给我们一种计算密集的 Hessian 矩阵的方法：
- en: '[PRE20]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This shape makes sense: if we start with a function \(f : \mathbb{R}^n \to
    \mathbb{R}^m\), then at a point \(x \in \mathbb{R}^n\) we expect to get the shapes'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '这种形状是合理的：如果我们从一个函数 \(f : \mathbb{R}^n \to \mathbb{R}^m\) 开始，那么在点 \(x \in \mathbb{R}^n\)
    我们期望得到以下形状'
- en: \(f(x) \in \mathbb{R}^m\), the value of \(f\) at \(x\),
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(f(x) \in \mathbb{R}^m\)，在 \(x\) 处的 \(f\) 的值，
- en: \(\partial f(x) \in \mathbb{R}^{m \times n}\), the Jacobian matrix at \(x\),
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\partial f(x) \in \mathbb{R}^{m \times n}\)，在 \(x\) 处的雅可比矩阵，
- en: \(\partial² f(x) \in \mathbb{R}^{m \times n \times n}\), the Hessian at \(x\),
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\partial² f(x) \in \mathbb{R}^{m \times n \times n}\)，在 \(x\) 处的 Hessian 矩阵，
- en: and so on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以及其他一些内容。
- en: 'To implement `hessian`, we could have used `jacfwd(jacrev(f))` or `jacrev(jacfwd(f))`
    or any other composition of the two. But forward-over-reverse is typically the
    most efficient. That’s because in the inner Jacobian computation we’re often differentiating
    a function wide Jacobian (maybe like a loss function \(f : \mathbb{R}^n \to \mathbb{R}\)),
    while in the outer Jacobian computation we’re differentiating a function with
    a square Jacobian (since \(\nabla f : \mathbb{R}^n \to \mathbb{R}^n\)), which
    is where forward-mode wins out.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '要实现 `hessian`，我们可以使用 `jacfwd(jacrev(f))` 或 `jacrev(jacfwd(f))` 或这两者的任何组合。但是前向超过反向通常是最有效的。这是因为在内部雅可比计算中，我们通常是在不同iating
    一个函数宽雅可比（也许像损失函数 \(f : \mathbb{R}^n \to \mathbb{R}\)），而在外部雅可比计算中，我们是在不同iating
    具有方雅可比的函数（因为 \(\nabla f : \mathbb{R}^n \to \mathbb{R}^n\)），这就是前向模式胜出的地方。'
- en: 'How it’s made: two foundational autodiff functions'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制造过程：两个基础的自动微分函数
- en: '### Jacobian-Vector products (JVPs, aka forward-mode autodiff)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '### 雅可比-向量积（JVPs，也称为前向模式自动微分）'
- en: JAX includes efficient and general implementations of both forward- and reverse-mode
    automatic differentiation. The familiar `grad` function is built on reverse-mode,
    but to explain the difference in the two modes, and when each can be useful, we
    need a bit of math background.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: JAX 包括前向模式和反向模式自动微分的高效和通用实现。熟悉的 `grad` 函数建立在反向模式之上，但要解释两种模式的区别，以及每种模式何时有用，我们需要一些数学背景。
- en: JVPs in math
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学中的雅可比向量积
- en: 'Mathematically, given a function \(f : \mathbb{R}^n \to \mathbb{R}^m\), the
    Jacobian of \(f\) evaluated at an input point \(x \in \mathbb{R}^n\), denoted
    \(\partial f(x)\), is often thought of as a matrix in \(\mathbb{R}^m \times \mathbb{R}^n\):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '在数学上，给定一个函数 \(f : \mathbb{R}^n \to \mathbb{R}^m\)，在输入点 \(x \in \mathbb{R}^n\)
    处评估的雅可比矩阵 \(\partial f(x)\)，通常被视为一个 \(\mathbb{R}^m \times \mathbb{R}^n\) 中的矩阵：'
- en: \(\qquad \partial f(x) \in \mathbb{R}^{m \times n}\).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \(\qquad \partial f(x) \in \mathbb{R}^{m \times n}\).
- en: 'But we can also think of \(\partial f(x)\) as a linear map, which maps the
    tangent space of the domain of \(f\) at the point \(x\) (which is just another
    copy of \(\mathbb{R}^n\)) to the tangent space of the codomain of \(f\) at the
    point \(f(x)\) (a copy of \(\mathbb{R}^m\)):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也可以将 \(\partial f(x)\) 看作是一个线性映射，它将 \(f\) 的定义域在点 \(x\) 的切空间（即另一个 \(\mathbb{R}^n\)
    的副本）映射到 \(f\) 的值域在点 \(f(x)\) 的切空间（一个 \(\mathbb{R}^m\) 的副本）：
- en: '\(\qquad \partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '\(\qquad \partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\).'
- en: This map is called the [pushforward map](https://en.wikipedia.org/wiki/Pushforward_(differential))
    of \(f\) at \(x\). The Jacobian matrix is just the matrix for this linear map
    in a standard basis.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此映射称为 \(f\) 在 \(x\) 处的[推前映射](https://en.wikipedia.org/wiki/Pushforward_(differential))。雅可比矩阵只是标准基中这个线性映射的矩阵。
- en: 'If we don’t commit to one specific input point \(x\), then we can think of
    the function \(\partial f\) as first taking an input point and returning the Jacobian
    linear map at that input point:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不确定一个特定的输入点 \(x\)，那么我们可以将函数 \(\partial f\) 视为首先接受一个输入点并返回该输入点处的雅可比线性映射：
- en: '\(\qquad \partial f : \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m\).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '\(\qquad \partial f : \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m\)。'
- en: In particular, we can uncurry things so that given input point \(x \in \mathbb{R}^n\)
    and a tangent vector \(v \in \mathbb{R}^n\), we get back an output tangent vector
    in \(\mathbb{R}^m\). We call that mapping, from \((x, v)\) pairs to output tangent
    vectors, the *Jacobian-vector product*, and write it as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们可以解开事物，这样给定输入点 \(x \in \mathbb{R}^n\) 和切向量 \(v \in \mathbb{R}^n\)，我们得到一个输出切向量在
    \(\mathbb{R}^m\) 中。我们称这种映射，从 \((x, v)\) 对到输出切向量，为*雅可比向量积*，并将其写为
- en: \(\qquad (x, v) \mapsto \partial f(x) v\)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \(\qquad (x, v) \mapsto \partial f(x) v\)
- en: JVPs in JAX code
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: JAX 代码中的雅可比向量积
- en: Back in Python code, JAX’s `jvp` function models this transformation. Given
    a Python function that evaluates \(f\), JAX’s `jvp` is a way to get a Python function
    for evaluating \((x, v) \mapsto (f(x), \partial f(x) v)\).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 回到Python代码中，JAX的 `jvp` 函数模拟了这种转换。给定一个评估 \(f\) 的Python函数，JAX的 `jvp` 是获取评估 \((x,
    v) \mapsto (f(x), \partial f(x) v)\) 的Python函数的一种方法。
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature),
    we could write
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 用类似Haskell的类型签名来说，我们可以写成
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: where we use `T a` to denote the type of the tangent space for `a`. In words,
    `jvp` takes as arguments a function of type `a -> b`, a value of type `a`, and
    a tangent vector value of type `T a`. It gives back a pair consisting of a value
    of type `b` and an output tangent vector of type `T b`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `T a` 来表示 `a` 的切线空间的类型。简言之，`jvp` 接受一个类型为 `a -> b` 的函数作为参数，一个类型为 `a`
    的值，以及一个类型为 `T a` 的切线向量值。它返回一个由类型为 `b` 的值和类型为 `T b` 的输出切线向量组成的对。
- en: The `jvp`-transformed function is evaluated much like the original function,
    but paired up with each primal value of type `a` it pushes along tangent values
    of type `T a`. For each primitive numerical operation that the original function
    would have applied, the `jvp`-transformed function executes a “JVP rule” for that
    primitive that both evaluates the primitive on the primals and applies the primitive’s
    JVP at those primal values.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`jvp` 转换后的函数的评估方式与原函数类似，但与每个类型为 `a` 的原始值配对时，它会沿着类型为 `T a` 的切线值进行推进。对于原始函数将应用的每个原始数值操作，`jvp`
    转换后的函数会执行一个“JVP 规则”，该规则同时在这些原始值上评估原始数值，并应用其JVP。'
- en: 'That evaluation strategy has some immediate implications about computational
    complexity: since we evaluate JVPs as we go, we don’t need to store anything for
    later, and so the memory cost is independent of the depth of the computation.
    In addition, the FLOP cost of the `jvp`-transformed function is about 3x the cost
    of just evaluating the function (one unit of work for evaluating the original
    function, for example `sin(x)`; one unit for linearizing, like `cos(x)`; and one
    unit for applying the linearized function to a vector, like `cos_x * v`). Put
    another way, for a fixed primal point \(x\), we can evaluate \(v \mapsto \partial
    f(x) \cdot v\) for about the same marginal cost as evaluating \(f\).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该评估策略对计算复杂度有一些直接影响：由于我们在进行评估时同时评估JVP，因此我们不需要为以后存储任何内容，因此内存成本与计算深度无关。此外，`jvp`
    转换后的函数的FLOP成本约为评估函数的成本的3倍（例如对于评估原始函数的一个单位工作，如 `sin(x)`；一个单位用于线性化，如 `cos(x)`；和一个单位用于将线性化函数应用于向量，如
    `cos_x * v`）。换句话说，对于固定的原始点 \(x\)，我们可以以大致相同的边际成本评估 \(v \mapsto \partial f(x) \cdot
    v\)，如同评估 \(f\) 一样。
- en: That memory complexity sounds pretty compelling! So why don’t we see forward-mode
    very often in machine learning?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 那么内存复杂度听起来非常有说服力！那为什么我们在机器学习中很少见到正向模式呢？
- en: To answer that, first think about how you could use a JVP to build a full Jacobian
    matrix. If we apply a JVP to a one-hot tangent vector, it reveals one column of
    the Jacobian matrix, corresponding to the nonzero entry we fed in. So we can build
    a full Jacobian one column at a time, and to get each column costs about the same
    as one function evaluation. That will be efficient for functions with “tall” Jacobians,
    but inefficient for “wide” Jacobians.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，首先考虑如何使用JVP构建完整的Jacobian矩阵。如果我们将JVP应用于一个单位切线向量，它会显示出我们输入的非零条目对应的Jacobian矩阵的一列。因此，我们可以逐列地构建完整的Jacobian矩阵，获取每列的成本大约与一个函数评估相同。对于具有“高”Jacobian的函数来说，这将是高效的，但对于“宽”Jacobian来说则效率低下。
- en: 'If you’re doing gradient-based optimization in machine learning, you probably
    want to minimize a loss function from parameters in \(\mathbb{R}^n\) to a scalar
    loss value in \(\mathbb{R}\). That means the Jacobian of this function is a very
    wide matrix: \(\partial f(x) \in \mathbb{R}^{1 \times n}\), which we often identify
    with the Gradient vector \(\nabla f(x) \in \mathbb{R}^n\). Building that matrix
    one column at a time, with each call taking a similar number of FLOPs to evaluate
    the original function, sure seems inefficient! In particular, for training neural
    networks, where \(f\) is a training loss function and \(n\) can be in the millions
    or billions, this approach just won’t scale.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在机器学习中进行基于梯度的优化，你可能想要最小化一个从 \(\mathbb{R}^n\) 中的参数到 \(\mathbb{R}\) 中标量损失值的损失函数。这意味着这个函数的雅可比矩阵是一个非常宽的矩阵：\(\partial
    f(x) \in \mathbb{R}^{1 \times n}\)，我们通常将其视为梯度向量 \(\nabla f(x) \in \mathbb{R}^n\)。逐列构建这个矩阵，每次调用需要类似数量的浮点运算来评估原始函数，看起来确实效率低下！特别是对于训练神经网络，其中
    \(f\) 是一个训练损失函数，而 \(n\) 可以是百万或十亿级别，这种方法根本不可扩展。
- en: 'To do better for functions like this, we just need to use reverse-mode.  ###
    Vector-Jacobian products (VJPs, aka reverse-mode autodiff)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地处理这类函数，我们只需要使用反向模式。### 向量-雅可比积（VJPs，又称反向自动微分）
- en: Where forward-mode gives us back a function for evaluating Jacobian-vector products,
    which we can then use to build Jacobian matrices one column at a time, reverse-mode
    is a way to get back a function for evaluating vector-Jacobian products (equivalently
    Jacobian-transpose-vector products), which we can use to build Jacobian matrices
    one row at a time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向模式中，我们得到了一个用于评估雅可比向量积的函数，然后我们可以使用它逐列构建雅可比矩阵；而反向模式则是一种获取用于评估向量-雅可比积（或等效地雅可比-转置向量积）的函数的方式，我们可以用它逐行构建雅可比矩阵。
- en: VJPs in math
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学中的 VJPs
- en: 'Let’s again consider a function \(f : \mathbb{R}^n \to \mathbb{R}^m\). Starting
    from our notation for JVPs, the notation for VJPs is pretty simple:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '再次考虑一个函数 \(f : \mathbb{R}^n \to \mathbb{R}^m\)。从我们对 JVP 的表示开始，对于 VJP 的表示非常简单：'
- en: \(\qquad (x, v) \mapsto v \partial f(x)\),
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \(\qquad (x, v) \mapsto v \partial f(x)\),
- en: 'where \(v\) is an element of the cotangent space of \(f\) at \(x\) (isomorphic
    to another copy of \(\mathbb{R}^m\)). When being rigorous, we should think of
    \(v\) as a linear map \(v : \mathbb{R}^m \to \mathbb{R}\), and when we write \(v
    \partial f(x)\) we mean function composition \(v \circ \partial f(x)\), where
    the types work out because \(\partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\).
    But in the common case we can identify \(v\) with a vector in \(\mathbb{R}^m\)
    and use the two almost interchangeably, just like we might sometimes flip between
    “column vectors” and “row vectors” without much comment.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 \(v\) 是在 \(x\) 处 \(f\) 的余切空间的元素（同构于另一个 \(\mathbb{R}^m\) 的副本）。在严格时，我们应该将
    \(v\) 视为一个线性映射 \(v : \mathbb{R}^m \to \mathbb{R}\)，当我们写 \(v \partial f(x)\) 时，我们意味着函数复合
    \(v \circ \partial f(x)\)，其中类型之间的对应关系是因为 \(\partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\)。但在通常情况下，我们可以将
    \(v\) 与 \(\mathbb{R}^m\) 中的一个向量等同看待，并几乎可以互换使用，就像有时我们可以在“列向量”和“行向量”之间轻松切换而不加过多评论一样。'
- en: 'With that identification, we can alternatively think of the linear part of
    a VJP as the transpose (or adjoint conjugate) of the linear part of a JVP:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个认识，我们可以将 VJP 的线性部分看作是 JVP 线性部分的转置（或共轭伴随）：
- en: \(\qquad (x, v) \mapsto \partial f(x)^\mathsf{T} v\).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \(\qquad (x, v) \mapsto \partial f(x)^\mathsf{T} v\).
- en: For a given point \(x\), we can write the signature as
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定点 \(x\)，我们可以将签名写为
- en: '\(\qquad \partial f(x)^\mathsf{T} : \mathbb{R}^m \to \mathbb{R}^n\).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '\(\qquad \partial f(x)^\mathsf{T} : \mathbb{R}^m \to \mathbb{R}^n\).'
- en: The corresponding map on cotangent spaces is often called the [pullback](https://en.wikipedia.org/wiki/Pullback_(differential_geometry))
    of \(f\) at \(x\). The key for our purposes is that it goes from something that
    looks like the output of \(f\) to something that looks like the input of \(f\),
    just like we might expect from a transposed linear function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的余切空间映射通常称为\[拉回](https://en.wikipedia.org/wiki/Pullback_(differential_geometry))
    \(f\) 在 \(x\) 处的。对我们而言，关键在于它从类似 \(f\) 输出的东西到类似 \(f\) 输入的东西，就像我们从一个转置线性函数所期望的那样。
- en: VJPs in JAX code
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: JAX 代码中的 VJPs
- en: Switching from math back to Python, the JAX function `vjp` can take a Python
    function for evaluating \(f\) and give us back a Python function for evaluating
    the VJP \((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学切换回 Python，JAX 函数 `vjp` 可以接受一个用于评估 \(f\) 的 Python 函数，并给我们返回一个用于评估 VJP \((x,
    v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\) 的 Python 函数。
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature),
    we could write
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就[类似 Haskell 类型签名](https://wiki.haskell.org/Type_signature)的形式来说，我们可以写成
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: where we use `CT a` to denote the type for the cotangent space for `a`. In words,
    `vjp` takes as arguments a function of type `a -> b` and a point of type `a`,
    and gives back a pair consisting of a value of type `b` and a linear map of type
    `CT b -> CT a`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `CT a` 表示 `a` 的余切空间的类型。换句话说，`vjp` 接受类型为 `a -> b` 的函数和类型为 `a` 的点作为参数，并返回一个由类型为
    `b` 的值和类型为 `CT b -> CT a` 的线性映射组成的对。
- en: 'This is great because it lets us build Jacobian matrices one row at a time,
    and the FLOP cost for evaluating \((x, v) \mapsto (f(x), v^\mathsf{T} \partial
    f(x))\) is only about three times the cost of evaluating \(f\). In particular,
    if we want the gradient of a function \(f : \mathbb{R}^n \to \mathbb{R}\), we
    can do it in just one call. That’s how `grad` is efficient for gradient-based
    optimization, even for objectives like neural network training loss functions
    on millions or billions of parameters.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '这很棒，因为它让我们一次一行地构建雅可比矩阵，并且评估 \((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\)
    的 FLOP 成本仅约为评估 \(f\) 的三倍。特别是，如果我们想要函数 \(f : \mathbb{R}^n \to \mathbb{R}\) 的梯度，我们可以一次性完成。这就是
    `grad` 对基于梯度的优化非常高效的原因，即使是对于数百万或数十亿个参数的神经网络训练损失函数这样的目标。'
- en: 'There’s a cost, though: though the FLOPs are friendly, memory scales with the
    depth of the computation. Also, the implementation is traditionally more complex
    than that of forward-mode, though JAX has some tricks up its sleeve (that’s a
    story for a future notebook!).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个成本：虽然 FLOP 友好，但内存随计算深度而增加。而且，该实现在传统上比前向模式更为复杂，但 JAX 对此有一些窍门（这是未来笔记本的故事！）。
- en: For more on how reverse-mode works, see [this tutorial video from the Deep Learning
    Summer School in 2017](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 关于反向模式的工作原理，可以查看[2017 年深度学习暑期学校的教程视频](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)。
- en: Vector-valued gradients with VJPs
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 VJPs 的矢量值梯度
- en: 'If you’re interested in taking vector-valued gradients (like `tf.gradients`):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对使用矢量值梯度（如 `tf.gradients`）感兴趣：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Hessian-vector products using both forward- and reverse-mode
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用前向和反向模式的黑塞矢量积
- en: 'In a previous section, we implemented a Hessian-vector product function just
    using reverse-mode (assuming continuous second derivatives):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们仅使用反向模式实现了一个黑塞-矢量积函数（假设具有连续二阶导数）：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: That’s efficient, but we can do even better and save some memory by using forward-mode
    together with reverse-mode.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是高效的，但我们甚至可以更好地节省一些内存，通过使用前向模式和反向模式。
- en: 'Mathematically, given a function \(f : \mathbb{R}^n \to \mathbb{R}\) to differentiate,
    a point \(x \in \mathbb{R}^n\) at which to linearize the function, and a vector
    \(v \in \mathbb{R}^n\), the Hessian-vector product function we want is'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '从数学上讲，给定一个要区分的函数 \(f : \mathbb{R}^n \to \mathbb{R}\)，要线性化函数的一个点 \(x \in \mathbb{R}^n\)，以及一个向量
    \(v \in \mathbb{R}^n\)，我们想要的黑塞-矢量积函数是'
- en: \((x, v) \mapsto \partial² f(x) v\)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \((x, v) \mapsto \partial² f(x) v\)
- en: 'Consider the helper function \(g : \mathbb{R}^n \to \mathbb{R}^n\) defined
    to be the derivative (or gradient) of \(f\), namely \(g(x) = \partial f(x)\).
    All we need is its JVP, since that will give us'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑助手函数 \(g : \mathbb{R}^n \to \mathbb{R}^n\) 定义为 \(f\) 的导数（或梯度），即 \(g(x) =
    \partial f(x)\)。我们所需的只是它的 JVP，因为这将给我们'
- en: \((x, v) \mapsto \partial g(x) v = \partial² f(x) v\).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \((x, v) \mapsto \partial g(x) v = \partial² f(x) v\).
- en: 'We can translate that almost directly into code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎可以直接将其转换为代码：
- en: '[PRE29]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Even better, since we didn’t have to call `jnp.dot` directly, this `hvp` function
    works with arrays of any shape and with arbitrary container types (like vectors
    stored as nested lists/dicts/tuples), and doesn’t even have a dependence on `jax.numpy`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，由于我们不需要直接调用 `jnp.dot`，这个 `hvp` 函数可以处理任何形状的数组以及任意的容器类型（如嵌套列表/字典/元组中存储的向量），甚至与`jax.numpy`
    没有任何依赖。
- en: 'Here’s an example of how to use it:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何使用它的示例：
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Another way you might consider writing this is using reverse-over-forward:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种你可能考虑写这个的方法是使用反向-前向模式：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'That’s not quite as good, though, because forward-mode has less overhead than
    reverse-mode, and since the outer differentiation operator here has to differentiate
    a larger computation than the inner one, keeping forward-mode on the outside works
    best:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这不是很好，因为前向模式的开销比反向模式小，由于外部区分算子要区分比内部更大的计算，将前向模式保持在外部是最好的：
- en: '[PRE33]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Composing VJPs, JVPs, and `vmap`
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组成 VJP、JVP 和 `vmap`
- en: Jacobian-Matrix and Matrix-Jacobian products
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 雅可比-矩阵和矩阵-雅可比乘积
- en: Now that we have `jvp` and `vjp` transformations that give us functions to push-forward
    or pull-back single vectors at a time, we can use JAX’s `vmap` [transformation](https://github.com/google/jax#auto-vectorization-with-vmap)
    to push and pull entire bases at once. In particular, we can use that to write
    fast matrix-Jacobian and Jacobian-matrix products.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有`jvp`和`vjp`变换，它们为我们提供了推送或拉回单个向量的函数，我们可以使用JAX的`vmap` [变换](https://github.com/google/jax#auto-vectorization-with-vmap)一次推送和拉回整个基。特别是，我们可以用它来快速编写矩阵-雅可比和雅可比-矩阵乘积。
- en: '[PRE35]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The implementation of `jacfwd` and `jacrev`
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`jacfwd`和`jacrev`的实现'
- en: Now that we’ve seen fast Jacobian-matrix and matrix-Jacobian products, it’s
    not hard to guess how to write `jacfwd` and `jacrev`. We just use the same technique
    to push-forward or pull-back an entire standard basis (isomorphic to an identity
    matrix) at once.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了快速的雅可比-矩阵和矩阵-雅可比乘积，写出`jacfwd`和`jacrev`并不难。我们只需使用相同的技术一次推送或拉回整个标准基（等同于单位矩阵）。
- en: '[PRE40]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Interestingly, [Autograd](https://github.com/hips/autograd) couldn’t do this.
    Our [implementation](https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/differential_operators.py#L60)
    of reverse-mode `jacobian` in Autograd had to pull back one vector at a time with
    an outer-loop `map`. Pushing one vector at a time through the computation is much
    less efficient than batching it all together with `vmap`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，[Autograd](https://github.com/hips/autograd)做不到这一点。我们在Autograd中反向模式`jacobian`的[实现](https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/differential_operators.py#L60)必须逐个向量地拉回，使用外层循环`map`。逐个向量地通过计算远不及使用`vmap`一次将所有内容批处理高效。
- en: 'Another thing that Autograd couldn’t do is `jit`. Interestingly, no matter
    how much Python dynamism you use in your function to be differentiated, we could
    always use `jit` on the linear part of the computation. For example:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件Autograd做不到的事情是`jit`。有趣的是，无论您在要进行微分的函数中使用多少Python动态性，我们总是可以在计算的线性部分上使用`jit`。例如：
- en: '[PRE42]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Complex numbers and differentiation
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复数和微分
- en: JAX is great at complex numbers and differentiation. To support both [holomorphic
    and non-holomorphic differentiation](https://en.wikipedia.org/wiki/Holomorphic_function),
    it helps to think in terms of JVPs and VJPs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: JAX在复数和微分方面表现出色。为了支持[全纯和非全纯微分](https://en.wikipedia.org/wiki/Holomorphic_function)，理解JVP和VJP很有帮助。
- en: 'Consider a complex-to-complex function \(f: \mathbb{C} \to \mathbb{C}\) and
    identify it with a corresponding function \(g: \mathbb{R}² \to \mathbb{R}²\),'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑一个复到复的函数 \(f: \mathbb{C} \to \mathbb{C}\) 并将其与相应的函数 \(g: \mathbb{R}² \to
    \mathbb{R}²\) 对应起来，'
- en: '[PRE44]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: That is, we’ve decomposed \(f(z) = u(x, y) + v(x, y) i\) where \(z = x + y i\),
    and identified \(\mathbb{C}\) with \(\mathbb{R}²\) to get \(g\).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们分解了 \(f(z) = u(x, y) + v(x, y) i\) 其中 \(z = x + y i\)，并将 \(\mathbb{C}\)
    与 \(\mathbb{R}²\) 对应起来得到了 \(g\)。
- en: Since \(g\) only involves real inputs and outputs, we already know how to write
    a Jacobian-vector product for it, say given a tangent vector \((c, d) \in \mathbb{R}²\),
    namely
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(g\) 只涉及实数输入和输出，我们已经知道如何为它编写雅可比-向量积，例如给定切向量 \((c, d) \in \mathbb{R}²\)，
- en: \(\begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0 v(x,
    y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \(\begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0 v(x,
    y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).
- en: To get a JVP for the original function \(f\) applied to a tangent vector \(c
    + di \in \mathbb{C}\), we just use the same definition and identify the result
    as another complex number,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得应用于切向量 \(c + di \in \mathbb{C}\) 的原始函数 \(f\) 的JVP，我们只需使用相同的定义，并将结果标识为另一个复数，
- en: \(\partial f(x + y i)(c + d i) = \begin{matrix} \begin{bmatrix} 1 & i \end{bmatrix}
    \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0
    v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \(\partial f(x + y i)(c + d i) = \begin{matrix} \begin{bmatrix} 1 & i \end{bmatrix}
    \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0
    v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).
- en: 'That’s our definition of the JVP of a \(\mathbb{C} \to \mathbb{C}\) function!
    Notice it doesn’t matter whether or not \(f\) is holomorphic: the JVP is unambiguous.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对复到复函数 \(f\) 的JVP的定义！注意，无论 \(f\) 是否全纯，JVP都是明确的。
- en: 'Here’s a check:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个检查：
- en: '[PRE45]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'What about VJPs? We do something pretty similar: for a cotangent vector \(c
    + di \in \mathbb{C}\) we define the VJP of \(f\) as'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那么VJP呢？我们做了类似的事情：对于余切向量 \(c + di \in \mathbb{C}\)，我们将 \(f\) 的VJP定义为
- en: \((c + di)^* \; \partial f(x + y i) = \begin{matrix} \begin{bmatrix} c & -d
    \end{bmatrix} \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1
    u(x, y) \\ \partial_0 v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix}
    1 \\ -i \end{bmatrix}\).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \((c + di)^* \; \partial f(x + y i) = \begin{matrix} \begin{bmatrix} c & -d
    \end{bmatrix} \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1
    u(x, y) \\ \partial_0 v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix}
    1 \\ -i \end{bmatrix}\).
- en: What’s with the negatives? They’re just to take care of complex conjugation,
    and the fact that we’re working with covectors.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要有负号？这些只是为了处理复共轭，以及我们正在处理余切向量的事实。
- en: 'Here’s a check of the VJP rules:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是VJP规则的检查：
- en: '[PRE48]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: What about convenience wrappers like `grad`, `jacfwd`, and `jacrev`?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的包装器如`grad`、`jacfwd`和`jacrev`有什么作用？
- en: 'For \(\mathbb{R} \to \mathbb{R}\) functions, recall we defined `grad(f)(x)`
    as being `vjp(f, x)1`, which works because applying a VJP to a `1.0` value reveals
    the gradient (i.e. Jacobian, or derivative). We can do the same thing for \(\mathbb{C}
    \to \mathbb{R}\) functions: we can still use `1.0` as the cotangent vector, and
    we just get out a complex number result summarizing the full Jacobian:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于\(\mathbb{R} \to \mathbb{R}\)函数，回想我们定义`grad(f)(x)`为`vjp(f, x)1`，这是因为将VJP应用于`1.0`值会显示梯度（即雅可比矩阵或导数）。对于\(\mathbb{C}
    \to \mathbb{R}\)函数，我们可以做同样的事情：我们仍然可以使用`1.0`作为余切向量，而我们得到的只是总结完整雅可比矩阵的一个复数结果：
- en: '[PRE50]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: For general \(\mathbb{C} \to \mathbb{C}\) functions, the Jacobian has 4 real-valued
    degrees of freedom (as in the 2x2 Jacobian matrices above), so we can’t hope to
    represent all of them within a complex number. But we can for holomorphic functions!
    A holomorphic function is precisely a \(\mathbb{C} \to \mathbb{C}\) function with
    the special property that its derivative can be represented as a single complex
    number. (The [Cauchy-Riemann equations](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations)
    ensure that the above 2x2 Jacobians have the special form of a scale-and-rotate
    matrix in the complex plane, i.e. the action of a single complex number under
    multiplication.) And we can reveal that one complex number using a single call
    to `vjp` with a covector of `1.0`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的\(\mathbb{C} \to \mathbb{C}\)函数，雅可比矩阵有4个实值自由度（如上面的2x2雅可比矩阵），因此我们不能希望在一个复数中表示所有这些自由度。但对于全纯函数，我们可以！全纯函数恰好是一个\(\mathbb{C}
    \to \mathbb{C}\)函数，其导数可以表示为一个单一的复数。（[柯西-黎曼方程](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations)确保上述2x2雅可比矩阵在复平面内的作用具有复数乘法下的比例和旋转矩阵的特殊形式。）我们可以使用一个`vjp`调用并带有`1.0`的余切向量来揭示那一个复数。
- en: 'Because this only works for holomorphic functions, to use this trick we need
    to promise JAX that our function is holomorphic; otherwise, JAX will raise an
    error when `grad` is used for a complex-output function:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这仅适用于全纯函数，为了使用这个技巧，我们需要向JAX保证我们的函数是全纯的；否则，在复数输出函数上使用`grad`时，JAX会引发错误：
- en: '[PRE52]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'All the `holomorphic=True` promise does is disable the error when the output
    is complex-valued. We can still write `holomorphic=True` when the function isn’t
    holomorphic, but the answer we get out won’t represent the full Jacobian. Instead,
    it’ll be the Jacobian of the function where we just discard the imaginary part
    of the output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`holomorphic=True`的承诺仅仅是在输出是复数值时禁用错误。当函数不是全纯时，我们仍然可以写`holomorphic=True`，但得到的答案将不表示完整的雅可比矩阵。相反，它将是在我们只丢弃输出的虚部的函数的雅可比矩阵。'
- en: '[PRE54]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'There are some useful upshots for how `grad` works here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里`grad`的工作有一些有用的结论：
- en: We can use `grad` on holomorphic \(\mathbb{C} \to \mathbb{C}\) functions.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在全纯的\(\mathbb{C} \to \mathbb{C}\)函数上使用`grad`。
- en: 'We can use `grad` to optimize \(f : \mathbb{C} \to \mathbb{R}\) functions,
    like real-valued loss functions of complex parameters `x`, by taking steps in
    the direction of the conjugate of `grad(f)(x)`.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`grad`来优化\(\mathbb{C} \to \mathbb{R}\)函数，例如复参数`x`的实值损失函数，通过朝着`grad(f)(x)`的共轭方向迈出步伐。
- en: If we have an \(\mathbb{R} \to \mathbb{R}\) function that just happens to use
    some complex-valued operations internally (some of which must be non-holomorphic,
    e.g. FFTs used in convolutions) then `grad` still works and we get the same result
    that an implementation using only real values would have given.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们有一个\(\mathbb{R} \to \mathbb{R}\)的函数，它恰好在内部使用一些复数运算（其中一些必须是非全纯的，例如在卷积中使用的FFT），那么`grad`仍然有效，并且我们得到与仅使用实数值的实现相同的结果。
- en: In any case, JVPs and VJPs are always unambiguous. And if we wanted to compute
    the full Jacobian matrix of a non-holomorphic \(\mathbb{C} \to \mathbb{C}\) function,
    we can do it with JVPs or VJPs!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，JVPs和VJPs都是明确的。如果我们想计算非全纯函数\(\mathbb{C} \to \mathbb{C}\)的完整Jacobian矩阵，我们可以用JVPs或VJPs来做到！
- en: 'You should expect complex numbers to work everywhere in JAX. Here’s differentiating
    through a Cholesky decomposition of a complex matrix:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该期望复数在JAX中的任何地方都能正常工作。这里是通过复杂矩阵的Cholesky分解进行微分：
- en: '[PRE56]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: More advanced autodiff
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高级的自动微分
- en: In this notebook, we worked through some easy, and then progressively more complicated,
    applications of automatic differentiation in JAX. We hope you now feel that taking
    derivatives in JAX is easy and powerful.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本笔记本中，我们通过一些简单的，然后逐渐复杂的应用中，使用JAX中的自动微分。我们希望现在您感觉在JAX中进行导数运算既简单又强大。
- en: 'There’s a whole world of other autodiff tricks and functionality out there.
    Topics we didn’t cover, but hope to in an “Advanced Autodiff Cookbook” include:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 还有很多其他自动微分的技巧和功能。我们没有涵盖的主题，但希望在“高级自动微分手册”中进行涵盖：
- en: Gauss-Newton Vector Products, linearizing once
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯-牛顿向量乘积，一次线性化
- en: Custom VJPs and JVPs
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义的VJPs和JVPs
- en: Efficient derivatives at fixed-points
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在固定点处高效地求导
- en: Estimating the trace of a Hessian using random Hessian-vector products.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机的Hessian-vector products来估计Hessian的迹。
- en: Forward-mode autodiff using only reverse-mode autodiff.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用反向模式自动微分的前向模式自动微分。
- en: Taking derivatives with respect to custom data types.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对自定义数据类型进行导数计算。
- en: Checkpointing (binomial checkpointing for efficient reverse-mode, not model
    snapshotting).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点（二项式检查点用于高效的反向模式，而不是模型快照）。
- en: Optimizing VJPs with Jacobian pre-accumulation.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化VJPs通过Jacobian预积累。
