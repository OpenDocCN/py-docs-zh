- en: jax.nn module
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: jax.nn 模块
- en: 原文：[`jax.readthedocs.io/en/latest/jax.nn.html`](https://jax.readthedocs.io/en/latest/jax.nn.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/jax.nn.html`](https://jax.readthedocs.io/en/latest/jax.nn.html)
- en: '`jax.nn.initializers` module'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jax.nn.initializers` 模块'
- en: Common functions for neural network libraries.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络库常见函数。
- en: Activation functions
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: '| `relu` | Rectified linear unit activation function. |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| `relu` | 线性整流单元激活函数。 |'
- en: '| `relu6` | Rectified Linear Unit 6 activation function. |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| `relu6` | 线性整流单元6激活函数。 |'
- en: '| `sigmoid`(x) | Sigmoid activation function. |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| `sigmoid`(x) | Sigmoid激活函数。 |'
- en: '| `softplus`(x) | Softplus activation function. |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| `softplus`(x) | Softplus激活函数。 |'
- en: '| `sparse_plus`(x) | Sparse plus function. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| `sparse_plus`(x) | 稀疏加法函数。 |'
- en: '| `sparse_sigmoid`(x) | Sparse sigmoid activation function. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| `sparse_sigmoid`(x) | 稀疏Sigmoid激活函数。 |'
- en: '| `soft_sign`(x) | Soft-sign activation function. |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| `soft_sign`(x) | Soft-sign激活函数。 |'
- en: '| `silu`(x) | SiLU (aka swish) activation function. |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| `silu`(x) | SiLU（又称swish）激活函数。 |'
- en: '| `swish`(x) | SiLU (aka swish) activation function. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| `swish`(x) | SiLU（又称swish）激活函数。 |'
- en: '| `log_sigmoid`(x) | Log-sigmoid activation function. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `log_sigmoid`(x) | 对数Sigmoid激活函数。 |'
- en: '| `leaky_relu`(x[, negative_slope]) | Leaky rectified linear unit activation
    function. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `leaky_relu`(x[, negative_slope]) | 泄漏整流线性单元激活函数。 |'
- en: '| `hard_sigmoid`(x) | Hard Sigmoid activation function. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `hard_sigmoid`(x) | 硬Sigmoid激活函数。 |'
- en: '| `hard_silu`(x) | Hard SiLU (swish) activation function |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `hard_silu`(x) | 硬SiLU（swish）激活函数。 |'
- en: '| `hard_swish`(x) | Hard SiLU (swish) activation function |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `hard_swish`(x) | 硬SiLU（swish）激活函数。 |'
- en: '| `hard_tanh`(x) | Hard \(\mathrm{tanh}\) activation function. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `hard_tanh`(x) | 硬\tanh激活函数。 |'
- en: '| `elu`(x[, alpha]) | Exponential linear unit activation function. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `elu`(x[, alpha]) | 指数线性单元激活函数。 |'
- en: '| `celu`(x[, alpha]) | Continuously-differentiable exponential linear unit
    activation. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `celu`(x[, alpha]) | 连续可微的指数线性单元激活函数。 |'
- en: '| `selu`(x) | Scaled exponential linear unit activation. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `selu`(x) | 缩放的指数线性单元激活函数。 |'
- en: '| `gelu`(x[, approximate]) | Gaussian error linear unit activation function.
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `gelu`(x[, approximate]) | 高斯误差线性单元激活函数。 |'
- en: '| `glu`(x[, axis]) | Gated linear unit activation function. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `glu`(x[, axis]) | 门控线性单元激活函数。 |'
- en: '| `squareplus`(x[, b]) | Squareplus activation function. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `squareplus`(x[, b]) | Squareplus激活函数。 |'
- en: '| `mish`(x) | Mish activation function. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `mish`(x) | Mish激活函数。 |'
- en: Other functions
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他函数
- en: '| `softmax`(x[, axis, where, initial]) | Softmax function. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `softmax`(x[, axis, where, initial]) | Softmax函数。 |'
- en: '| `log_softmax`(x[, axis, where, initial]) | Log-Softmax function. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `log_softmax`(x[, axis, where, initial]) | 对数Softmax函数。 |'
- en: '| `logsumexp`() | Log-sum-exp reduction. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `logsumexp`() | 对数-总和-指数归约。 |'
- en: '| `standardize`(x[, axis, mean, variance, ...]) | Normalizes an array by subtracting
    `mean` and dividing by \(\sqrt{\mathrm{variance}}\). |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `standardize`(x[, axis, mean, variance, ...]) | 通过减去`mean`并除以\(\sqrt{\mathrm{variance}}\)来标准化数组。
    |'
- en: '| `one_hot`(x, num_classes, *[, dtype, axis]) | One-hot encodes the given indices.
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `one_hot`(x, num_classes, *[, dtype, axis]) | 对给定索引进行One-hot编码。 |'
