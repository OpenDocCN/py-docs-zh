- en: Distributed data loading in a multi-host/multi-process environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式数据加载在多主机/多进程环境中
- en: 原文：[`jax.readthedocs.io/en/latest/distributed_data_loading.html`](https://jax.readthedocs.io/en/latest/distributed_data_loading.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/distributed_data_loading.html`](https://jax.readthedocs.io/en/latest/distributed_data_loading.html)
- en: This high-level guide demonstrates how you can perform distributed data loading
    — when you run JAX in a multi-host or multi-process environment, and the data
    required for the JAX computations is split across the multiple processes. This
    document covers the overall approach for how to think about distributed data loading,
    and then how to apply it to *data-parallel* (simpler) and *model-parallel* (more
    complicated) workloads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个高级指南演示了如何执行分布式数据加载——当你在多主机或多进程环境中运行JAX时，用于JAX计算的数据被分布在多个进程中。本文档涵盖了分布式数据加载的整体方法，以及如何将其应用于*数据并行*（更简单）和*模型并行*（更复杂）的工作负载。
- en: 'Distributed data loading is usually more efficient (the data is split across
    processes) but also *more complex* compared with its alternatives, such as: 1)
    loading the *full global data in a single process*, splitting it up and sending
    the needed parts to the other processes via RPC; and 2) loading the *full global
    data in all processes* and only using the needed parts in each process. Loading
    the full global data is often simpler but more expensive. For example, in machine
    learning the training loop can get blocked while waiting for data, and additional
    network bandwidth gets used per each process.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式数据加载通常比起其它方法更高效（数据分割在各个进程之间），但同时也*更复杂*。例如：1）在单一进程中加载整个全局数据，将其分割并通过RPC发送到其它进程需要的部分；和2）在所有进程中加载整个全局数据，然后在每个进程中只使用需要的部分。加载整个全局数据通常更简单但更昂贵。例如，在机器学习中，训练循环可能会因等待数据而阻塞，并且每个进程会使用额外的网络带宽。
- en: Note
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When using distributed data loading, it’s important that each device (for example,
    each GPU or TPU) has access to the input data shard(s) that it needs to run the
    computation. This is what usually makes distributed data loading more complicated
    and challenging to implement correctly (compared with the alternatives described
    above). If the incorrect data shards end up on the wrong devices, the computation
    can still run without errors, since the computation has no way to know what the
    input data “should” be. However, the final result will often be incorrect, since
    the input data was different than intended.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用分布式数据加载时，每个设备（例如每个GPU或TPU）必须访问其需要运行计算的输入数据分片。这通常使得分布式数据加载比前述的替代方案更复杂和具有挑战性。如果错误的数据分片最终出现在错误的设备上，计算仍然可以正常运行，因为计算无法知道输入数据“应该”是什么。然而，最终结果通常是不正确的，因为输入数据与预期不同。
- en: General approach for loading a `jax.Array`
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载`jax.Array`的一般方法
- en: 'Consider a case of creating a single `jax.Array` from raw data not produced
    by JAX. These concepts apply beyond loading batched data records, such as any
    multi-process `jax.Array` that wasn’t directly produced by a JAX computation.
    Examples include: 1) loading model weights from a checkpoint; or 2) loading a
    large spatially-sharded image.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个情况，从未由JAX生成的原始数据创建单个`jax.Array`。这些概念适用于不仅限于加载批量数据记录，例如任何未直接由JAX计算产生的多进程`jax.Array`。例如：1）从检查点加载模型权重；或者2）加载大型空间分片图像。
- en: Every `jax.Array` has an associated `Sharding`, which describes which shard
    of the global data is required by each global device. When you create a `jax.Array`
    from scratch, you also need to create its `Sharding`. This is how JAX can understand
    how the data is laid out across devices. You can create whatever `Sharding` you
    want. In practice, you usually pick a `Sharding` based on what kind of parallelism
    strategy you are implementing (you will learn more about data and model parallelism
    in more detail later in this guide). You can also pick a `Sharding` based on how
    the raw data will be produced within each process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`jax.Array`都有一个相关的`Sharding`，描述了每个全局设备所需的全局数据的哪个分片。当你从头创建一个`jax.Array`时，你还需要创建其`Sharding`。这是JAX理解数据在各个设备上布局的方式。你可以创建任何你想要的`Sharding`。在实践中，通常根据你正在实现的并行策略选择一个`Sharding`（稍后在本指南中将更详细地了解数据和模型并行）。你也可以根据原始数据在每个进程中如何生成来选择一个`Sharding`。
- en: 'Once you have defined a `Sharding`, you can use `addressable_devices()` to
    provide a list of devices needed to load data for within the current process.
    (Note: The term “addressable devices” is a more general version of “local devices”.
    The goal is to make sure that each process’s data loader provides the right data
    to all of that process’ local devices.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了`Sharding`，你可以使用`addressable_devices()`为当前进程需要加载数据的设备提供一个设备列表。（注：术语“可寻址设备”是“本地设备”的更一般版本。目标是确保每个进程的数据加载器为其所有本地设备提供正确的数据。）
- en: Examples
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: 'For example, consider a `(64, 128)` `jax.Array` that you need to shard across
    4 processes with 2 devices each (8 devices total). This will result in 8 unique
    data shards, one for each device. There are many ways to shard this `jax.Array`.
    You can perform a 1D sharding across the second dimension of the `jax.Array`,
    giving each device a `(64, 16)` shard, as demonstrated below:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个`(64, 128)`的`jax.Array`，你需要将其分片到4个进程，每个进程有2个设备（总共8个设备）。这将导致8个唯一的数据分片，每个设备一个。有许多分片`jax.Array`的方法。你可以沿着`jax.Array`的第二维进行1D分片，每个设备得到一个`(64,
    16)`的分片，如下所示：
- en: '![8 unique data shards](img/1620c7594731e8a4457c14e5c5672ff1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![8个唯一的数据分片](img/1620c7594731e8a4457c14e5c5672ff1.png)'
- en: In the above figure, each data shard has its own color to indicate which process
    needs to load that shard. For example, you assume process `0`’s 2 devices contain
    shards `A` and `B`, corresponding to the first `(64, 32)` piece of the global
    data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，每个数据分片都有自己的颜色，表示哪个进程需要加载该分片。例如，假设进程`0`的2个设备包含分片`A`和`B`，对应于全局数据的第一个`(64,
    32)`部分。
- en: 'You can pick a different distribution of shards to devices. For example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择不同的分片到设备的分布方式。例如：
- en: '![8 unique data shards - different distribution](img/553038501f071e8cdbb6d2947da7e3e9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![8个唯一的数据分片 - 不同的分布](img/553038501f071e8cdbb6d2947da7e3e9.png)'
- en: 'Here is another example — a 2D sharding:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个示例——二维分片：
- en: '![2D sharding](img/2b5e3038a613ddadddc37216afbadaff.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![2D分片](img/2b5e3038a613ddadddc37216afbadaff.png)'
- en: 'However the `jax.Array` happens to be sharded, you have to make sure that each
    process’s data loader is provided with/loads the required shard(s) of the global
    data. There are several high-level methods for achieving this: 1) load the global
    data in each process; 2) use a per-device data pipeline; 3) use a consolidated
    per-process data pipeline; 4) load data in some convenient way and then reshard
    inside computation.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，无论`jax.Array`如何分片，你都必须确保每个进程的数据加载器提供/加载全局数据所需的分片。有几种高级方法可以实现这一点：1）在每个进程中加载全局数据；2）使用每设备数据流水线；3）使用合并的每进程数据流水线；4）以某种方便的方式加载数据，然后在计算中重新分片。
- en: 'Option 1: Load the global data in each process'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选项1：在每个进程中加载全局数据
- en: '![Loading the global data in each process](img/8a324f72bc5b92cbd952104642608662.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![在每个进程中加载全局数据](img/8a324f72bc5b92cbd952104642608662.png)'
- en: 'Using this option, each process:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此选项，每个进程：
- en: Loads the full value needed; and
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的完整值；并且
- en: Transfers only the needed shards to that process’s local devices.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅将所需的分片传输到该进程的本地设备。
- en: This is not an efficient approach to distributed data loading, since each process
    will throw away the data not needed by its local devices, and the total data ingested
    can be higher than necessary. But this option works and is relatively simple to
    implement, while the performance overhead may be acceptable for certain workloads
    (for example, if the global data is small).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是一个高效的分布式数据加载方法，因为每个进程都会丢弃其本地设备不需要的数据，并且总体加载的数据量可能会比必要的要多。但这个选项可以运行，并且相对简单实现，对于某些工作负载的性能开销可能是可以接受的（例如，如果全局数据量较小）。
- en: 'Option 2: Use a per-device data pipeline'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选项2：使用每设备数据流水线
- en: '![Using a per-device data pipeline](img/c003002ad8634050d6b779752d92cf2d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![使用每设备数据流水线](img/c003002ad8634050d6b779752d92cf2d.png)'
- en: In this option, each process sets up a data loader for each of its local devices
    (that is, each device gets its own data loader for just the data shard it requires).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在此选项中，每个进程为其每个本地设备设置一个数据加载器（即，每个设备仅为其所需的数据分片设置自己的数据加载器）。
- en: 'This is efficient in terms of the data loaded. It can also sometimes be simpler
    to consider each device independently rather than all of a process’s local devices
    at once (refer to *Option 3: Use a consolidated per-process data pipeline* below).
    However, having multiple concurrent data loaders can sometimes cause performance
    issues.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这在加载数据方面非常高效。有时，独立考虑每个设备可能比一次性考虑所有进程的本地设备更简单（参见下面的*选项3：使用合并的每进程数据流水线*）。然而，多个并发数据加载器有时会导致性能问题。
- en: 'Option 3: Use a consolidated per-process data pipeline'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选项3：使用集中的每个进程数据管道
- en: '![Using a consolidated per-process data pipeline](img/4ec605833b964c8162d77874b20639ba.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![使用集中的每个进程数据管道](img/4ec605833b964c8162d77874b20639ba.png)'
- en: 'If you choose this option, each process:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择此选项，每个过程：
- en: Sets up a single data loader that loads the data required for all of its local
    devices; and then
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个单一的数据加载器，加载所有本地设备所需的数据；然后
- en: Shards the local data before transferring to each local device.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在传输到每个本地设备之前对本地数据进行分片。
- en: This is the *most efficient way to do distributed loading*. However, it’s also
    the *most complex*, since logic is needed both to figure out which data is needed
    by each device, and to create a single data loading that loads only all of that
    data (and, ideally, no other extra data).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*最有效的分布式加载方式*。然而，这也是*最复杂的*，因为需要逻辑来确定每个设备所需的数据，以及创建一个单一的数据加载，仅加载所有这些数据（理想情况下，没有其他额外的数据）。
- en: 'Option 4: Load data in some convenient way, reshard inside computation'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选项4：以某种便捷方式加载数据，在计算中重新分片
- en: '![Loading  data in some convenient way, reshard inside computation](img/0990b88b78dc3375026a784b5d15ab95.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![以某种便捷方式加载数据，在计算中重新分片](img/0990b88b78dc3375026a784b5d15ab95.png)'
- en: This option is more challenging to explain, but often easier to implement than
    the above options (from 1 to 3).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选项比前述选项（从1到3）更难解释，但通常比它们更容易实现。
- en: Imagine a scenario where it’s difficult or rather impossible to set up data
    loaders that load exactly the data you need, either for per-device or per-process
    loaders. However, it may still be possible to set up a data loader per process
    that loads `1 / num_processes` of the data, just not in the right sharding.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，设置数据加载器以精确加载您需要的数据，无论是为每个设备还是每个进程加载器，这可能很困难或几乎不可能。然而，仍然可以为每个进程设置一个数据加载器，加载数据的`1
    / num_processes`，只是没有正确的分片。
- en: 'Then, continuing with your 2D example sharding from before, assume it is easier
    for each process to load a single column of the data:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，继续使用您之前的2D示例分片，假设每个过程更容易加载数据的单个列：
- en: Then, you can create a `jax.Array` with a `Sharding` representing the per-column
    data, pass that directly into the computation, and use `jax.lax.with_sharding_constraint()`
    to immediately reshard the column-sharded input to the desired sharding. And since
    the data is resharded inside the computation, it will be resharded over the accelerator
    communication links (for example, TPU ICI or NVLink).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以创建一个带有表示每列数据的`Sharding`的`jax.Array`，直接将其传递到计算中，并使用`jax.lax.with_sharding_constraint()`立即将列分片输入重新分片为所需的分片。由于数据在计算中重新分片，它将通过加速器通信链路（例如TPU
    ICI或NVLink）进行重新分片。
- en: 'This Option 4 has similar benefits to Option 3 (*Use a consolidated per-process
    data pipeline*):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 选项4与选项3（*使用集中的每个进程数据管道*）具有类似的优点：
- en: Each process still has a single data loader; and
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个过程仍然具有单个数据加载器；和
- en: The global data is loaded exactly once across all processes; and
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局数据在所有过程中仅加载一次；和
- en: The global data has the additional benefit of offering more flexibility in how
    the data is loaded.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局数据的额外好处在于提供如何加载数据的更大灵活性。
- en: However, this approach uses accelerator interconnect bandwidth to perform the
    resharding, which may slow down certain workloads. Option 4 also requires that
    the input data be expressed as a separate `Sharding`, in addition to the target
    `Sharding`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法使用加速器互连带宽执行重新分片，可能会降低某些工作负载的速度。选项4还要求将输入数据表示为单独的`Sharding`，除了目标`Sharding`。
- en: Replication
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制
- en: Replication describes a process where multiple devices have the same data shard.
    The general options mentioned above (Options 1 through 4) still work with replication.
    The only difference is that some processes may end up loading the same data shards.
    This section describes full replication and partial replication.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 复制描述了多个设备具有相同数据分片的过程。上述提到的一般选项（选项1到4）仍然适用于复制。唯一的区别是某些过程可能会加载相同的数据分片。本节描述了完全复制和部分复制。
- en: Full replication
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全部复制
- en: '**Full replication** is a process where all devices have a full copy of the
    data (that is, the data “shard” is the entire array value).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**完全复制**是所有设备都具有数据的完整副本的过程（即，“分片”是整个数组值）。'
- en: 'In the below example, since there are 8 devices in total (2 per process), you
    will end up with 8 copies of the full data. Each copy of the data is unsharded,
    that is the copy lives on a single device:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，由于总共有 8 个设备（每个进程 2 个），您将得到完整数据的 8 个副本。数据的每个副本都未分片，即副本存在于单个设备上：
- en: '![Full replication](img/1cb84863cdedd7bd0d22230758242771.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![完全复制](img/1cb84863cdedd7bd0d22230758242771.png)'
- en: Partial replication
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部分复制
- en: '**Partial replication** describes a process where there are multiple copies
    of the data, and each copy is sharded across multiple devices. For a given array
    value, there are generally many possible ways to perform partial replication (Note:
    There is always a single fully-replicated `Sharding` for a given array shape).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**部分复制**描述了一个过程，其中数据有多个副本，并且每个副本分片到多个设备上。对于给定的数组值，通常有许多执行部分复制的可能方法（注意：对于给定的数组形状，总是存在单一完全复制的`Sharding`）。'
- en: Below are two possible examples.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是两个可能的示例。
- en: In the first example below, each copy is sharded across the two local devices
    of a process, for a total of 4 copies. This means that each process will need
    to load the full global data, since its local devices will have a full copy of
    the data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的第一个示例中，每个副本都分片到进程的两个本地设备上，总共有 4 个副本。这意味着每个进程都需要加载完整的全局数据，因为其本地设备将具有数据的完整副本。
- en: '![Partial replication - example 1](img/768f3cfac402875aec715ae83d5d8f0f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![部分复制 - 示例 1](img/768f3cfac402875aec715ae83d5d8f0f.png)'
- en: 'In the second example below, each copy is still sharded across two devices,
    but each device pair is spread across two different processes. Process `0` (pink)
    and process `1` (yellow) both need to load just the first row of the data, and
    process `2` (green) and process `3` (blue) both need to load just the second row
    of the data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的第二个示例中，每个副本仍然分片到两个设备上，但每个设备对是分布在两个不同的进程中。进程 `0`（粉色）和进程 `1`（黄色）都只需要加载数据的第一行，而进程
    `2`（绿色）和进程 `3`（蓝色）都只需要加载数据的第二行：
- en: '![Partial replication - example 2](img/0316e1037c0a936ae5295cdf7e2217dc.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![部分复制 - 示例 2](img/0316e1037c0a936ae5295cdf7e2217dc.png)'
- en: Now that you’ve gone over the high-level options for creating a `jax.Array`,
    let’s apply them to data loading for ML applications.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了创建 `jax.Array` 的高级选项，让我们将它们应用于机器学习应用程序的数据加载。
- en: Data parallelism
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行性
- en: 'In *pure data parallelism* (without model parallelism):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在*纯数据并行性*（无模型并行性）中：
- en: You replicate the model on each device; and
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在每个设备上复制模型；和
- en: Each model replica (that is, each device) receives a different per-replica batch
    of data.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个模型副本（即每个设备）接收不同的副本批次数据。
- en: '![Data parallelism - example 1](img/ed0ee4ec93d8c1e7b06e7f3b4f1fca51.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![数据并行性 - 示例 1](img/ed0ee4ec93d8c1e7b06e7f3b4f1fca51.png)'
- en: When representing the input data as a single `jax.Array`, the Array contains
    the data across all replicas for this step (this is called *global batch*), with
    each shard of the `jax.Array` containing a single per-replica batch. You can represent
    this as a 1D sharding across all devices (check the example below) — in other
    words, the global batch is composed of all the per-replica batches concatenated
    together across the batch axis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入数据表示为单个 `jax.Array` 时，该数组包含此步骤所有副本的数据（称为*全局批处理*），其中 `jax.Array` 的每个分片包含单个副本批处理。您可以将其表示为跨所有设备的
    1D 分片（请查看下面的示例）——换句话说，全局批处理由所有副本批处理沿批处理轴连接在一起组成。
- en: '![Data parallelism - example 2](img/ba9ccdcbb454af1cccf0d1441e54be69.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![数据并行性 - 示例 2](img/ba9ccdcbb454af1cccf0d1441e54be69.png)'
- en: Applying this framework, you may conclude that process `0` should get the first
    quarter (2 out of 8) of the global batch, while process `1` should get the second,
    and so on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 应用此框架，您可以得出结论，进程 `0` 应该获取全局批处理的第一个季度（8 的 2 分之一），而进程 `1` 应该获取第二个季度，依此类推。
- en: But how can you know what the first quarter is? And how do you make sure process
    `0` gets the first quarter? Luckily, there’s a very important trick about data
    parallelism that means you don’t have to answer these questions and makes the
    whole setup simpler.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您如何知道第一个季度是什么？您如何确保进程 `0` 获得第一个季度？幸运的是，数据并行性有一个非常重要的技巧，这意味着您不必回答这些问题，并使整个设置更简单。
- en: Important trick about data parallelism
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据并行性的重要技巧
- en: The trick is you don’t need to care which per-replica batch lands on which replica.
    Therefore, it doesn’t matter which process loads a batch. The reason is that since
    each device corresponds to a model replica performing the same thing, it doesn’t
    matter which device gets which per-replica batch within the global batch.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 诀窍在于您不需要关心哪个每副本批次会落到哪个副本上。因此，不管哪个进程加载了一个批次都无所谓。原因在于每个设备都对应执行相同操作的模型副本，每个设备获取全局批次中的每个每副本批次都无关紧要。
- en: What this means is that you are free to rearrange the per-replica batches within
    the global batch. In other words, you are free to randomize which data shard each
    device gets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着您可以自由重新排列全局批次中的每副本批次。换句话说，您可以随机化每个设备获取哪个数据分片。
- en: 'For example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '![Data parallelism - example 3](img/3e63f708dff32add917838e5b324e494.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![数据并行处理 - 示例 3](img/3e63f708dff32add917838e5b324e494.png)'
- en: Usually, rearranging the data shards of a `jax.Array`, as demonstrated above,
    is not a good idea – you’re effectively permuting the value of the `jax.Array`!
    However, for data parallelism, the global batch order isn’t meaningful, and you
    are free to rearrange the per-replica batches in the global batch, as already
    mentioned before.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，重新排列`jax.Array`的数据分片并不是一个好主意 —— 事实上，您是在对`jax.Array`的值进行置换！然而，对于数据并行处理来说，全局批次顺序并不重要，您可以自由重新排列全局批次中的每个每副本批次，正如前面已经提到的那样。
- en: This simplifies data loading because it means each device just needs an independent
    stream of per-replica batches, which can be easily implemented in most data loaders
    by creating an independent pipeline per process and chunking the resulting per-process
    batch into per-replica batches.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这简化了数据加载，因为这意味着每个设备只需要独立的每副本批次流，大多数数据加载器可以通过为每个进程创建一个独立的流水线并将结果分割为每副本批次来轻松实现。
- en: '![Data parallelism - example 4](img/bc7cc20323c4155fcc2e3796b156a09c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![数据并行处理 - 示例 4](img/bc7cc20323c4155fcc2e3796b156a09c.png)'
- en: 'This is an instance of the *Option 2: Consolidated per-process data pipeline*.
    You can also use other options (such as 0, 1 and 3, which are covered earlier
    in this document), but this one is relatively simple and efficient.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '这是*选项 2: 合并每进程数据流水线*的一个实例。您也可以使用其他选项（如 0、1 和 3，在本文档的早期部分有介绍），但这个选项相对简单和高效。'
- en: 'Here’s an example of how to implement this setup using tf.data:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如何使用 tf.data 实现此设置的示例：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Data + model parallelism
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据 + 模型并行处理
- en: 'In **model parallelism** you shard each model replica across multiple devices.
    If you use **pure model parallelism** (without data parallelism):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在**模型并行处理**中，您将每个模型副本分片到多个设备上。如果您使用**纯模型并行处理**（不使用数据并行处理）：
- en: There’s just one model replica sharded across all devices; and
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个模型副本分片在所有设备上；并且
- en: The data is (usually) fully replicated across all devices.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据通常在所有设备上完全复制。
- en: 'This guide considers a case where you use **both data and model parallelism**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南考虑了同时使用**数据和模型并行处理**的情况：
- en: You shard each of the multiple model replicas over multiple devices; and
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将多个模型副本中的每一个分片到多个设备上；并且
- en: You partially replicate the data over each model replica — each device in the
    same model replica gets the same per-replica batch, and devices across model replicas
    get different per-replica batches.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以部分复制数据到每个模型副本 —— 每个模型副本中的设备得到相同的每副本批次，不同模型副本之间的设备得到不同的每副本批次。
- en: Model parallelism within a process
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进程内的模型并行处理
- en: For the purposes of data loading, the simplest approach can be to shard each
    model replica within the local devices of a single process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据加载，最简单的方法可以是在单个进程的本地设备中将每个模型副本分片。
- en: 'For this example, let’s switch to 2 processes with 4 devices each (instead
    of 4 processes with 2 devices each). Consider a scenario where each model replica
    is sharded over the 2 local devices of a single process. This results in 2 model
    replicas per process and 4 model replicas total, as demonstrated below:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们切换到每个有 4 个设备的 2 个进程（而不是每个有 2 个设备的 4 个进程）。考虑一个情况，每个模型副本都分片在单个进程的 2 个本地设备上。这导致每个进程有
    2 个模型副本，总共 4 个模型副本，如下所示：
- en: '![Data and model parallelism - example 1](img/50baf940e74eaaba1fae84e721a0aaa4.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![数据和模型并行处理 - 示例 1](img/50baf940e74eaaba1fae84e721a0aaa4.png)'
- en: 'Here, once again, the input data is represented as a single `jax.Array` with
    a 1D sharding where each shard is a per-replica batch with an exception:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，再次强调，输入数据表示为单个`jax.Array`，其中每个分片是一个每副本批次的 1D 分片，有一个例外：
- en: Unlike in the pure data parallelism case, you introduce partial replication
    and make 2 copies of the 1D-sharded global batch.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同于纯数据并行情况，你引入了部分复制，并制作了1D分片全局批次的2个副本。
- en: This is because each model replica is composed of 2 devices that each need a
    copy of the per-replica batch.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为每个模型副本由两个设备组成，每个设备都需要一个副本批次的拷贝。
- en: '![Data and model parallelism - example 2](img/425e3c08e61ea384503426e6ae0c0694.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![数据和模型并行性 - 示例 2](img/425e3c08e61ea384503426e6ae0c0694.png)'
- en: 'Keeping each model replica within a single process can make things simpler
    because you can reuse the pure data parallelism setup described above, except
    you also need to replicate the per-replica batches:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个模型副本保持在单个进程内可以使事情变得更简单，因为你可以重用上述纯数据并行设置，除非你还需要复制每个副本的批次：
- en: '![Data and model parallelism - example 3](img/950f554f9d89e6e9dc75e0db17844e61.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![数据和模型并行性 - 示例 3](img/950f554f9d89e6e9dc75e0db17844e61.png)'
- en: Note
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*It’s also very important to replicate the per-replica batches to the correct
    devices!* While the very important trick about data parallelism means you don’t
    care which batch ends up on which replica, *you do care that a single replica
    only gets a single batch*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*同样重要的是要将每个副本批次复制到正确的设备上!* 虽然数据并行性的一个非常重要的技巧意味着你不在乎哪个批次最终落到哪个副本上，*但你确实关心单个副本只得到一个批次*。'
- en: 'For example, this is OK:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是可以的：
- en: '![Data and model parallelism - example 4](img/e1fa137c8543477be6e375d42972ccce.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![数据和模型并行性 - 示例 4](img/e1fa137c8543477be6e375d42972ccce.png)'
- en: 'However, if you’re not careful about which local device you load each batch
    onto, you may accidentally create unreplicated data, even though the `Sharding`
    (and the parallelism strategy) says the data is replicated:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你在加载每批数据到本地设备时不小心，可能会意外地创建未复制的数据，即使`分片`（和并行策略）表明数据已经复制：
- en: '![Data and model parallelism - example 4](img/c9ccdf8d01126f142e7af011fbdf39e7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![数据和模型并行性 - 示例 4](img/c9ccdf8d01126f142e7af011fbdf39e7.png)'
- en: JAX will raise an error if you accidentally create a `jax.Array` with unreplicated
    data that should be replicated within a single process (this isn’t always true
    for model parallelism across processes though; see the next section).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你意外地创建了应该在单个进程内复制的未复制数据的`jax.Array`，JAX将会报错（不过对于跨进程的模型并行性，情况并非总是如此；请参阅下一节）。
- en: 'Here’s an example of how to implement per-process model parallelism and data
    parallelism using `tf.data`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`tf.data`实现每个进程模型并行性和数据并行性的示例：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Model parallelism across processes
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨进程的模型并行性
- en: 'It can get more interesting when model replicas are spread across processes,
    either:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型副本分布在不同进程中时，可能会变得更加有趣，无论是：
- en: Because a single replica can’t fit within a process; or
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为单个副本无法适应一个进程；或者
- en: Because the device assignment just isn’t set up that way.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为设备分配并不是按照这种方式设置的。
- en: 'For example, going back to the previous setup of 4 processes with 2 devices
    each, if you assign devices to replicas like so:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，回到之前的设置，4个每个有2个设备的进程，如果你像这样为副本分配设备：
- en: '![Model parallelism across processes - example 1](img/faead091f47b3535fb0995d91ffc61f4.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![跨进程的模型并行性 - 示例 1](img/faead091f47b3535fb0995d91ffc61f4.png)'
- en: This is the same parallelism strategy as the previous per-process model parallelism
    example – 4 model replicas each sharded across 2 devices. The only difference
    is the device assignment – each replica’s two devices are split across different
    processes, and each process is only responsible for one copy of each per-replica
    batch (but for two replicas).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的每个进程模型并行性示例相同的并行策略 - 4个模型副本，每个副本分布在2个设备上。唯一的区别在于设备分配 - 每个副本的两个设备分布在不同的进程中，每个进程只负责每个副本批次的一份拷贝（但是对于两个副本）。
- en: Splitting the model replicas across processes like this may seem like an arbitrary
    and unnecessary thing to do (and in this example it arguably is), but actual deployments
    may end up with this kind of device assignment to best take advantage of the communication
    links between devices.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样跨进程分割模型副本可能看起来是一种随意且不必要的做法（在这个例子中，这可能是这样），但实际的部署可能会采用这种设备分配方式，以最大程度地利用设备之间的通信链路。
- en: Data loading now becomes more complicated because some extra coordination is
    required across processes. In the pure data parallelism and per-process model
    parallelism cases, it was only important that each process loaded a unique data
    stream. Now certain processes must load the same data, and some must load different
    data. In the above example, processes `0` and `2` (in colors pink and green, respectively)
    must load the same 2 per-replica batches, and processes `1` and `3` (colors yellow
    and blue, respectively) must also load the same 2 per-replica batches (but different
    from process `0` and `2`’s batches).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载现在变得更加复杂，因为跨进程需要一些额外的协调。在纯数据并行和每个进程模型并行的情况下，每个进程只需加载唯一的数据流即可。现在某些进程必须加载相同的数据，而另一些进程必须加载不同的数据。在上述示例中，进程`0`和`2`（分别显示为粉色和绿色）必须加载相同的2个每个副本的批次，并且进程`1`和`3`（分别显示为黄色和蓝色）也必须加载相同的2个每个副本的批次（但不同于进程`0`和`2`的批次）。
- en: 'Furthermore, it’s important that each process doesn’t mix up its 2 per-replica
    batches. While you don’t care which batch lands on which replica (the very important
    trick about data parallelism), you need to care that all the devices in a replica
    get the same batch. For example, this would be bad:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个进程不混淆它的2个每个副本的批次是非常重要的。虽然您不关心哪个批次落在哪个副本（这是关于数据并行的一个非常重要的技巧），但您需要确保同一个副本中的所有设备获取相同的批次。例如，以下情况是不好的：
- en: '![Model parallelism across processes - example 2](img/03432bedef0f4b0734a8ee71aea5f19c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![进程间的模型并行化示例2](img/03432bedef0f4b0734a8ee71aea5f19c.png)'
- en: Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As of August 2023, JAX cannot detect if `jax.Array` shards across processes
    are supposed to be replicated but aren’t, and will produce wrong results when
    the computation is run. So be careful not to do this!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2023年8月，JAX 无法检测到如果`jax.Array`在进程之间的分片应该复制但实际没有复制，则在运行计算时会产生错误结果。因此，请务必注意避免这种情况！
- en: 'To get the correct per-replica batch on each device, you need to represent
    the global input data as the following `jax.Array`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要在每个设备上获取正确的每个副本批次，您需要将全局输入数据表示为以下的`jax.Array`：
- en: '![Model parallelism across processes - example 3](img/17939e84e2dbae73c80e0b1c07382fa2.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![进程间的模型并行化示例3](img/17939e84e2dbae73c80e0b1c07382fa2.png)'
