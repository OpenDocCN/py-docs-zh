- en: shmap (shard_map) for simple per-device code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`shmap`（shard_map）用于简单的每个设备代码'
- en: 原文：[`jax.readthedocs.io/en/latest/jep/14273-shard-map.html`](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/jep/14273-shard-map.html`](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html)
- en: '*sholto@, sharadmv@, jekbradbury@, zhangqiaorjc@, mattjj@*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*sholto@, sharadmv@, jekbradbury@, zhangqiaorjc@, mattjj@*'
- en: '*January 2023*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*2023年1月*'
- en: Motivation
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: 'JAX supports two schools of thought for multi-device programming:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: JAX支持两种多设备编程的思路：
- en: '**Compiler, take the wheel!** Let the compiler automatically partition bulk
    array functions over devices.'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编译器，带我飞！** 让编译器自动在设备间分配大数组函数。'
- en: '**Just let me write what I mean, damnit!** Give me per-device code and explicit
    communication collectives.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**让我写我想表达的内容！** 给我每个设备的代码和显式的通信集合。'
- en: We need great APIs for both, and rather than being mutually exclusive alternatives,
    they need to compose with each other.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要既出色的API，而不是互斥的替代方案，它们需要相互组合。
- en: With `pjit` (now just `jit`) we have [a next-gen API](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)
    for the first school. But we haven’t quite leveled-up the second school. `pmap`
    follows the second school, but over time we found it has fatal flaws. `xmap` solved
    those flaws, but it doesn’t quite give us per-device shapes, and it includes several
    other big ideas too. Meanwhile, new demands for per-device explicit-collectives
    programming have emerged, like in [Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`pjit`（现在是`jit`），我们拥有了[下一代API](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)来支持第一种思路。但是我们还没有完全升级第二种思路。`pmap`遵循第二种思路，但随着时间的推移，我们发现它存在致命缺陷。`xmap`解决了这些问题，但它并没有完全给我们提供每个设备的形状，并且还包含了其他几个重大的想法。同时，对于像在[高效扩展Transformer推理](https://arxiv.org/abs/2211.05102)中的每个设备显式集合编程的新需求也在不断涌现。
- en: 'We can level-up the second school with `shmap`. `shmap` is:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`shmap`升级第二种思路。`shmap`是：
- en: a simple multi-device parallelism API which lets us write per-device code with
    explicit collectives, where logical shapes match per-device physical buffer shapes
    and collectives correspond exactly to cross-device communication;
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的多设备并行API，允许我们编写每个设备的代码，并且使用显式的集合操作，其中逻辑形状与每个设备的物理缓冲区形状匹配，并且集合操作正好对应于跨设备的通信；
- en: a specialization of `xmap` with scaled-back features and a few tweaks;
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`xmap`的特化，具有简化的功能和一些调整；
- en: a fairly direct surfacing of the XLA SPMD Partitioner’s ‘manual’ mode;
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLA SPMD分区器的‘手动’模式的一个相当直接的表现；
- en: a fun-to-say Seussian name which could stand for `shard_map`, `shpecialized_xmap`,
    `sholto_map`, or `sharad_map`.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个好玩的Seussian名称，可以代表`shard_map`、`shpecialized_xmap`、`sholto_map`或`sharad_map`。
- en: '**For `pjit` users**, `shmap` is a complementary tool. It can be used inside
    a `pjit` computation to drop temporarily into a “manual collectives” mode, like
    an escape hatch from the compiler’s automatic partitioning. That way, users get
    the convenience and familiar just-NumPy programming model of `pjit` for most of
    their code, along with the ability to hand-optimize collective communication with
    `shmap` wherever it’s needed. It’s the best of both worlds!'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于`pjit`用户**，`shmap`是一个补充工具。它可以在`pjit`计算中使用，暂时切换到“手动集合”模式，就像是从编译器的自动分区中逃脱一样。这样，用户可以在大部分代码中享受到`pjit`的便利和熟悉的NumPy编程模型，同时在需要时使用`shmap`来手动优化集合通信。这是两全其美的解决方案！'
- en: '**For `pmap` users**, `shmap` is a strict upgrade. It’s more expressive, performant,
    and composable with other JAX APIs, without making basic batch data parallelism
    any harder.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于`pmap`用户**，`shmap`是一个严格的升级。它更加表达力强，性能更好，并且与其他JAX API可以良好组合，而不会使基本的批量数据并行化变得更加困难。'
- en: For more on practical use, you can jump to When should you use `shmap` and when
    should you use `pjit`?. If you’re wondering why we need a new thing at all, or
    what the problems with `pmap` are, jump to Why don’t `pmap` or `xmap` already
    solve this?. Or keep reading the next section to see some `shmap` examples and
    the API spec.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更多的实际使用情况，你可以跳转到[何时使用`shmap`和何时使用`pjit`](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html)？如果你想知道我们为什么需要一个全新的东西，或者`pmap`存在什么问题，可以跳到为什么`pmap`或`xmap`不能解决这个问题？或者继续阅读下一节，查看一些`shmap`示例和API规范。
- en: So, let’s see `shmap`!
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所以，让我们看看`shmap`！
- en: TL;DR example (with a more detailed explanation to follow)
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TL;DR示例（详细解释将在后续部分提供）
- en: 'Sho shick:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Sho shick：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Notice:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: no nesting needed (or `axis_index_groups`) for multiple axes of parallelism,
    unlike `pmap`;
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要嵌套（或`axis_index_groups`）来处理多个轴的并行性，不像`pmap`；
- en: no reshapes in the caller, unlike `pmap` and hard-`xmap`, and logical shapes
    correspond to per-device physical shapes, unlike (non-hard) `xmap`;
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用者中没有重塑，不像 `pmap` 和 hard-`xmap`，逻辑形状对应于每个设备的物理形状，不像（非硬）`xmap`；
- en: precise device placement control by using `mesh`, unlike `pmap`;
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用 `mesh` 实现精确的设备放置控制，不像 `pmap`；
- en: there’s only one set of axis names for logical and physical, unlike `xmap`;
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑和物理轴名称只有一个集合，不像 `xmap`；
- en: the result is a `jax.Array` which could be efficiently passed to a `pjit`, unlike
    `pmap`;
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果是一个可以有效传递给 `pjit` 的 `jax.Array`，不像 `pmap`；
- en: this same code works efficiently inside a `pjit`/`jit`, unlike `pmap`;
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此相同的代码在 `pjit`/`jit` 内部有效地工作，不像 `pmap`；
- en: this code works eagerly, so we can `pdb` in the middle and print values, unlike
    `xmap`’s current implementation (though by design `xmap` without the sequential
    schedule can in principle work eagerly too).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此代码以急切方式工作，因此我们可以在中间使用 `pdb` 并打印值，不像 `xmap` 的当前实现（尽管设计上 `xmap` 没有顺序安排也可以急切地工作）。
- en: 'Here’s another matmul variant with a fully sharded result:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一种具有完全分片结果的矩阵乘法变体：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Slow down, start with the basics!
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 放慢速度，从基础知识开始！
- en: Rank-reducing vs rank-preserving maps over array axes
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在数组轴上的降秩和保持秩的映射比较
- en: 'We can think of `pmap` (and `vmap` and `xmap`) as unstacking each array input
    along an axis (e.g. unpacking a 2D matrix into its 1D rows), applying its body
    function to each piece, and stacking the results back together, at least when
    collectives aren’t involved:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 `pmap`（和 `vmap` 和 `xmap`）视为沿轴解堆叠每个数组输入（例如，将 2D 矩阵解包成其 1D 行），对每个片段应用其体函数，并将结果堆叠在一起，至少在不涉及集合时是这样的：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For example, if `xs` had shape `f32[8,5]` then each `x` has shape `f32[5]`,
    and if each `f(x)` has shape `f32[3,7]` then the final stacked result `pmap(f)(xs)`
    has shape `f32[8,3,7]`. That is, each application of the body function `f` takes
    as argument inputs with one fewer axis than the corresponding argument to `pmap(f)`.
    We can say these are *rank-reducing maps* with unstacking/stacking of inputs/outputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 `xs` 的形状为 `f32[8,5]`，那么每个 `x` 的形状为 `f32[5]`，如果每个 `f(x)` 的形状为 `f32[3,7]`，那么最终堆叠的结果
    `pmap(f)(xs)` 的形状为 `f32[8,3,7]`。也就是说，每次对体函数 `f` 的应用都比 `pmap(f)` 对应的参数少一个轴。我们可以说这些是*降秩映射*，输入/输出的解堆叠/堆叠。
- en: 'The number of logical applications of `f` is determined by the size of the
    input axis being mapped over: for example, if we map over an input axis of size
    8, semantically we get 8 logical applications of the function, which for pmap
    always correspond to 8 devices physically computing them.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`f` 的逻辑应用次数由被映射的输入轴的大小确定：例如，如果我们在大小为 8 的输入轴上进行映射，从语义上讲，我们得到函数的 8 次逻辑应用，这对于
    `pmap` 总是对应于 8 个物理设备计算。'
- en: 'In contrast, `shmap` does not have this rank-reducing behavior. Instead, we
    can think of it as slicing (or “unconcatenating”) along input axes into blocks,
    applying the body function, and concatenating the results back together (again
    when collectives aren’t involved):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，`shmap` 没有这种降秩行为。相反，我们可以将其视为沿输入轴切片（或“非连接”）为块，应用体函数，并将结果再次连接在一起（在没有涉及集合时）：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Recall that `jnp.split` slices its input into equally-sized blocks with the
    same rank, so that if in the above example `y` has shape `f32[8,5]` then each
    `y_blk` has shape `f32[2,5]`, and if each `f(y_blk)` has shape `f32[3,7]` then
    the final concatenated result `shard_map(f, ...)(y)` has shape `f32[12,7]`. So
    `shmap` (`shard_map`) maps over shards, or blocks, of its inputs. We can say it’s
    a *rank-preserving ma*p with unconcatenating/concatenating of its inputs/outputs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`jnp.split` 将其输入切片成相同大小的块，因此如果在上述示例中 `y` 的形状为 `f32[8,5]`，那么每个 `y_blk` 的形状为
    `f32[2,5]`，如果每个 `f(y_blk)` 的形状为 `f32[3,7]`，那么最终连接的结果 `shard_map(f, ...)(y)` 的形状为
    `f32[12,7]`。因此 `shmap`（`shard_map`）映射其输入的分片或块。我们可以说它是一个*保持秩映射*，其输入/输出的解连接/连接。
- en: 'The number of logical applications of `f` is determined by the mesh size, not
    by any input axis size: for example, if we have a mesh of total size 4 (i.e. over
    4 devices) then semantically we get 4 logical applications of the function, corresponding
    to the 4 devices physically computing them.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`f` 的逻辑应用次数由网格大小确定，而不是任何输入轴大小：例如，如果我们有总大小为 4 的网格（即超过 4 个设备），那么从语义上讲，我们得到函数的
    4 次逻辑应用，对应于 4 个物理设备计算它们。'
- en: Controlling how each input is split (unconcatenated) and tiled with `in_specs`
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过使用 `in_specs` 控制每个输入的切分（解连接）和平铺；
- en: 'Each of the `in_specs` identifies some of the corresponding input array’s axes
    with mesh axes by name using `PartitionSpec`s, representing how to split (or unconcatenate)
    that input into the blocks to which the body function is applied. That identification
    determines the shard sizes; when an input axis is identified with a mesh axis,
    the input is split (unconcatenated) along that logical axis into a number of pieces
    equal to the corresponding mesh axis size. (It’s an error if the corresponding
    mesh axis size does not evenly divide the input array axis size.) If an input’s
    pspec does not mention a mesh axis name, then there’s no splitting over that mesh
    axis. For example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '每个`in_specs`通过`PartitionSpec`标识了一些相应输入数组的轴，通过网格轴名称表示如何将该输入拆分（或取消连接）成应用主体函数的块。该标识确定了分片大小；当一个输入轴标识为一个网格轴时，输入沿该逻辑轴分割（取消连接）为与相应网格轴大小相等的多个部分。（如果相应网格轴大小不能整除输入数组轴大小，则会产生错误。）如果输入的`pspec`未提及网格轴名称，则在该网格轴上不会进行分割。例如:'
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, because the input pspec did not mention the mesh axis name `'j'`, no input
    array axis is split over that mesh axis; similarly, because the second axis of
    the input array is not identified with (and hence split over) any mesh axis, application
    of `f1` gets a full view of the input along that axis.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输入的`pspec`未提及网格轴名`'j'`，所以没有任何输入数组轴在该网格轴上进行分割；同样地，因为输入数组的第二轴未与任何网格轴标识（因此未在其上进行分割），`f1`的应用将完整查看该轴上的输入。
- en: 'When a mesh axis is not mentioned in an input pspec, we can always rewrite
    to a less efficient program where all mesh axes are mentioned but the caller performs
    a `jnp.tile`, for example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入的`pspec`中未提及网格轴时，我们总是可以重写为一个效率较低的程序，其中所有网格轴都被提及，但调用者执行`jnp.tile`，例如：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In other words, because each input pspec can mention each mesh axis name zero
    or one times, rather than having to mention each name exactly once, we can say
    that in addition to the `jnp.split` built into its input, `shard_map` also has
    a `jnp.tile` built into its input, at least logically (though the tiling may not
    need to be carried out physically, depending on the arguments’ physical sharding
    layout). The tiling to use is not unique; we could also have tiled along the first
    axis, and used the pspec `P(('j', 'i'), None)`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，因为每个输入的`pspec`可以零次或一次提及每个网格轴名，而不必确切一次提及每个名字，所以我们可以说，除了其输入中内置的`jnp.split`，`shard_map`还具有一个内置的`jnp.tile`，至少在逻辑上是如此（尽管根据参数的物理分片布局，不一定需要在物理上执行平铺）。要使用的平铺方法不是唯一的；我们也可以沿着第一个轴平铺，并使用`P(('j',
    'i'), None)`的`pspec`。
- en: Physical data movement is possible on inputs, as each device needs to have a
    copy of the appropriate data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入上的物理数据移动是可能的，因为每个设备都需要具有适当数据的副本。
- en: Controlling how each output assembled by concatenation, block transposition,
    and untiling using `out_specs`
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用`out_specs`控制每个输出通过连接、块转置和使用`untiling`组装。
- en: Analogously to the input side, each of the `out_specs` identifies some of the
    corresponding output array’s axes with mesh axes by name, representing how the
    output blocks (one for each application of the body function, or equivalently
    one for each physical device) should be assembled back together to form the final
    output value. For example, in both the `f1` and `f2` examples above the `out_specs`
    indicate we should form the final output by concatenating together the block results
    along both axes, resulting in both cases an array `y` of shape `(12,24)`. (It’s
    an error if an output shape of the body function, i.e. an output block shape,
    has a rank too small for the concatenation described by the corresponding output
    pspec.)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于输入端，每个`out_specs`通过名称将一些相应输出数组的轴标识为网格轴，表示如何将输出块（每个主体函数应用的一个或等效地每个物理设备的一个）组装回来以形成最终输出值。例如，在上述`f1`和`f2`示例中，`out_specs`表明我们应通过沿两个轴连接块结果来形成最终输出，从而在两种情况下得到形状为`(12,24)`的数组`y`。（如果主体函数的输出形状，即输出块形状，对应的输出`pspec`所描述的连接过程具有过小的秩，则会产生错误。）
- en: 'When a mesh axis name is not mentioned in an output pspec, it represents an
    *un-tiling*: when the user writes an output pspec which does not mention one of
    the mesh axis names, they promise that the output blocks are equal along that
    mesh axis, and so only one block along that axis is used in the output (rather
    than concatenating all the blocks together along that mesh axis). For example,
    using the same mesh as above:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出 pspec 中未提到网格轴名称时，它表示一个*未平铺*：当用户编写一个输出 pspec，其中未提到网格轴名称之一时，他们保证输出块在该网格轴上是相等的，因此在输出中仅使用该轴上的一个块（而不是沿该网格轴将所有块连接在一起）。例如，使用与上述相同的网格：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Notice that the body function closing over an array value is equivalent to
    passing it as an augment with a corresponding input pspec of `P(None, None)`.
    As another example, following more closely to the other examples above:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，闭包在数组值上的主体函数等同于将其作为具有相应输入 pspec `P(None, None)` 的增广传递。作为另一个例子，更接近前面例子的另一个例子：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Notice that the result has a second axis size of 6, half the size of the input’s
    second axis. In this case, the un-tile expressed by not mentioning the mesh axis
    name `''j''` in the output pspec was safe because of the collective `psum`, which
    ensures each output block is equal along the corresponding mesh axis. Here are
    two more examples where we vary which mesh axes are mentioned in the output pspec:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，结果的第二个轴大小为 6，是输入第二个轴大小的一半。在这种情况下，通过在输出 pspec 中不提到网格轴名称 `'j'` 来表达的未平铺是安全的，因为集体
    `psum` 确保每个输出块在相应的网格轴上是相等的。以下是另外两个例子，其中我们变化了在输出 pspec 中提到的网格轴：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: On the physical side, not mentioning a mesh axis name in an output pspec assembles
    an `Array` from the output device buffers with replicated layout along that mesh
    axis.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理方面，未在输出 pspec 中提到网格轴名称会从输出设备缓冲区组装一个 `Array`，在该网格轴上具有复制的布局。
- en: There is no runtime check that the output blocks are actually equal along a
    mesh axis to be un-tiled along, or equivalently that the corresponding physical
    buffers have equal values and thus can be interpreted as a replicated layout for
    a single logical array. But we can provide a static check mechanism which raises
    an error on all potentially-incorrect programs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 没有运行时检查输出块实际上是否沿网格轴相等以进行未平铺，或者等效地说，相应的物理缓冲区是否具有相等的值，因此可以解释为单个逻辑数组的复制布局。但我们可以提供一个静态检查机制，在所有潜在不正确的程序上引发错误。
- en: Because the `out_specs` can mention mesh axis names zero or one times, and because
    they can be mentioned in any order, we can say that in addition to the `jnp.concatenate`
    built into its output, `shard_map` also has both an untile and a block transpose
    built into its output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `out_specs` 可以提到网格轴名称零次或一次，并且它们可以以任意顺序提到，所以我们可以说，除了其输出中内置的 `jnp.concatenate`
    外，`shard_map` 还包含一个未平铺和一个块转置。
- en: Physical data movement is not possible on outputs, no matter the output pspec.
    Instead, `out_specs` just encodes how to assemble the block outputs into `Array`s,
    or physically how to interpret the buffers across devices as the physical layout
    of a single logical `Array`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出上不可能进行物理数据移动，无论输出 pspec 如何。相反，`out_specs` 只是编码如何将块输出组装成 `Array`，或者物理上如何将缓冲区解释为单个逻辑
    `Array` 的物理布局。
- en: API Specification
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: API 规范
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'where:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '`mesh` encodes devices arranged in an array and with associated axis names,
    just like it does for `xmap` and for `sharding.NamedSharding`;'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mesh` 编码设备按照数组排列，并且具有相关联的轴名称，就像对 `xmap` 和 `sharding.NamedSharding` 也是如此；'
- en: '`in_specs` and `out_specs` are `PartitionSpec`s which can [affinely](https://en.wikipedia.org/wiki/Substructural_type_system)
    mention axis names from `mesh` (not separate logical names as in `xmap`) to express
    slicing/unconcatenation and concatenation of inputs and outputs, respectively
    (not unstacking and stacking like `pmap` and `xmap` do), with unmentioned names
    corresponding to replication and untiling (assert-replicated-so-give-me-one-copy),
    respectively;'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_specs` 和 `out_specs` 是 `PartitionSpec`，它们可以[仿射地](https://en.wikipedia.org/wiki/Substructural_type_system)提到
    `mesh` 中的轴名称（不像 `xmap` 中的分开的逻辑名称）来表示输入和输出的切片/非拼接和拼接，分别（不像 `pmap` 和 `xmap` 那样的解包和堆叠），未提到的名称对应于复制和未平铺（断言已复制，因此给我一个副本）；'
- en: the shapes of the arguments passed to `f` have the same ranks as the arguments
    passed to `shard_map`-of-`f` (unlike `pmap` and `xmap` where the ranks are reduced),
    and the shape of an argument to `f` is computed from the shape `shape` of the
    corresponding argument to `shard_map`-of-`f` and the corresponding `PartitionSpec`
    spec as roughly `tuple(sz // (1 if n is None else mesh.shape[n]) for sz, n in
    zip(shape, spec))`;
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递给`f`的参数的形状与传递给`shard_map`-of-`f`的参数的形状相同（不像`pmap`和`xmap`，其中形状被降低），而且参数传递给`f`的形状是从对应于`shard_map`-of-`f`的形状`shape`和相应的`PartitionSpec`
    spec计算得到的，大致为`tuple(sz // (1 if n is None else mesh.shape[n]) for sz, n in zip(shape,
    spec))`；
- en: the body of `f` can apply collectives using names from `mesh`.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数`f`的主体可以使用来自`mesh`的名称应用收集操作。
- en: '`shmap` is eager by default, meaning that we dispatch computations primitive-by-primitive,
    so that the user can employ Python control flow on fully replicated values and
    interactive `pdb` debugging to print any values. To stage out and end-to-end compile
    a `shmap`ped function, just put a `jit` around it. A consequence is that `shmap`
    doesn’t have its own dispatch and compilation paths like `xmap` and `pmap` currently
    do; it’s just the `jit` path.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`shmap`默认是急切的，这意味着我们逐个原语地调度计算，使用户能够在完全复制的值上使用Python控制流和交互式`pdb`调试以打印任何值。要将`shmap`函数进行阶段输出并进行端到端编译，只需在其周围放置一个`jit`。一个结果是，`shmap`没有像当前的`xmap`和`pmap`那样有其自己的调度和编译路径；它只是`jit`路径的一部分。'
- en: 'When it’s staged out by e.g. an enclosing `jit`, the lowering of `shmap` to
    StableHLO is trivial: it just involves switching into ‘manual SPMD mode’ on the
    inputs, and switching back on the outputs. (We don’t currently plan to support
    partially-manual-partially-automatic modes.)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当它被例如封闭的`jit`阶段输出时，将`shmap`降低到StableHLO是微不足道的：它仅涉及切换到输入的“手动SPMD模式”，并在输出上切换回来。（我们目前不计划支持部分手动部分自动模式。）
- en: The interaction with effects is the same as with `pmap`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与效果的交互与`pmap`的交互相同。
- en: 'The interaction with autodiff is also just like `pmap` (rather than attempting
    the new semantics that `xmap` did, corresponding to having unmapped intermediates
    and hence `grad`’s `reduce_axes` as well as making `psum` transpose to `pbroadcast`
    rather than `psum`). But it thus inherits an unsolved problem from `pmap`: in
    some cases, instead of transposing `psum` to `psum`, and thus performing a backward
    pass `psum` corresponding to the forward pass `psum`, it can be beneficial to
    move the backward pass `psum` to elsewhere in the backward pass, exploiting linearity.
    Many advanced `pmap` users addressed this challenge by using `custom_vjp` to implement
    `psum_idrev` and `id_psumrev` functions, but since it’s easy to accidentally leave
    those imbalanced, that technique is a foot-cannon. We have some ideas on how to
    provide this functionality in a safer way.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与自动微分的交互也与`pmap`类似（而不是尝试`xmap`所做的新语义，对应于具有未映射中间变量的`grad`的`reduce_axes`以及使`psum`转置为`pbroadcast`而不是`psum`）。但是它因此继承了来自`pmap`的一个未解决的问题：在某些情况下，将后向传播的`psum`移动到后向传播的其他位置，利用线性特性，而不是将`psum`转置为`psum`，从而执行与前向传播`psum`对应的后向传播`psum`，这可能是有益的。许多高级的`pmap`用户通过使用`custom_vjp`来实现`psum_idrev`和`id_psumrev`函数来解决这一挑战，但由于很容易意外地使其失衡，这种技术是有风险的。我们对如何以更安全的方式提供此功能有一些想法。
- en: When should you use `shmap` and when should you use `pjit`?
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时应该使用`shmap`，何时应该使用`pjit`？
- en: 'One philosophy is: it is almost always simpler to write a program in `jit==pjit`
    — but if a given part of the program is less optimized by the compiler than it
    could be, drop into `shmap`!'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一种哲学是：在`jit==pjit`中编写程序通常更简单 —— 但是如果程序的某个部分的优化程度不如编译器可能的话，就使用`shmap`！
- en: A realistic transformer example
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个现实的变换器示例
- en: In fact, we can implement a simple version of the [“collective matmul”](https://dl.acm.org/doi/pdf/10.1145/3567955.3567959)
    algorithm recently introduced in XLA to overlap communication and computation
    using `shmap` and 30 lines of Python. The basic idea of the algorithm can be grasped
    with a simple example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以使用30行Python实现简单版本的[“集体矩阵乘法”](https://dl.acm.org/doi/pdf/10.1145/3567955.3567959)算法，该算法最近在XLA中引入，以重叠通信和计算使用`shmap`。算法的基本思想可以通过一个简单的例子掌握。
- en: Suppose we want to compute `C = A @ B` where `A` is sharded by a 1D mesh on
    the 0-th dimension while `B` and `C` are replicated.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要计算`C = A @ B`，其中`A`由第0维的1D网格分片，而`B`和`C`是复制的。
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A profile shows the blocking all-gather across 8 devices before the matmul can
    start. This is suboptimal because `A` is sharded on a non-contracting dimension,
    and each shard of `A` can be matmul’ed with `B` independently and this chunked
    computation can be overlapped with fetching of the next shard of `A` from another
    device.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件显示了在矩阵乘法开始之前，所有设备上的阻塞全收集。这是次优的，因为`A`在非收缩维上被分片，每个`A`的分片可以独立地与`B`进行矩阵乘法，并且这种分块计算可以与从另一设备获取下一个`A`分片重叠。
- en: '![image](img/98f8dcc74f709dc1ea95f4279bb677d6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![image](img/98f8dcc74f709dc1ea95f4279bb677d6.png)'
- en: This overlap can be implemented using `shmap` and explicit collectives.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重叠可以通过`shmap`和显式集体来实现。
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A profile shows that the all-gather is gone, and replaced with overlapped matmul
    with async collective permute. This profile matches very closely with the collective
    matmul paper result.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个配置文件显示，全收集消失了，并且用异步集体置换的重叠矩阵乘法替换。此配置文件与集体矩阵乘法论文结果非常接近。
- en: '![image](img/506d1f99cb0fb29ebabfd38d364ba03b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![image](img/506d1f99cb0fb29ebabfd38d364ba03b.png)'
- en: This collective matmul technique can be used to speed up feedforward blocks
    in transformer layers. This typically consists of two matrix multiplications followed
    by a `ReduceScatter` (to resolve partial sums from a parallelized matrix multiplication)
    and preceded by an `AllGather` (to collect the sharded dimensions along some axes
    and allow partial sum computation). Together, the `ReduceScatter` from one layer
    and the `AllGather` for the next amount to an `AllReduce`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集体矩阵乘法技术可以用于加速变压器层中的前馈块。这通常包括两个矩阵乘法，后跟一个`ReduceScatter`（用于解决并行矩阵乘法的部分和）和前导的`AllGather`（用于沿某些轴收集分片维度并允许部分和计算）。在一起，一层的`ReduceScatter`和下一层的`AllGather`相当于一个`AllReduce`。
- en: In a typical profile, the two matmuls will be followed by an `AllReduce`, and
    they will not be overlapped. Collective matmul can be used to achieve the overlap,
    but is difficult to trigger, has a minimum slice size and does not yet cover all
    topologies, tensor shapes and variants of collective matmul (i.e latency and throughput
    optimized variants). [In a recent paper](https://arxiv.org/abs/2211.05102), we
    found a ~40% gain in many circumstances from manually implementing collective
    matmul variants in `shmap` style.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型配置文件中，两个矩阵乘法后将跟随一个`AllReduce`，它们不会重叠。集体矩阵乘法可以用来实现重叠，但很难触发，具有最小切片大小，并且尚未涵盖所有拓扑结构、张量形状和集体矩阵乘法的变体（即延迟和吞吐量优化的变体）。[在最近的一篇论文中](https://arxiv.org/abs/2211.05102)，我们发现，在许多情况下，通过手动实现集体矩阵乘法变体，可以获得约40%的增益，类似于`shmap`风格。
- en: But it isn’t always more complex! We expect this to be a much more natural way
    to think about pipelined computation, and plan to do some demos of that soon!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不总是更复杂！我们预计这将是一种更自然的管道计算方式，计划很快进行一些演示！
- en: Another realistic example
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另一个现实例子
- en: 'Here’s how `shmap` might look in a transformer layer pass with a 2D weight
    gathered pattern ([paper](https://arxiv.org/abs/2211.05102), Sec 3.2.3 on p. 5):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了`shmap`在变换器层传递中的样子，采用了2D权重收集模式（[论文](https://arxiv.org/abs/2211.05102)，第3.2.3节，第5页）：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the profile below, both the first and second matmul were replaced by manually
    lowered versions, where the compute (fusions) are fully overlapped with the communication
    (ppermute)! One fun hint that we are using a latency optimised variant is that
    the ppmerute pixels are jittered — because there are two overlapping ppermutes
    using opposite ICI axes at the same time!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的配置文件中，第一和第二个矩阵乘法都被手动降低版本替换，计算（融合）完全与通信（ppermute）重叠！一个有趣的提示是，我们使用的是延迟优化变体，因此ppmerute像素是抖动的
    — 因为同时使用两个重叠的ppermute，使用相反的ICI轴！
- en: All-to-all is much harder to overlap, so was left on the table.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 全对全的重叠要困难得多，因此被搁置了。
- en: '![image](img/88cbec721d0e2b6e21326f0696cac68a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![image](img/88cbec721d0e2b6e21326f0696cac68a.png)'
- en: Why don’t `pmap` or `xmap` already solve this?
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么`pmap`或`xmap`还没有解决这个问题？
- en: '`pmap` was our first multi-device parallelism API. It follows the per-device-code-and-explicit-collectives
    school. But it had major shortcomings which make it unsuitable for today’s programs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`pmap`是我们的第一个多设备并行性API。它遵循每设备代码和显式集体的学派。但它存在重大缺陷，使其不适用于今天的程序：'
- en: '**Mapping multiple axes required nested `pmap`s.** Not only are nested `pmap`s
    cumbersome to write, but also they make it difficult to control (or even predict)
    the device placement of data and computation, and difficult to preserve data sharding
    (see the next two bullets). Today’s programs require multiple axes of parallelism.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**映射多个轴需要嵌套 `pmap`。** 不仅嵌套 `pmap` 写起来麻烦，而且很难控制（甚至预测）数据和计算的设备放置，也很难保留数据分片（参见接下来的两个子弹）。如今的程序需要多个轴的并行处理。'
- en: '**Controlling device placement was impossible.** Especially with multiple axes
    of parallelism, programmers need to control how those axes are aligned with hardware
    resources and their communication topologies. But (nested) `pmap` doesn’t offer
    control over how mapped program instances are placed on hardware; there’s just
    an automatic device order which the user can’t control. ([Gopher](https://arxiv.org/abs/2112.11446)’s
    use of `axis_index_groups` and a single un-nested `pmap` was essentially a hack
    to get around this by flattening multiple axes of parallelism down to one.)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无法控制设备放置。** 特别是在多轴并行处理时，程序员需要控制这些轴如何与硬件资源及其通信拓扑对齐。但（嵌套）`pmap` 不提供如何在硬件上放置映射程序实例的控制；用户只能使用自动设备顺序，无法控制它。([Gopher](https://arxiv.org/abs/2112.11446)
    使用 `axis_index_groups` 和单个未嵌套的 `pmap` 基本上是一种通过将多个并行轴压缩为一个轴来绕过此问题的方法。)'
- en: '**`jit`/`pjit` composability.** `jit`-of-`pmap` is a performance footgun, as
    is nesting `pmap`s, as is e.g. `scan`-of-`pmap`, because sharding is not preserved
    when returning from an inner `pmap`. To preserve sharding we would need pattern
    matching on jaxprs to ensure we’re working with perfectly nested pmaps, or a pmap
    just inside a `jit`. Moreover, `pjit` was no help here because `pmap` targets
    XLA replicas while `pjit` targets the XLA SPMD Partitioner, and composing those
    two is hard.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`jit`/`pjit` 可组合性。** `jit`-of-`pmap` 是一个性能陷阱，像是嵌套 `pmap`、例如 `scan`-of-`pmap`
    一样，因为从内部 `pmap` 返回时未能保留分片。要保留分片，我们需要在 jaxprs 上进行模式匹配，以确保我们正在处理完全嵌套的 pmaps，或者在
    `jit` 内部只有一个 pmap。此外，`pjit` 无助于此处，因为 `pmap` 面向 XLA 副本，而 `pjit` 则面向 XLA SPMD Partitioner，这两者的组合很困难。'
- en: '**`jax.Array` compatibility (and hence `pjit` compatibility).** Because the
    sharding of `pmap` outputs can’t be expressed as `Shardings` / `OpShardings`,
    due to `pmap`’s stacking rather than concatenative semantics, the output of a
    `pmap` computation can’t currently be passed to a `pjit` computation without bouncing
    to host (or dispatching a reshaping computation).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`jax.Array` 兼容性（因此 `pjit` 兼容性）。** 由于 `pmap` 输出的分片不能表示为 `Shardings` / `OpShardings`，因为
    `pmap` 使用的是堆叠而不是连接语义，所以目前无法将 `pmap` 计算的输出直接传递给 `pjit` 计算，而需要经过主机反弹（或调度重塑计算）。'
- en: '**Multi-controller semantics (and hence `pjit` compatibility).** Multi-controller
    `pmap` concatenates values across controllers, which works well but differs from
    single-controller `pmap`’s stacking semantics. More practically, it precludes
    the use of non-fully-addressable `jax.Array` inputs and outputs as we use with
    multi-controller `pjit`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多控制器语义（因此 `pjit` 兼容性）。** 多控制器 `pmap` 在控制器间连接值，这很有效，但与单控制器 `pmap` 的堆叠语义不同。更实际地说，它排除了与多控制器
    `pjit` 一起使用的非完全可寻址 `jax.Array` 输入和输出的可能性。'
- en: '**Eager mode.** We didn’t make `pmap` eager-first, and though we eventually
    (after 4+ years!) added eager operation with `disable_jit()`, the fact that `pmap`
    has `jit` fused into it means it has its own compilation and dispatch path (actually
    two dispatch paths: in Python for handling `Tracer`s, and in C++ for performance
    on raw `Array` inputs!), a heavy implementation burden.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**急切模式。** 我们没有将 `pmap` 设计为急切模式，尽管最终（四年多后！）通过 `disable_jit()` 添加了急切操作，但事实上 `pmap`
    中融入了 `jit` 意味着它有自己的编译和调度路径（实际上有两个调度路径：Python 处理 `Tracer`，以及 C++ 处理原始 `Array` 输入！），这是一个沉重的实现负担。'
- en: '**Reshapes needed in the caller.** A typical use case with `pmap` on 8 devices
    might look like starting with a batch axis of size 128, reshaping it to split
    into two axes with sizes (8, 16), and then `pmap`ping over the first. These reshapes
    are awkward and the compiler often interprets them as copies instead of view —
    increasing memory and time usage.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调用方需要重塑。** 在 8 个设备上使用 `pmap` 的典型用例可能看起来是从大小为 128 的批处理轴开始，将其重塑为大小为 (8, 16)
    的两个轴，然后在第一个轴上进行 `pmap`。这些重塑是笨拙的，编译器通常将它们解释为复制而不是视图，增加了内存和时间的使用。'
- en: These shortcomings aren’t so bad when only doing batch data parallelism. But
    when more parallelism is involved, `pmap` just can’t cut it!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺点在仅进行批量数据并行时并不算太糟糕。但是当涉及更多并行处理时，`pmap` 就显得力不从心！
- en: '`xmap` paved the way as a next-gen evolution of `pmap` and solved (almost)
    all these issues. `shmap` follows in `xmap`’s footsteps and solves these problems
    in essentially the same ways; indeed, `shmap` is like a specialized subset of
    `xmap` (what some call the “hard `xmap`” subset), with a few tweaks.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`xmap`作为`pmap`的下一代演进铺平了道路并解决了（几乎）所有这些问题。`shmap`则沿着`xmap`的步伐前行，并以基本相同的方式解决了这些问题；实际上，`shmap`就像是`xmap`的一个专门子集（有些人称之为“硬`xmap`”子集），只是稍作调整。'
- en: For the initial prototype, we chose to implement `shmap` as a separate primitive
    from `xmap`, because limiting the set of features it supports makes it easier
    to focus on the core functionality. For example, `shmap` doesn’t allow unmapped
    intermediates, making it easier not to worry about the interactions between named
    axes and autodiff. Furthermore, not having to reason about interactions of all
    pairs of features makes it easier to add capabilities beyond what’s implemented
    in `xmap` today, such as support for eager mode.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初始原型，我们选择将`shmap`实现为与`xmap`分离的单独原语，因为限制它支持的功能集使得更容易专注于核心功能。例如，`shmap`不允许未映射的中间值，这样就更容易不用担心命名轴与自动微分之间的交互。此外，不需要考虑所有功能对之间的交互使得可以更容易地添加超出当前`xmap`实现的功能，比如支持急切模式。
- en: Both `shmap` and `xmap` share significant portions of the lowering code. We
    could consider merging both in the future, or even focusing solely on `shmap`,
    depending on how the usage will evolve.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`shmap`和`xmap`都共享降低代码的重要部分。未来我们可以考虑合并两者，或者甚至完全专注于`shmap`，这取决于使用方式的演变。'
