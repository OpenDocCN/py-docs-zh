- en: Writing TPU kernels with Pallas
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Pallasç¼–å†™TPUå†…æ ¸
- en: åŸæ–‡ï¼š[`jax.readthedocs.io/en/latest/pallas/tpu/details.html`](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[`jax.readthedocs.io/en/latest/pallas/tpu/details.html`](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html)
- en: This page focuses on the details that are important when attempting to run Pallas
    kernels on Google TPUs. For one, the TPU backend is still in an experimental phase,
    and only a subset of JAX NumPy will be accepted. Furthermore, writing performant
    code for TPUs might require thinking carefully about the native capabilities of
    the hardware. While many patterns that are unnatural to the hardware will be accepted,
    they might end up requiring software emulation, and can slow down the computation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬é¡µå…³æ³¨è¯•å›¾åœ¨Google TPUä¸Šè¿è¡ŒPallaså†…æ ¸æ—¶çš„é‡è¦ç»†èŠ‚ã€‚é¦–å…ˆï¼ŒTPUåç«¯ä»å¤„äºå®éªŒé˜¶æ®µï¼Œå¹¶ä¸”åªæ¥å—JAX NumPyçš„å­é›†ã€‚æ­¤å¤–ï¼Œä¸ºTPUç¼–å†™é«˜æ€§èƒ½ä»£ç å¯èƒ½éœ€è¦ä»”ç»†è€ƒè™‘ç¡¬ä»¶çš„æœ¬æœºèƒ½åŠ›ã€‚è™½ç„¶è®¸å¤šå¯¹ç¡¬ä»¶ä¸è‡ªç„¶çš„æ¨¡å¼å°†è¢«æ¥å—ï¼Œä½†å®ƒä»¬æœ€ç»ˆå¯èƒ½éœ€è¦è½¯ä»¶æ¨¡æ‹Ÿï¼Œå¹¶å¯èƒ½å‡æ…¢è®¡ç®—é€Ÿåº¦ã€‚
- en: Warning
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: This feature should still be considered experimental as work is still in progress
    (in particular on improving the error messages).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åŠŸèƒ½ä»åº”è§†ä¸ºå®éªŒæ€§åŠŸèƒ½ï¼Œå› ä¸ºå·¥ä½œä»åœ¨è¿›è¡Œä¸­ï¼ˆç‰¹åˆ«æ˜¯åœ¨æ”¹è¿›é”™è¯¯æ¶ˆæ¯æ–¹é¢ï¼‰ã€‚
- en: Note
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: While all the features described here are experimental, we remain very serious
    about maintaining their correctness. As such, it might not be uncommon to see
    a â€œnot implementedâ€ error while attempting to write TPU kernels. But, if a kernel
    is accepted by the compiler, it *must* return the expected results.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ­¤å¤„æè¿°çš„æ‰€æœ‰åŠŸèƒ½éƒ½æ˜¯å®éªŒæ€§çš„ï¼Œä½†æˆ‘ä»¬ä»ç„¶éå¸¸è®¤çœŸåœ°ç»´æŠ¤å…¶æ­£ç¡®æ€§ã€‚å› æ­¤ï¼Œåœ¨å°è¯•ç¼–å†™TPUå†…æ ¸æ—¶å¯èƒ½çœ‹åˆ°â€œæœªå®ç°â€é”™è¯¯å¹¶ä¸ç½•è§ã€‚ä½†æ˜¯ï¼Œå¦‚æœç¼–è¯‘å™¨æ¥å—äº†å†…æ ¸ï¼Œå®ƒ*å¿…é¡»*è¿”å›é¢„æœŸçš„ç»“æœã€‚
- en: If you see unexpected outputs, please compare them against a kernel run with
    `interpret=True` passed in to `pallas_call`. If the results diverge, please file
    a [bug report](https://github.com/google/jax/issues/new/choose).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çœ‹åˆ°æ„å¤–çš„è¾“å‡ºï¼Œè¯·å°†å…¶ä¸ä¼ é€’`interpret=True`åˆ°`pallas_call`çš„å†…æ ¸è¿è¡Œè¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœç»“æœä¸ä¸€è‡´ï¼Œè¯·æäº¤[é”™è¯¯æŠ¥å‘Š](https://github.com/google/jax/issues/new/choose)ã€‚
- en: What is a TPU?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯TPUï¼Ÿ
- en: '![A TPUv4 board](https://lh3.googleusercontent.com/PBWR5LFWaz8Nx4F7vRstDjt_nvUYdfxe9H3O9i3KMam_RmmwIOQMr1GAq3RUfowET2cK5kAcb_zGpw=e14-rw-lo-sc0xffffff-w2540)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![ä¸€ä¸ªTPUv4æ¿](https://lh3.googleusercontent.com/PBWR5LFWaz8Nx4F7vRstDjt_nvUYdfxe9H3O9i3KMam_RmmwIOQMr1GAq3RUfowET2cK5kAcb_zGpw=e14-rw-lo-sc0xffffff-w2540)'
- en: TPU is a hardware accelerator developed at Google. You can think of TPUs as
    GPUs, but specialized for machine learning workloads specifically. As such, their
    architecture differs quite significantly. However, we believe that Pallas can
    make it easy to start writing TPU kernels, even without having a full understanding
    of the underlying hardware. Having said that, understanding the hardware well
    will certainly make it easier to write performant kernels.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TPUæ˜¯Googleå¼€å‘çš„ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚æ‚¨å¯ä»¥å°†TPUè§†ä¸ºä¸“é—¨ç”¨äºæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½çš„GPUã€‚å› æ­¤ï¼Œå®ƒä»¬çš„æ¶æ„æœ‰ç›¸å½“å¤§çš„å·®å¼‚ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ç›¸ä¿¡Pallaså¯ä»¥ä½¿æ‚¨è½»æ¾å¼€å§‹ç¼–å†™TPUå†…æ ¸ï¼Œå³ä½¿æ‚¨æ²¡æœ‰å®Œå…¨ç†è§£åº•å±‚ç¡¬ä»¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¯è™½å¦‚æ­¤ï¼Œæ·±å…¥äº†è§£ç¡¬ä»¶å°†ç¡®å®ä½¿ç¼–å†™é«˜æ€§èƒ½å†…æ ¸å˜å¾—æ›´åŠ å®¹æ˜“ã€‚
- en: In a nutshell, the main difference between TPUs and GPUs is that TPUs are sequential
    machines with a very wide vector register (kind of like a CPU!). At the same time,
    they allow the software to schedule certain operations in the background, making
    them execute asynchronously with respect to the main instruction stream. This
    includes things like HBM memory accesses (which cannot be issued directly, but
    instead have to be prefetched to lower levels of the memory hierarchy by the DMA
    subunits), matrix multiplies (supported by the MXU unit) or matrix transpositions
    and permutes (supported by the XLU unit).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è¨€ä¹‹ï¼ŒTPUä¸GPUçš„ä¸»è¦åŒºåˆ«åœ¨äºTPUæ˜¯é¡ºåºæœºå™¨ï¼Œå…·æœ‰éå¸¸å®½çš„å‘é‡å¯„å­˜å™¨ï¼ˆç±»ä¼¼äºCPUï¼ï¼‰ã€‚ä¸æ­¤åŒæ—¶ï¼Œå®ƒä»¬å…è®¸è½¯ä»¶å®‰æ’æŸäº›æ“ä½œåœ¨åå°æ‰§è¡Œï¼Œä½¿å…¶ä¸ä¸»æŒ‡ä»¤æµå¼‚æ­¥æ‰§è¡Œã€‚è¿™åŒ…æ‹¬HBMå†…å­˜è®¿é—®ï¼ˆæ— æ³•ç›´æ¥å‘å‡ºï¼Œè€Œæ˜¯å¿…é¡»é€šè¿‡DMAå­å•å…ƒé¢„å–åˆ°è¾ƒä½å±‚æ¬¡çš„å†…å­˜å±‚æ¬¡ç»“æ„ï¼‰ã€çŸ©é˜µä¹˜æ³•ï¼ˆç”±MXUå•å…ƒæ”¯æŒï¼‰æˆ–çŸ©é˜µè½¬ç½®å’Œç½®æ¢ï¼ˆç”±XLUå•å…ƒæ”¯æŒï¼‰ã€‚
- en: If youâ€™re interested in learning more about the TPU architecture in detail,
    we recommend reading a collection of papers published over the years. While many
    of them talk about specific TPU generations, many of the ideas described transfer
    to later generations as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯¹è¯¦ç»†äº†è§£TPUæ¶æ„æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å»ºè®®é˜…è¯»å¤šå¹´æ¥å‘è¡¨çš„ä¸€ç³»åˆ—è®ºæ–‡é›†ã€‚è™½ç„¶è®¸å¤šè®ºæ–‡è°ˆè®ºç‰¹å®šçš„TPUä»£ï¼Œä½†å…¶ä¸­è®¸å¤šæè¿°çš„æ€æƒ³ä¹Ÿé€‚ç”¨äºåç»­ä»£ã€‚
- en: '[A Domain-Specific Supercomputer for Training Deep Neural Networks](https://dl.acm.org/doi/10.1145/3360307)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç”¨äºè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„é¢†åŸŸç‰¹å®šè¶…çº§è®¡ç®—æœº](https://dl.acm.org/doi/10.1145/3360307)'
- en: '[The Design Process for Googleâ€™s Training Chips: TPUv2 and TPUv3](https://ieeexplore.ieee.org/document/9351692)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GoogleåŸ¹è®­èŠ¯ç‰‡çš„è®¾è®¡è¿‡ç¨‹ï¼šTPUv2å’ŒTPUv3](https://ieeexplore.ieee.org/document/9351692)'
- en: '[Ten Lessons From Three Generations Shaped Googleâ€™s TPUv4i : Industrial Product](https://ieeexplore.ieee.org/document/9499913)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä¸‰ä»£å½¢å¡‘Google TPUv4içš„åå¤§ç»éªŒæ•™è®­ï¼šå·¥ä¸šäº§å“](https://ieeexplore.ieee.org/document/9499913)'
- en: '[TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with
    Hardware Support for Embeddings](https://dl.acm.org/doi/abs/10.1145/3579371.3589350)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TPU v4ï¼šæ”¯æŒåµŒå…¥å¼ç¡¬ä»¶çš„æœºå™¨å­¦ä¹ å…‰å­¦å¯é‡æ„è¶…çº§è®¡ç®—æœº](https://dl.acm.org/doi/abs/10.1145/3579371.3589350)'
- en: Noteworthy properties and restrictions
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„å±æ€§å’Œé™åˆ¶
- en: '`BlockSpec`s and grid iteration'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`BlockSpec`s å’Œç½‘æ ¼è¿­ä»£'
- en: '`BlockSpec`s generally behave as expected in Pallas â€” every invocation of the
    kernel body gets access to slices of the inputs and is meant to initialize a slice
    of the output.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Pallasä¸­ï¼Œ`BlockSpec`sé€šå¸¸æŒ‰é¢„æœŸè¡Œä¸ºâ€”â€”æ¯æ¬¡æ ¸å¿ƒä½“è°ƒç”¨éƒ½ä¼šè®¿é—®è¾“å…¥çš„ç‰‡æ®µï¼Œå¹¶ä¸”æ—¨åœ¨åˆå§‹åŒ–è¾“å‡ºçš„ä¸€ä¸ªç‰‡æ®µã€‚
- en: Warning
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: Not all window shapes are supported. If the last two dimensions of your input
    are larger than 8 and 128 respectively, the window shape in those dimensions must
    be a multiple of the respective factor. If the input dimension is smaller, the
    window should span the full dimension.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶éæ‰€æœ‰çš„çª—å£å½¢çŠ¶éƒ½å—æ”¯æŒã€‚å¦‚æœä½ çš„è¾“å…¥çš„æœ€åä¸¤ä¸ªç»´åº¦åˆ†åˆ«å¤§äº8å’Œ128ï¼Œé‚£ä¹ˆè¿™äº›ç»´åº¦ä¸­çš„çª—å£å½¢çŠ¶å¿…é¡»æ˜¯å¯¹åº”å› å­çš„å€æ•°ã€‚å¦‚æœè¾“å…¥ç»´åº¦è¾ƒå°ï¼Œåˆ™çª—å£åº”è·¨è¶Šæ•´ä¸ªç»´åº¦ã€‚
- en: 'One interesting aspect of Pallas TPU kernels is the way they handle memory
    spaces: While the inputs to `pallas_call` will often reside in HBM (the main TPU
    memory), the references passed in to the kernel body will point to buffers in
    lower levels of memory hierarchy (VMEM or SMEM). This enables the kernel body
    to write and read them at very high speeds, while all the communication with HBM
    (which has very high latency) is handled by the compiler and overlapped with compute.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas TPUæ ¸å¿ƒçš„ä¸€ä¸ªæœ‰è¶£æ–¹é¢æ˜¯å®ƒä»¬å¤„ç†å†…å­˜ç©ºé—´çš„æ–¹å¼ï¼šè™½ç„¶`pallas_call`çš„è¾“å…¥é€šå¸¸é©»ç•™åœ¨HBMï¼ˆä¸»TPUå†…å­˜ï¼‰ä¸­ï¼Œä½†ä¼ é€’åˆ°æ ¸å¿ƒä½“çš„å¼•ç”¨å°†æŒ‡å‘å†…å­˜å±‚æ¬¡ç»“æ„è¾ƒä½çš„ç¼“å†²åŒºï¼ˆVMEMæˆ–SMEMï¼‰ã€‚è¿™ä½¿å¾—æ ¸å¿ƒä½“èƒ½å¤Ÿä»¥éå¸¸é«˜çš„é€Ÿåº¦è¯»å†™å®ƒä»¬ï¼Œè€Œæ‰€æœ‰ä¸HBMçš„é€šä¿¡ï¼ˆå…·æœ‰éå¸¸é«˜çš„å»¶è¿Ÿï¼‰ç”±ç¼–è¯‘å™¨å¤„ç†å¹¶ä¸è®¡ç®—é‡å ã€‚
- en: 'Whatâ€™s more, compared to GPUs, TPUs are actually highly sequential machines.
    Ergo, the grid is generally not processed in parallel, but sequentially, in lexicographic
    order (though see the Multicore TPU configurations section for exceptions). This
    unlocks some interesting capabilities:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸GPUç›¸æ¯”ï¼ŒTPUå®é™…ä¸Šæ˜¯é«˜åº¦åºåˆ—åŒ–çš„æœºå™¨ã€‚å› æ­¤ï¼Œç½‘æ ¼é€šå¸¸ä¸æ˜¯å¹¶è¡Œå¤„ç†çš„ï¼Œè€Œæ˜¯æŒ‰å­—å…¸é¡ºåºé¡ºåºå¤„ç†ï¼ˆå°½ç®¡è¯·å‚é˜…å¤šæ ¸TPUé…ç½®éƒ¨åˆ†çš„ä¾‹å¤–æƒ…å†µï¼‰ã€‚è¿™è§£é”äº†ä¸€äº›æœ‰è¶£çš„åŠŸèƒ½ï¼š
- en: When two (lexicographically) consecutive grid indices use the same slice of
    an input, the HBM transfer for the second iteration is skipped, as the data is
    already available.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“ä¸¤ä¸ªï¼ˆæŒ‰å­—å…¸é¡ºåºï¼‰è¿ç»­çš„ç½‘æ ¼ç´¢å¼•ä½¿ç”¨ç›¸åŒè¾“å…¥çš„ç‰‡æ®µæ—¶ï¼Œç¬¬äºŒæ¬¡è¿­ä»£çš„HBMä¼ è¾“å°†è¢«è·³è¿‡ï¼Œå› ä¸ºæ•°æ®å·²ç»å¯ç”¨ã€‚
- en: Multiple invocations of the kernel body can write to the same slice of the output,
    without any risk of race conditions. However, we do require that all invocations
    that write to a particular slice are consecutive.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šä¸ªæ ¸å¿ƒä½“è°ƒç”¨å¯ä»¥å‘è¾“å‡ºçš„åŒä¸€ç‰‡æ®µå†™å…¥ï¼Œè€Œä¸ä¼šæœ‰ä»»ä½•ç«æ€æ¡ä»¶çš„é£é™©ã€‚ä½†æˆ‘ä»¬ç¡®å®è¦æ±‚å†™å…¥ç‰¹å®šç‰‡æ®µçš„æ‰€æœ‰è°ƒç”¨æ˜¯è¿ç»­çš„ã€‚
- en: The â€œconsecutiveâ€ restriction on the output usually means that the some prefix
    of the grid dimensions always vary the slice of the output an invocation needs
    to access, while the output window remains constant for the remaining suffix.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¾“å‡ºçš„â€œè¿ç»­â€é™åˆ¶é€šå¸¸æ„å‘³ç€ç½‘æ ¼ç»´åº¦çš„æŸäº›å‰ç¼€æ€»æ˜¯å˜åŒ–ï¼Œè€Œè°ƒç”¨éœ€è¦è®¿é—®çš„è¾“å‡ºçª—å£å¯¹äºå…¶ä½™åç¼€ä¿æŒä¸å˜ã€‚
- en: 'For example, when implementing a Pallas TPU kernel for matrix multiplication,
    one would generally use a 3 dimensional grid: the first two dimensions would correspond
    to slicing along the first axis of the left operand and the second axis of the
    second operand. The third and *last* grid axis would tile the reduction dimension.
    The grid axis corresponding to the reduction dimension has to be the last one,
    since the output window does not vary along this axis. The output reference can
    be then used as an accumulator for partial results.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨å®ç°çŸ©é˜µä¹˜æ³•çš„Pallas TPUæ ¸å¿ƒæ—¶ï¼Œé€šå¸¸ä¼šä½¿ç”¨ä¸‰ç»´ç½‘æ ¼ï¼šå‰ä¸¤ä¸ªç»´åº¦å¯¹åº”äºæ²¿å·¦æ“ä½œæ•°çš„ç¬¬ä¸€è½´å’Œç¬¬äºŒæ“ä½œæ•°çš„ç¬¬äºŒè½´åˆ‡ç‰‡ã€‚ç¬¬ä¸‰å’Œ*æœ€å*ç½‘æ ¼è½´å°†ç“¦ç‰‡åŒ–å‡å°‘ç»´åº¦ã€‚ä¸å‡å°‘ç»´åº¦å¯¹åº”çš„ç½‘æ ¼è½´å¿…é¡»æ˜¯æœ€åä¸€ä¸ªï¼Œå› ä¸ºè¾“å‡ºçª—å£æ²¿æ­¤è½´ä¸å˜ã€‚è¾“å‡ºå¼•ç”¨éšåå¯ç”¨ä½œéƒ¨åˆ†ç»“æœçš„ç´¯åŠ å™¨ã€‚
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: VMEM is fairly large for such a low-level memory hierarchy (16MB+), making it
    possible to use large window sizes. And, oftentimes, the larger the window size,
    the better the eventual hardware utilization will be. However, it is possible
    to specify a window size that (together with space necessary to hold spilled vector
    registers) exceeds the size of VMEM. In this case, you will likely see a low-level
    compiler error message complaining about an out-of-memory error.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™æ ·ä¸€ä¸ªä½çº§å†…å­˜å±‚æ¬¡ç»“æ„ï¼ˆ16MB+ï¼‰ï¼ŒVMEMç›¸å½“å¤§ï¼Œè¿™ä½¿å¾—å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„çª—å£å¤§å°ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œçª—å£å¤§å°è¶Šå¤§ï¼Œæœ€ç»ˆç¡¬ä»¶åˆ©ç”¨ç‡å°±è¶Šå¥½ã€‚ç„¶è€Œï¼Œå¯èƒ½ä¼šæŒ‡å®šä¸€ä¸ªçª—å£å¤§å°ï¼Œè¯¥å¤§å°ï¼ˆåŠ ä¸Šä¿å­˜æº¢å‡ºçŸ¢é‡å¯„å­˜å™¨æ‰€éœ€çš„ç©ºé—´ï¼‰è¶…è¿‡äº†VMEMçš„å¤§å°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½ä¼šçœ‹åˆ°ä¸€ä¸ªä½çº§ç¼–è¯‘å™¨é”™è¯¯æ¶ˆæ¯ï¼ŒæŠ±æ€¨å†…å­˜ä¸è¶³é”™è¯¯ã€‚
- en: Dimension ordering is meaningful
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»´åº¦æ’åºæ˜¯æœ‰æ„ä¹‰çš„
- en: In JAX programs, the ordering of intermediate arrays inside `jax.jit` usually
    has no impact on performance, as the compiler is free to rearrange them. However,
    as Pallas is meant to expose lower-level capabilities, the dimension order can
    have great impact on the quality of generated code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨JAXç¨‹åºä¸­ï¼Œ`jax.jit`å†…éƒ¨æ•°ç»„çš„æ’åºé€šå¸¸ä¸ä¼šå½±å“æ€§èƒ½ï¼Œå› ä¸ºç¼–è¯‘å™¨å¯ä»¥è‡ªç”±åœ°é‡æ–°æ’åˆ—å®ƒä»¬ã€‚ä½†æ˜¯ï¼Œç”±äºPallasæ—¨åœ¨æš´éœ²æ›´ä½çº§çš„åŠŸèƒ½ï¼Œç»´åº¦é¡ºåºå¯¹ç”Ÿæˆçš„ä»£ç è´¨é‡æœ‰å¾ˆå¤§å½±å“ã€‚
- en: Recall that the TPUs perform bulk of the computation on 2D vector registers.
    Pallas TPU will only ever consider mapping the last two dimensions of intermediate
    arrays to those vector register dimensions (sublanes and lanes respectively).
    An array of shape `(n, 1, 1)` is guaranteed to require at least `n` vector registers
    to represent. If `n` becomes too large, this can lead to spills, and potential
    VMEM OOM errors due to an overly large memory footprint. But it also might not
    â€” the low-level compiler is free to rearrange the instructions to lower the register
    pressure, and is in fact very good at it. Still, it is a good rule of thumb to
    keep the last two dimensions large (especially the last dimension), while keeping
    the leading dimensions small.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼ŒTPUä¸»è¦åœ¨2DçŸ¢é‡å¯„å­˜å™¨ä¸Šæ‰§è¡Œå¤§éƒ¨åˆ†è®¡ç®—ã€‚Pallas TPUåªä¼šè€ƒè™‘å°†ä¸­é—´æ•°ç»„çš„æœ€åä¸¤ä¸ªç»´åº¦æ˜ å°„åˆ°è¿™äº›çŸ¢é‡å¯„å­˜å™¨ç»´åº¦ï¼ˆå­é€šé“å’Œé€šé“ï¼‰ã€‚å½¢çŠ¶ä¸º`(n,
    1, 1)`çš„æ•°ç»„ä¿è¯éœ€è¦è‡³å°‘`n`ä¸ªçŸ¢é‡å¯„å­˜å™¨æ¥è¡¨ç¤ºã€‚å¦‚æœ`n`å˜å¾—å¤ªå¤§ï¼Œåˆ™å¯èƒ½ä¼šå¯¼è‡´æº¢å‡ºï¼Œå¹¶ç”±äºè¿‡å¤§çš„å†…å­˜å ç”¨è€Œå¯¼è‡´VMEMå†…å­˜ä¸è¶³é”™è¯¯ã€‚ä½†è¿™ä¹Ÿå¯èƒ½ä¸ä¼šå‘ç”Ÿ
    â€” ä½çº§ç¼–è¯‘å™¨å¯ä»¥é‡æ–°æ’åˆ—æŒ‡ä»¤ä»¥é™ä½å¯„å­˜å™¨å‹åŠ›ï¼Œå¹¶ä¸”å®é™…ä¸Šåœ¨è¿™æ–¹é¢åšå¾—éå¸¸å¥½ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¿æŒæœ€åä¸¤ä¸ªç»´åº¦å¤§ï¼ˆç‰¹åˆ«æ˜¯æœ€åä¸€ä¸ªç»´åº¦ï¼‰ï¼ŒåŒæ—¶ä½¿å‰å¯¼ç»´åº¦ä¿æŒå°æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç»éªŒæ³•åˆ™ã€‚
- en: Multicore TPU configurations
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šæ ¸TPUé…ç½®
- en: 'In newer TPU generations, the two cores on a chip are often abstracted as a
    single device. To take advantage of multiple cores, Pallas has to break the sequential
    grid execution guarantees, and will need to parallelize one of the grid axes over
    cores. This is an opt-in procedure. To allow that, `pallas_call` requires an extra
    parameter named `dimension_semantics`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ›´æ–°çš„TPUç”Ÿæˆä¸­ï¼ŒèŠ¯ç‰‡ä¸Šçš„ä¸¤ä¸ªæ ¸å¿ƒé€šå¸¸è¢«æŠ½è±¡ä¸ºå•ä¸ªè®¾å¤‡ã€‚ä¸ºäº†åˆ©ç”¨å¤šä¸ªæ ¸å¿ƒï¼ŒPallaså¿…é¡»æ‰“ç ´é¡ºåºç½‘æ ¼æ‰§è¡Œçš„ä¿è¯ï¼Œå¹¶ä¸”éœ€è¦åœ¨æ ¸å¿ƒä¸Šå¹¶è¡ŒåŒ–ä¸€ä¸ªç½‘æ ¼è½´ã€‚è¿™æ˜¯ä¸€ä¸ªé€‰æ‹©åŠ å…¥çš„è¿‡ç¨‹ã€‚ä¸ºäº†å…è®¸è¿™æ ·åšï¼Œ`pallas_call`éœ€è¦ä¸€ä¸ªé¢å¤–çš„åä¸º`dimension_semantics`çš„å‚æ•°ï¼š
- en: That parameter is a list, with as many entries as many axes there are in the
    grid. Only `parallel` dimensions can be partitioned over cores. As a rule of thumb,
    the dimensions are parallel, unless the output window does not vary. As such,
    `dimension_semantics` is always a number of `parallel` axes followed by a number
    of `arbitrary` axes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‚æ•°æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶æ¡ç›®æ•°é‡ä¸ç½‘æ ¼ä¸­çš„è½´æ•°é‡ç›¸åŒã€‚åªæœ‰`parallel`ç»´åº¦å¯ä»¥åœ¨æ ¸å¿ƒä¸Šåˆ†åŒºã€‚ä½œä¸ºä¸€ä¸ªç»éªŒæ³•åˆ™ï¼Œç»´åº¦æ˜¯å¹¶è¡Œçš„ï¼Œé™¤éè¾“å‡ºçª—å£ä¸å˜ã€‚å› æ­¤ï¼Œ`dimension_semantics`å§‹ç»ˆæ˜¯ä¸€äº›`parallel`è½´çš„æ•°å­—ï¼Œåè·Ÿä¸€äº›`arbitrary`è½´çš„æ•°å­—ã€‚
- en: While partitioning a kernel over a 2-core TPU device often leads to a 2x speedup,
    it can be in fact significantly smaller. This is especially true if different
    instances of the body have highly varying cost. If all of the expensive steps
    get mapped to one core, but all cheap steps are assigned to the other, the second
    core will be sitting idle until the first one completes its tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡åœ¨2æ ¸TPUè®¾å¤‡ä¸Šåˆ†åŒºå†…æ ¸é€šå¸¸ä¼šå¯¼è‡´2å€é€Ÿåº¦æå‡ï¼Œä½†å®é™…ä¸Šå¯èƒ½ä¼šæ˜¾è‘—å°äºæ­¤å€¼ã€‚ç‰¹åˆ«æ˜¯å¦‚æœä½“çš„ä¸åŒå®ä¾‹å…·æœ‰éå¸¸ä¸åŒçš„æˆæœ¬ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºçœŸå®ã€‚å¦‚æœæ‰€æœ‰æ˜‚è´µçš„æ­¥éª¤éƒ½æ˜ å°„åˆ°ä¸€ä¸ªæ ¸å¿ƒï¼Œè€Œæ‰€æœ‰å»‰ä»·çš„æ­¥éª¤éƒ½åˆ†é…ç»™å¦ä¸€ä¸ªæ ¸å¿ƒï¼Œåˆ™ç¬¬äºŒä¸ªæ ¸å¿ƒå°†åœ¨ç¬¬ä¸€ä¸ªå®Œæˆå…¶ä»»åŠ¡ä¹‹å‰å¤„äºç©ºé—²çŠ¶æ€ã€‚
- en: Pallas TPU generally favors partitioning axes of a size that is a multiple of
    the number of TPU cores, and prefers to partition leading grid axes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas TPUé€šå¸¸åå¥½å°†å¤§å°ä¸ºTPUæ ¸å¿ƒæ•°é‡å€æ•°çš„è½´è¿›è¡Œåˆ†åŒºï¼Œå¹¶ä¸”æ›´å–œæ¬¢åˆ†åŒºä¸»å¯¼çš„ç½‘æ ¼è½´ã€‚
- en: Placing operands in SMEM
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ“ä½œæ•°æ”¾å…¥SMEM
- en: Most of the compute on the TPU will happen on the vector unit. Still, there
    are many cases where it is useful to perform a number of scalar operations, e.g.,
    to carry out control-flow. For that reason, TPUs come with a separate scalar unit,
    and a separate scalar memory (SMEM) attached to it. As a rule of thumb, any data
    used to perform control-flow decisions should be placed in SMEM.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°TPUè®¡ç®—å°†åœ¨å‘é‡å•å…ƒä¸Šè¿›è¡Œã€‚ç„¶è€Œï¼Œæœ‰è®¸å¤šæƒ…å†µä¸‹è¿›è¡Œä¸€äº›æ ‡é‡æ“ä½œæ˜¯æœ‰ç”¨çš„ï¼Œä¾‹å¦‚æ‰§è¡Œæ§åˆ¶æµã€‚å› æ­¤ï¼ŒTPUé…å¤‡äº†ä¸€ä¸ªå•ç‹¬çš„æ ‡é‡å•å…ƒï¼Œå¹¶é™„æœ‰ä¸€ä¸ªå•ç‹¬çš„æ ‡é‡å­˜å‚¨å™¨ï¼ˆSMEMï¼‰ã€‚æŒ‰ç…§ä¸€ä¸ªç»éªŒæ³•åˆ™ï¼Œç”¨äºæ‰§è¡Œæ§åˆ¶æµå†³ç­–çš„ä»»ä½•æ•°æ®åº”æ”¾ç½®åœ¨SMEMä¸­ã€‚
- en: SMEM is a low-latency memory that supports random access, but lets you only
    read and write 32-bit values with a single instruction (very small compared to
    the 4KBi granularity of VMEM transactions, but much more flexible due to lack
    of alignment requirements!).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SMEMæ˜¯ä¸€ç§ä½å»¶è¿Ÿå†…å­˜ï¼Œæ”¯æŒéšæœºè®¿é—®ï¼Œä½†åªèƒ½ç”¨å•ä¸ªæŒ‡ä»¤è¯»å†™32ä½å€¼ï¼ˆä¸VMEMäº‹åŠ¡çš„4KBiç²’åº¦ç›¸æ¯”éå¸¸å°ï¼Œä½†ç”±äºæ²¡æœ‰å¯¹é½è¦æ±‚è€Œæ›´åŠ çµæ´»ï¼ï¼‰ã€‚
- en: The scalar memory is also very useful when implementing kernels that do not
    access the tiles of inputs in a regular pattern, such as when writing block-sparse
    kernels. In Pallas, this can be achieved by replacing the `grid` argument to `pallas_call`
    with a `grid_spec` of `PrefetchScalarGridSpec` with a non-zero `num_scalar_prefetch`
    argument. If `num_scalar_prefetch` is `n`, then the first `n` arguments to `pallas_call`
    will be placed in SMEM. No `BlockSpec`s should be specified for those arguments.
    But, the `BlockSpec`s for all subsequent arguments will receive not only the grid
    indices, but also the SMEM references to the leading operands.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å®ç°ä¸æŒ‰è§„åˆ™æ¨¡å¼è®¿é—®è¾“å…¥å—çš„å†…æ ¸æ—¶ï¼Œæ ‡é‡å†…å­˜ä¹Ÿéå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚ç¼–å†™å—ç¨€ç–å†…æ ¸æ—¶ã€‚åœ¨Pallasä¸­ï¼Œå¯ä»¥é€šè¿‡å°†`pallas_call`çš„`grid`å‚æ•°æ›¿æ¢ä¸ºå…·æœ‰éé›¶`num_scalar_prefetch`å‚æ•°çš„`PrefetchScalarGridSpec`çš„`grid_spec`æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å¦‚æœ`num_scalar_prefetch`ä¸º`n`ï¼Œé‚£ä¹ˆ`pallas_call`çš„å‰`n`ä¸ªå‚æ•°å°†æ”¾ç½®åœ¨SMEMä¸­ã€‚å¯¹äºè¿™äº›å‚æ•°ï¼Œä¸åº”æŒ‡å®šä»»ä½•`BlockSpec`ã€‚ä½†æ˜¯ï¼Œå¯¹äºæ‰€æœ‰åç»­å‚æ•°çš„`BlockSpec`ï¼Œä¸ä»…ä¼šæ”¶åˆ°ç½‘æ ¼ç´¢å¼•ï¼Œè¿˜ä¼šæ”¶åˆ°é¢†å…ˆæ“ä½œæ•°çš„SMEMå¼•ç”¨ã€‚
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: We are working on implementing examples for this feature. Stay tuned!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨åŠªåŠ›å®ç°æ­¤åŠŸèƒ½çš„ç¤ºä¾‹ã€‚æ•¬è¯·å…³æ³¨ï¼
- en: Supported data types
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ”¯æŒçš„æ•°æ®ç±»å‹
- en: 'At the moment Pallas TPU only supports the following data types:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒPallas TPUä»…æ”¯æŒä»¥ä¸‹æ•°æ®ç±»å‹ï¼š
- en: '`jnp.float32`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.float32`'
- en: '`jnp.bfloat16`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.bfloat16`'
- en: '`jnp.int*` (all precisions, except for `jnp.int4`)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.int*`ï¼ˆæ‰€æœ‰ç²¾åº¦ï¼Œé™¤äº†`jnp.int4`ï¼‰'
- en: '`jnp.uint*` (all precisions)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.uint*`ï¼ˆæ‰€æœ‰ç²¾åº¦ï¼‰'
- en: Computation placement
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—æ”¾ç½®
- en: All scalar (i.e. 0D) arrays will be stored in scalar registers, and operations
    on then will be executed on the scalar core. All other operations (even on single-element,
    but 1D+ arrays) will be executed on the vector core.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ ‡é‡ï¼ˆå³0Dï¼‰æ•°ç»„å°†å­˜å‚¨åœ¨æ ‡é‡å¯„å­˜å™¨ä¸­ï¼Œå¹¶åœ¨æ ‡é‡æ ¸å¿ƒä¸Šæ‰§è¡Œæ“ä½œã€‚æ‰€æœ‰å…¶ä»–æ“ä½œï¼ˆç”šè‡³æ˜¯å¯¹å•ä¸ªå…ƒç´ ä½†æ˜¯1D+æ•°ç»„çš„æ“ä½œï¼‰å°†åœ¨å‘é‡æ ¸å¿ƒä¸Šæ‰§è¡Œã€‚
- en: Supported operations
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¯æŒçš„æ“ä½œ
- en: Matrix multiplication
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: çŸ©é˜µä¹˜æ³•
- en: Matrix multiplication always produces results in the float32 format. If your
    inputs are not float32, we recommend using `lax.dot` with `preferred_element_type`
    set to `jnp.float32`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µä¹˜æ³•å§‹ç»ˆä»¥`float32`æ ¼å¼ç”Ÿæˆç»“æœã€‚å¦‚æœæ‚¨çš„è¾“å…¥ä¸æ˜¯float32ï¼Œå»ºè®®ä½¿ç”¨`lax.dot`å¹¶å°†`preferred_element_type`è®¾ç½®ä¸º`jnp.float32`ã€‚
- en: When using `lax.dot_general`, it is possible to fuse transpositions of the last
    two dimensions of matrix multiplication operands into the operation, which can
    improve overall kernel performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨`lax.dot_general`æ—¶ï¼Œå¯ä»¥å°†çŸ©é˜µä¹˜æ³•æ“ä½œæ•°çš„æœ€åä¸¤ä¸ªç»´åº¦çš„è½¬ç½®èåˆåˆ°æ“ä½œä¸­ï¼Œè¿™å¯ä»¥æé«˜æ•´ä½“å†…æ ¸æ€§èƒ½ã€‚
- en: Precision control
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç²¾åº¦æ§åˆ¶
- en: Pallas TPU lowering is aware of `jax.default_matmul_precision`. For best performance
    (and lowest precision), use `bfloat16`. If you care about numerical accuracy,
    you might want to set the precision to `float32`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas TPUçš„é™ä½è€ƒè™‘åˆ°äº†`jax.default_matmul_precision`ã€‚ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼ˆå’Œæœ€ä½ç²¾åº¦ï¼‰ï¼Œè¯·ä½¿ç”¨`bfloat16`ã€‚å¦‚æœæ‚¨å…³å¿ƒæ•°å€¼ç²¾åº¦ï¼Œå¯èƒ½éœ€è¦å°†ç²¾åº¦è®¾ç½®ä¸º`float32`ã€‚
- en: Warning
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: Even if you pass in 32-bit operands to a matrix multiplication, they will be
    rounded to `bfloat16` unless `float32` precision is requested.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿å°†32ä½æ“ä½œæ•°ä¼ é€’ç»™çŸ©é˜µä¹˜æ³•ï¼Œé™¤éè¯·æ±‚`float32`ç²¾åº¦ï¼Œå¦åˆ™å®ƒä»¬å°†ä¼šè¢«å››èˆäº”å…¥ä¸º`bfloat16`ã€‚
- en: Transposition
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è½¬ç½®
- en: If the value has at least 4 dimensions, arbitrary transpositions of all but
    the last two axes are free. Otherwise, only the transposition of the last two
    axes is implemented. Note that some transpositions of the last two dimensions
    can be fused into matrix multiplication.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå€¼è‡³å°‘æœ‰4ä¸ªç»´åº¦ï¼Œåˆ™é™¤äº†æœ€åä¸¤ä¸ªè½´ä»¥å¤–çš„ä»»æ„è½¬ç½®éƒ½æ˜¯å…è´¹çš„ã€‚å¦åˆ™ï¼Œä»…å®ç°äº†æœ€åä¸¤ä¸ªè½´çš„è½¬ç½®ã€‚è¯·æ³¨æ„ï¼Œä¸€äº›æœ€åä¸¤ä¸ªç»´åº¦çš„è½¬ç½®å¯ä»¥èåˆåˆ°çŸ©é˜µä¹˜æ³•ä¸­ã€‚
- en: Accessing memory
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¿é—®å†…å­˜
- en: Arbitrary slices of references can be read or updated, subject to implementation
    constraints. Currently, no restrictions are placed on inputs that are 32-bit wide,
    but only some slicing patterns are supported for narrower types. Reads and writes
    that are aligned to multiples of, and have a length that is a multiple of 8 and
    128 respectively in the last two dimensions are always supported.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è¯»å–æˆ–æ›´æ–°å¼•ç”¨çš„ä»»æ„ç‰‡æ®µï¼Œå—å®ç°çº¦æŸçš„é™åˆ¶ã€‚ç›®å‰ï¼Œå¯¹äºå®½åº¦ä¸º 32 ä½çš„è¾“å…¥æ²¡æœ‰é™åˆ¶ï¼Œä½†åªæ”¯æŒæŸäº›æ›´çª„ç±»å‹çš„åˆ‡ç‰‡æ¨¡å¼ã€‚æ€»æ˜¯æ”¯æŒæœ€åä¸¤ä¸ªç»´åº¦ä¸­åˆ†åˆ«æ˜¯
    8 å’Œ 128 çš„å€æ•°çš„å¯¹é½è¯»å†™ã€‚
- en: Reads and writes to vector memory generally happen on tiles of shape `(8, 128)`.
    As such, when reading or writing to references that have at least two dimensions,
    the best performance is achieved when the base offset of the memory access has
    indices divisible by the tiling, and the size of the read region is a multiple
    of the tile size.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸åœ¨å‘é‡å†…å­˜çš„è¯»å†™å‘ç”Ÿåœ¨å½¢çŠ¶ä¸º `(8, 128)` çš„ç“¦ç‰‡ä¸Šã€‚å› æ­¤ï¼Œå½“è¯»å–æˆ–å†™å…¥è‡³å°‘æœ‰ä¸¤ä¸ªç»´åº¦çš„å¼•ç”¨æ—¶ï¼Œæœ€ä½³æ€§èƒ½æ˜¯åœ¨å†…å­˜è®¿é—®çš„åŸºç¡€åç§»å…·æœ‰ç“¦ç‰‡å¯æ•´é™¤çš„ç´¢å¼•ï¼Œå¹¶ä¸”è¯»å–åŒºåŸŸçš„å¤§å°æ˜¯ç“¦ç‰‡å¤§å°çš„å€æ•°ã€‚
- en: Elementwise operations
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€å…ƒç´ æ“ä½œ
- en: Many elementwise operations are supported. It is worth noting that the hardware
    generally only supports elementwise computation using 32-bit types. When loading
    operands that use lower-precision types, they should generally be upcast to a
    32-bit type before applying elementwise ops.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒè®¸å¤šé€å…ƒç´ æ“ä½œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¡¬ä»¶é€šå¸¸ä»…æ”¯æŒä½¿ç”¨ 32 ä½ç±»å‹è¿›è¡Œé€å…ƒç´ è®¡ç®—ã€‚åœ¨åŠ è½½ä½¿ç”¨è¾ƒä½ç²¾åº¦ç±»å‹çš„æ“ä½œæ•°æ—¶ï¼Œé€šå¸¸åº”å…ˆå°†å…¶å‡çº§ä¸º 32 ä½ç±»å‹å†åº”ç”¨é€å…ƒç´ æ“ä½œã€‚
- en: 'It is worth noting that they can vary *significantly* in their cost. As such,
    we outline three categories of supported operations: cheap (ğŸŸ¢), medium (ğŸŒ•) and
    expensive (ğŸ”´).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒä»¬çš„æˆæœ¬å¯èƒ½*æ˜¾è‘—*ä¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†ä¸‰ç±»æ”¯æŒçš„æ“ä½œï¼šå»‰ä»·ï¼ˆğŸŸ¢ï¼‰ã€ä¸­ç­‰ï¼ˆğŸŒ•ï¼‰å’Œæ˜‚è´µï¼ˆğŸ”´ï¼‰ã€‚
- en: '| Operation | Cost |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| æ“ä½œ | æˆæœ¬ |'
- en: '| --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `jnp.add`, `+` | ğŸŸ¢ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.add`ï¼Œ`+` | ğŸŸ¢ |'
- en: '| `jnp.sub`, `-` | ğŸŸ¢ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.sub`ï¼Œ`-` | ğŸŸ¢ |'
- en: '| `jnp.mul`, `*` | ğŸŸ¢ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.mul`ï¼Œ`*` | ğŸŸ¢ |'
- en: '| `/`, `//`, `%` | ğŸŒ• |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `/`ï¼Œ`//`ï¼Œ`%` | ğŸŒ• |'
- en: '| `jnp.max`, `jnp.min` | ğŸŸ¢ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.max`ï¼Œ`jnp.min` | ğŸŸ¢ |'
- en: '| `jnp.where` (select) | ğŸŸ¢ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.where`ï¼ˆé€‰æ‹©ï¼‰ | ğŸŸ¢ |'
- en: '| `jnp.abs` | ğŸŸ¢ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.abs` | ğŸŸ¢ |'
- en: '| `&#124;`, `^`, `&`, `~` | ğŸŸ¢ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `|`ï¼Œ`^`ï¼Œ`&`ï¼Œ`~` | ğŸŸ¢ |'
- en: '| `<<`, `>>` | ğŸŸ¢ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `<<`ï¼Œ`>>` | ğŸŸ¢ |'
- en: '| Comparisons (`==`, â€¦) | ğŸŸ¢ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| æ¯”è¾ƒè¿ç®—ï¼ˆ`==`ï¼Œ...ï¼‰ | ğŸŸ¢ |'
- en: '| Type casts (`.astype`) | ğŸŸ¢ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ç±»å‹è½¬æ¢ï¼ˆ`.astype`ï¼‰ | ğŸŸ¢ |'
- en: '| `jnp.exp` | ğŸŒ• |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.exp` | ğŸŒ• |'
- en: '| `jnp.tanh` | ğŸŒ• |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.tanh` | ğŸŒ• |'
- en: '| `jnp.pow` | ğŸŒ• |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.pow` | ğŸŒ• |'
- en: '| `jnp.sin` | ğŸ”´ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.sin` | ğŸ”´ |'
- en: '| `jnp.cos` | ğŸ”´ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.cos` | ğŸ”´ |'
- en: Many JAX functions are implemented in terms of other JAX primitives, so this
    list might not be comprehensive. For example, `jax.nn.relu` is implemented in
    terms of comparisons and `jnp.where` will work in Pallas kernels too.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤š JAX å‡½æ•°æ˜¯åŸºäºå…¶ä»– JAX åŸè¯­å®ç°çš„ï¼Œå› æ­¤æ­¤åˆ—è¡¨å¯èƒ½ä¸å®Œæ•´ã€‚ä¾‹å¦‚ï¼Œ`jax.nn.relu` æ˜¯åŸºäºæ¯”è¾ƒå®ç°çš„ï¼Œè€Œ `jnp.where`
    åœ¨ Pallas å†…æ ¸ä¸­ä¹Ÿèƒ½å·¥ä½œã€‚
- en: Array constructors
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°ç»„æ„é€ å‡½æ•°
- en: All constant array constructors are supported (`jnp.ones`, `jnp.zeros`, `jnp.full`).
    Notably, the `jax.random` module is **not** compatible with Pallas as of today.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å¸¸æ•°æ•°ç»„æ„é€ å‡½æ•°éƒ½å—æ”¯æŒï¼ˆ`jnp.ones`ï¼Œ`jnp.zeros`ï¼Œ`jnp.full`ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆªè‡³ä»Šå¤©ï¼Œ`jax.random` æ¨¡å—ä¸
    Pallas **ä¸** å…¼å®¹ã€‚
- en: Reductions
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å½’çº¦
- en: Sum, maximum and minimum reductions are supported, but only on a single array
    axis at a time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒæ±‚å’Œã€æœ€å¤§å€¼å’Œæœ€å°å€¼çš„å½’çº¦ï¼Œä½†ä¸€æ¬¡åªèƒ½åœ¨ä¸€ä¸ªæ•°ç»„è½´ä¸Šè¿›è¡Œã€‚
- en: Reductions over the last array dimension are generally the slowest. Reductions
    over the second last dimension are faster, but still slower than over the leading
    dimensions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æœ€åä¸€ä¸ªæ•°ç»„ç»´åº¦çš„å½’çº¦é€šå¸¸æ˜¯æœ€æ…¢çš„ã€‚å¯¹å€’æ•°ç¬¬äºŒä¸ªç»´åº¦çš„å½’çº¦æ›´å¿«ï¼Œä½†ä»æ¯”å‰é¢çš„ç»´åº¦æ…¢ã€‚
- en: Broadcasting
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¿æ’­
- en: The performance characteristics of broadcasting are very similar to those of
    reductions. Broadcasting along all but the two trailing dimensions is always supported
    and free. Broadcasting along the second to last dimension is slower, while broadcasting
    along the last dimension is the slowest.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¿æ’­çš„æ€§èƒ½ç‰¹æ€§ä¸å½’çº¦éå¸¸ç›¸ä¼¼ã€‚æ€»æ˜¯æ”¯æŒé™¤äº†æœ€åä¸¤ä¸ªç»´åº¦ä¹‹å¤–çš„æ‰€æœ‰å¹¿æ’­ï¼Œä¸”æ˜¯å…è´¹çš„ã€‚æ²¿ç€å€’æ•°ç¬¬äºŒä¸ªç»´åº¦è¿›è¡Œå¹¿æ’­è¾ƒæ…¢ï¼Œè€Œæ²¿ç€æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œå¹¿æ’­æœ€æ…¢ã€‚
- en: Reshapes
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é‡å¡‘
- en: As usual, reshapes in all dimensions but the last two dimensions are supported
    and free.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¸¸åœ°ï¼Œæ‰€æœ‰ç»´åº¦é™¤äº†æœ€åä¸¤ä¸ªç»´åº¦çš„é‡å¡‘éƒ½æ˜¯æ”¯æŒçš„ä¸”æ˜¯å…è´¹çš„ã€‚
- en: The only two supported cases when a reshape can modify the last two dimensions
    of an array is when (1) some leading dimensions are flattened onto the second
    to last dimension, or (2) it adds a dimension that was just removed by a reduction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€æ”¯æŒçš„æƒ…å†µæ˜¯å½“é‡å¡‘å¯ä»¥ä¿®æ”¹æ•°ç»„çš„æœ€åä¸¤ä¸ªç»´åº¦æ—¶ï¼Œå³ï¼ˆ1ï¼‰æŸäº›å‰å¯¼ç»´åº¦å±•å¹³åˆ°å€’æ•°ç¬¬äºŒä¸ªç»´åº¦ï¼Œæˆ–è€…ï¼ˆ2ï¼‰å®ƒæ·»åŠ äº†åˆšåˆšç”±å½’çº¦ç§»é™¤çš„ç»´åº¦ã€‚
- en: Control flow
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ§åˆ¶æµç¨‹
- en: The TPU backend features limited support for control flow at the moment. The
    currently supported functions are `cond`, `fori_loop` and `for_loop`. However,
    loop primitives get fully unrolled during the compilation at the moment, so try
    to keep the loop trip count reasonably small.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒTPUåç«¯å¯¹æ§åˆ¶æµçš„æ”¯æŒæœ‰é™ã€‚ç›®å‰æ”¯æŒçš„å‡½æ•°æœ‰`cond`ã€`fori_loop`å’Œ`for_loop`ã€‚ç„¶è€Œï¼Œåœ¨ç¼–è¯‘æ—¶ï¼Œå¾ªç¯åŸè¯­ä¼šå®Œå…¨å±•å¼€ï¼Œå› æ­¤è¯·å°½é‡ä¿æŒå¾ªç¯æ‰§è¡Œæ¬¡æ•°åˆç†å°ã€‚
- en: Overusing control flow can lead to significant regressions in low-level code
    generation, and it is recommended to try to squeeze as many computationally expensive
    operations into a single basic block as possible.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡åº¦ä½¿ç”¨æ§åˆ¶æµå¯èƒ½å¯¼è‡´ä½çº§ä»£ç ç”Ÿæˆä¸­çš„æ˜¾è‘—å›å½’ï¼Œå»ºè®®å°½é‡å°†å¤šä¸ªè®¡ç®—å¯†é›†å‹æ“ä½œæŒ¤å…¥ä¸€ä¸ªåŸºæœ¬å—ä¸­ã€‚
