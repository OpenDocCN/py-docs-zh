- en: Pallas Quickstart
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pallas 快速入门
- en: 原文：[`jax.readthedocs.io/en/latest/pallas/quickstart.html`](https://jax.readthedocs.io/en/latest/pallas/quickstart.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/pallas/quickstart.html`](https://jax.readthedocs.io/en/latest/pallas/quickstart.html)
- en: Pallas is an extension to JAX that enables writing custom kernels for GPU and
    TPU. Pallas allows you to use the same JAX functions and APIs but operates at
    a *lower* level of abstraction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas 是 JAX 的扩展，允许为 GPU 和 TPU 编写自定义核函数。Pallas 允许您使用相同的 JAX 函数和 API，但在抽象层面上操作更低。
- en: Specifically, Pallas requires users to think about memory access and how to
    divide up computations across multiple compute units in a hardware accelerator.
    On GPUs, Pallas lowers to Triton and on TPUs, Pallas lowers to Mosaic.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Pallas 要求用户考虑内存访问以及如何在硬件加速器的多个计算单元之间分割计算。在 GPU 上，Pallas 降级为 Triton，在 TPU
    上，Pallas 降级为 Mosaic。
- en: Let’s dive into some examples.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入一些例子。
- en: 'Note: Pallas is still an experimental API and you may be broken by changes!'
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：Pallas 仍然是一个实验性 API，可能会因更改而破坏代码！
- en: Hello world in Pallas
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Pallas 中的 hello world
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We’ll first write the “hello world” in Pallas, a kernel that adds two vectors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在 Pallas 中编写“hello world”，这是一个将两个向量相加的核函数。
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**`Ref` types**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**`Ref` 类型**'
- en: Let’s dissect this function a bit. Unlike most JAX functions you’ve probably
    written, it does not take in `jax.Array`s as inputs and doesn’t return any values.
    Instead it takes in *`Ref`* objects as inputs. Note that we also don’t have any
    outputs but we are given an `o_ref`, which corresponds to the desired output.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微解析一下这个函数。与您可能编写过的大多数 JAX 函数不同，它不以 `jax.Array` 作为输入，也不返回任何值。相反，它以 *`Ref`*
    对象作为输入。请注意，我们也没有任何输出，但我们有一个 `o_ref`，它对应于所需的输出。
- en: '**Reading from `Ref`s**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**从 `Ref` 读取**'
- en: In the body, we are first reading from `x_ref` and `y_ref`, indicated by the
    `[...]` (the ellipsis means we are reading the whole `Ref`; alternatively we also
    could have used `x_ref[:]`). Reading from a `Ref` like this returns a `jax.Array`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数体中，我们首先从 `x_ref` 和 `y_ref` 中读取，用 `[...]` 表示（省略号表示我们正在读取整个 `Ref`；或者我们也可以使用
    `x_ref[:]`）。像这样从 `Ref` 中读取返回一个 `jax.Array`。
- en: '**Writing to `Ref`s**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**向 `Ref` 写入**'
- en: We then write `x + y` to `o_ref`. Mutation has not historically been supported
    in JAX – `jax.Array`s are immutable! `Ref`s are new (experimental) types that
    allow mutation under certain circumstances. We can interpret writing to a `Ref`
    as mutating its underlying buffer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将 `x + y` 写入 `o_ref`。在 JAX 中历史上并不支持突变 - `jax.Array` 是不可变的！`Ref` 是新的（实验性）类型，在某些情况下允许突变。我们可以理解为向
    `Ref` 写入是对其底层缓冲区的突变。
- en: So we’ve written what we call a “kernel”, which we define as a program that
    will run as an atomic unit of execution on an accelerator, without any interaction
    with the host. How do we invoke it from a JAX computation? We use the `pallas_call`
    higher-order function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们编写了一个我们称之为“核函数”的程序，定义为在加速器上作为执行的原子单位运行，而不与主机进行任何交互。我们如何从 JAX 计算中调用它呢？我们使用
    `pallas_call` 高阶函数。
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`pallas_call` lifts the Pallas kernel function into an operation that can be
    called as part of a larger JAX program. But, to do so, it needs a few more details.
    Here we specify `out_shape`, an object that has a `.shape` and `.dtype` (or a
    list thereof). `out_shape` determines the shape/dtype of `o_ref` in our `add_vector_kernel`.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`pallas_call` 将 Pallas 核函数提升为可以作为较大 JAX 程序的一部分调用的操作。但是，为了做到这一点，它需要一些额外的细节。在这里，我们指定
    `out_shape`，一个具有 `.shape` 和 `.dtype`（或列表）的对象。`out_shape` 决定了我们在 `add_vector_kernel`
    中的 `o_ref` 的形状/数据类型。'
- en: '`pallas_call` returns a function that takes in and returns `jax.Array`s.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`pallas_call` 返回一个函数，该函数接受并返回 `jax.Array`。'
- en: '**What’s actually happening here?**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**这里实际上发生了什么？**'
- en: Thus far we’ve described how to think about Pallas kernels but what we’ve actually
    accomplished is we’re writing a function that’s executed very close to the compute
    units.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经描述了如何思考 Pallas 核函数，但我们实际上所做的是编写一个函数，该函数在计算单元附近执行。
- en: On GPU, `x_ref` corresponds to a value in high-bandwidth memory (HBM) and when
    we do `x_ref[...]` we are copying the value from HBM into static RAM (SRAM) (this
    is a costly operation generally speaking!). We then use GPU vector compute to
    execute the addition, then copy the resulting value in SRAM back to HBM.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上，`x_ref` 对应于高带宽内存（HBM）中的一个值，当我们执行 `x_ref[...]` 时，我们将该值从 HBM 复制到静态 RAM（SRAM）中（一般情况下这是一个昂贵的操作！）。然后，我们使用
    GPU 向量计算来执行加法，然后将结果值从 SRAM 复制回 HBM。
- en: On TPU, we do something slightly different. Before the kernel is ever executed,
    we fetch the value from HBM into SRAM. `x_ref` therefore corresponds to a value
    in SRAM and when we do `x_ref[...]` we are copying the value from SRAM into a
    register. We then use TPU vector compute to execute the addition, then copy the
    resulting value back into SRAM. After the kernel is executed, the SRAM value is
    copied back into HBM.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在TPU上，我们做了略有不同的事情。在内核被执行之前，我们从HBM中获取值到SRAM中。因此，`x_ref`对应于SRAM中的一个值，当我们执行`x_ref[...]`时，我们将该值从SRAM复制到寄存器中。然后，我们使用TPU向量计算来执行加法，然后将结果值复制回SRAM。在内核执行完毕后，将SRAM中的值复制回HBM。
- en: We are in the process of writing backend-specific Pallas guides. Coming soon!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在编写特定后端的Pallas指南。即将推出！
- en: Pallas programming model
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pallas编程模型
- en: In our “hello world” example, we wrote a very simple kernel. It takes advantage
    of the fact that our 8-sized arrays can comfortably fit inside the SRAM of hardware
    accelerators. In most real-world applications, this will not be the case!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的“hello world”示例中，我们编写了一个非常简单的内核。它利用了我们的大小为8的数组可以轻松地放入硬件加速器的SRAM中这一事实。在大多数实际应用中，情况通常并非如此！
- en: Part of writing Pallas kernels is thinking about how to take big arrays that
    live in high-bandwidth memory (HBM, also known as DRAM) and expressing computations
    that operate on “blocks” of those arrays that can fit in SRAM.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 编写Pallas内核的一部分是考虑如何处理生活在高带宽内存（HBM，也称为DRAM）中的大数组，并表达操作这些数组“块”的计算，这些块可以适应SRAM中。
- en: Grids
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格
- en: To automatically “carve” up the inputs and outputs, you provide a `grid` and
    `BlockSpec`s to `pallas_call`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要自动“切分”输入和输出，您需要向`pallas_call`提供一个`grid`和`BlockSpec`。
- en: 'A `grid` is a tuple of integers (e.g. `()`, `(2, 3, 4)`, or `(8,)`) that specifies
    an iteration space. For example, a grid `(4, 5)` would have 20 elements: `(0,
    0), (0, 1), ..., (0, 4), (1, 0), ..., (3, 4)`. We run the kernel function once
    for each element, a style of single-program multiple-data (SPMD) programming.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`grid`是一组整数的元组（例如`()`，`(2, 3, 4)`或`(8,)`），指定了一个迭代空间。例如，网格`(4, 5)`将有20个元素：`(0,
    0), (0, 1), ... , (0, 4), (1, 0), ... , (3, 4)`。我们为每个元素运行一次内核函数，这是单程序多数据（SPMD）编程风格。
- en: '![A visualization of a 2D grid](img/e76182e63f663f8a74115e9eb67c8016.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![一个二维网格的可视化](img/e76182e63f663f8a74115e9eb67c8016.png)'
- en: A 2D grid
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二维网格
- en: When we provide a `grid` to `pallas_call`, the kernel is executed as many times
    as `prod(grid)`. Each of these invocations is referred to as a “program”, To access
    which program (i.e. which element of the grid) the kernel is currently executing,
    we use `program_id(axis=...)`. For example, for invocation `(1, 2)`, `program_id(axis=0)`
    returns `1` and `program_id(axis=1)` returns `2`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向`pallas_call`提供一个`grid`时，内核将执行`prod(grid)`次。每次调用被称为“程序”，为了访问内核当前执行的程序（即`grid`的哪个元素），我们使用`program_id(axis=...)`。例如，对于调用`(1,
    2)`，`program_id(axis=0)`返回`1`，`program_id(axis=1)`返回`2`。
- en: Here’s an example kernel that uses a `grid` and `program_id`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用`grid`和`program_id`的内核示例。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We now execute it using `pallas_call` with an additional `grid` argument.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用`pallas_call`来执行它，还提供了一个额外的`grid`参数。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: On GPUs, each program is executed in parallel on separate threads. Thus, we
    need to think about race conditions on writes to HBM. A reasonable approach is
    to write our kernels in such a way that different programs write to disjoint places
    in HBM to avoid these parallel writes. On the other hand, parallelizing the computation
    is how we can execute operations like matrix multiplications really quickly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上，每个程序在单独的线程上并行执行。因此，我们需要考虑写入HBM时的竞争条件。一个合理的方法是编写我们的内核，使不同的程序写入HBM中的不同位置，以避免这些并行写入。另一方面，通过并行化计算，我们可以快速执行诸如矩阵乘法之类的操作。
- en: On TPUs, programs are executed in a combination of parallel and sequential (depending
    on the architecture) so there are slightly different considerations.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在TPU上，程序以并行和顺序（取决于架构）的组合方式执行，因此需要考虑略有不同。
- en: Block specs
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块规格
- en: With `grid` and `program_id` in mind, Pallas provides an abstraction that takes
    care of some common indexing patterns seen in a lot of kernels. To build intuition,
    let’s try to implement a matrix multiplication.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到`grid`和`program_id`，Pallas提供了一种抽象，处理了许多内核中常见的索引模式。为了建立直觉，让我们尝试实现一个矩阵乘法。
- en: A simple strategy for implementing a matrix multiplication in Pallas is to implement
    it recursively. We know our underlying hardware has support for small matrix multiplications
    (using GPU and TPU tensorcores), so we just express a big matrix multiplication
    in terms of smaller ones.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pallas 中实现矩阵乘法的一个简单策略是递归实现。我们知道我们的底层硬件支持小矩阵乘法（使用 GPU 和 TPU tensorcores），因此我们只需将大矩阵乘法表示为较小的矩阵乘法。
- en: Suppose we have input matrices \(X\) and \(Y\) and are computing \(Z = XY\).
    We first express \(X\) and \(Y\) as block matrices. \(X\) will have “row” blocks
    and \(Y\) will have “column” blocks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有输入矩阵 \(X\) 和 \(Y\) 并计算 \(Z = XY\)。我们首先将 \(X\) 和 \(Y\) 表达为块矩阵。\(X\) 将有“行”块，而
    \(Y\) 将有“列”块。
- en: \[\begin{split} \begin{align*} X = \begin{bmatrix} X_0 \\ X_1 \end{bmatrix}
    \end{align*} \end{split}\]\[ \begin{align*} Y = \begin{bmatrix} Y_0 & Y_1 \end{bmatrix}
    \end{align*} \]\[\begin{split} \begin{align*} Z &= \begin{bmatrix} X_0 \\ X_1
    \end{bmatrix} \begin{matrix} \begin{bmatrix} Y_0 & Y_1 \end{bmatrix} \\ ~ \end{matrix}
    \\ &= \begin{bmatrix} X_0 Y_0 & X_0 Y_1 \\ X_1 Y_0 & X_1 Y_1 \end{bmatrix} \end{align*}
    \end{split}\]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} X = \begin{bmatrix} X_0 \\ X_1 \end{bmatrix}
    \end{align*} \end{split}\]\[ \begin{align*} Y = \begin{bmatrix} Y_0 & Y_1 \end{bmatrix}
    \end{align*} \]\[\begin{split} \begin{align*} Z &= \begin{bmatrix} X_0 Y_0 & X_0
    Y_1 \\ X_1 Y_0 & X_1 Y_1 \end{bmatrix} \end{align*} \end{split}\]
- en: Our strategy is that because \(Z\) is also a block matrix, we can assign each
    of the programs in our Pallas kernel one of the output blocks. Computing each
    output block corresponds to doing a smaller matrix multiply between a “row” block
    of \(X\) and a “column” block of \(Y\).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略是，因为 \(Z\) 也是一个块矩阵，我们可以将我们 Pallas 内核中的每个程序分配给一个输出块。计算每个输出块相当于在 \(X\) 的“行”块和
    \(Y\) 的“列”块之间进行较小的矩阵乘法。
- en: To express this pattern, we use `BlockSpec`s. A `BlockSpec` specifies a block
    shape for each input and output, and an “index map” function, that maps a set
    of program indices to a block index.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要表达这种模式，我们使用 `BlockSpec`。`BlockSpec` 指定每个输入和输出的块形状，以及一个“索引映射”函数，将一组程序索引映射到一个块索引。
- en: '![A visualization of a BlockSpec`](img/08cdce677fc930901f3f2f8747dd3aea.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![一个 `BlockSpec` 的可视化](img/08cdce677fc930901f3f2f8747dd3aea.png)'
- en: A visualization of a `BlockSpec`
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`BlockSpec` 的可视化'
- en: For a concrete example, let’s say we’d like to multiply two `(1024, 1024)` matrices
    `x` and `y` together to produce `z`, and would like to parallelize the computation
    4 ways. We split up `z` into 4 `(512, 512)` blocks where each block is computed
    with a `(512, 1024) x (1024, 512)` matrix multiplication. To express this, we’d
    first use a `(2, 2)` grid (one block for each program).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 举个具体的例子，假设我们想要将两个 `(1024, 1024)` 矩阵 `x` 和 `y` 相乘得到 `z`，并且希望将计算并行化为4个部分。我们将 `z`
    切分为4个 `(512, 512)` 块，其中每个块使用 `(512, 1024) x (1024, 512)` 的矩阵乘法计算。为了表达这一点，我们首先使用一个
    `(2, 2)` 的网格（每个程序一个块）。
- en: 'For `x`, we use `BlockSpec(lambda i, j: (i, 0), (512, 1024))` – this carves
    `x` up into “row” blocks. To see this see how both program instances `(1, 0)`
    and `(1, 1)` pick the `(1, 0)` block in `x`. For `y`, we use a transposed version
    `BlockSpec(lambda i, j: (0, j), (1024, 512))`. Finally, for `z` we use `BlockSpec(lambda
    i, j: (i, j), (512, 512))`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 `x`，我们使用 `BlockSpec(lambda i, j: (i, 0), (512, 1024))` – 这将 `x` 切分成“行”块。观察程序实例
    `(1, 0)` 和 `(1, 1)` 如何选择 `x` 中的 `(1, 0)` 块。对于 `y`，我们使用其转置版本 `BlockSpec(lambda
    i, j: (0, j), (1024, 512))`。最后，对于 `z`，我们使用 `BlockSpec(lambda i, j: (i, j), (512,
    512))`。'
- en: These `BlockSpec`s are passed into `pallas_call` via `in_specs` and `out_specs`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 `BlockSpec` 通过 `in_specs` 和 `out_specs` 被传递给 `pallas_call`。
- en: Underneath the hood, `pallas_call` will automatically carve up your inputs and
    outputs into `Ref`s for each block that will be passed into the kernel.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，`pallas_call` 将自动将您的输入和输出划分为每个将传递到内核的块的 `Ref`。
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that this is a very naive implementation of a matrix multiplication but
    consider it a starting point for various types of optimizations. Let’s add an
    additional feature to our matrix multiply: fused activation. It’s actually really
    easy! Just pass a higher-order activation function into the kernel.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是矩阵乘法的一个非常简单的实现，但可以作为各种优化类型的起点。让我们为我们的矩阵乘法添加一个额外的特性：融合激活。这实际上非常简单！只需将一个高阶激活函数传递到内核中即可。
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To conclude, let’s highlight a cool feature of Pallas: it composes with `jax.vmap`!
    To turn this matrix multiplication into a batched version, we just need to `vmap`
    it.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们强调 Pallas 的一个很酷的特性：它可以与 `jax.vmap` 组合使用！要将此矩阵乘法转换为批处理版本，我们只需将其 `vmap`
    化。
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
