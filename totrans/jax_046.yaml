- en: Training a Simple Neural Network, with PyTorch Data Loading
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个简单的神经网络，使用PyTorch进行数据加载
- en: 原文：[`jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html`](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html`](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)
- en: '![Open in Colab](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)
    ![Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![在Colab中打开](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)
    ![在Kaggle中打开](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)'
- en: '**Copyright 2018 The JAX Authors.**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**版权所有 2018 年 JAX 作者。**'
- en: Licensed under the Apache License, Version 2.0 (the “License”); you may not
    use this file except in compliance with the License. You may obtain a copy of
    the License at
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Apache许可证第2.0版许可使用本文件；除非符合许可证，否则不得使用本文件。您可以在以下链接获取许可证的副本
- en: https://www.apache.org/licenses/LICENSE-2.0
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: https://www.apache.org/licenses/LICENSE-2.0
- en: Unless required by applicable law or agreed to in writing, software distributed
    under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS
    OF ANY KIND, either express or implied. See the License for the specific language
    governing permissions and limitations under the License.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 除非适用法律要求或书面同意，否则在许可证下发布的软件是按“原样”分发的，不提供任何明示或暗示的担保或条件。有关特定语言下的权限和限制，请参阅许可证。
- en: '![JAX](img/5f620f90762a1045911438d68b694265.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![JAX](img/5f620f90762a1045911438d68b694265.png)'
- en: Let’s combine everything we showed in the [quickstart](https://colab.research.google.com/github/google/jax/blob/main/docs/quickstart.html)
    to train a simple neural network. We will first specify and train a simple MLP
    on MNIST using JAX for the computation. We will use PyTorch’s data loading API
    to load images and labels (because it’s pretty great, and the world doesn’t need
    yet another data loading library).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们结合我们在[快速入门](https://colab.research.google.com/github/google/jax/blob/main/docs/quickstart.html)中展示的一切，来训练一个简单的神经网络。我们将首先使用JAX进行计算，指定并训练一个简单的MLP来处理MNIST数据集。我们将使用PyTorch的数据加载API加载图像和标签（因为它非常棒，世界上不需要另一个数据加载库）。
- en: Of course, you can use JAX with any API that is compatible with NumPy to make
    specifying the model a bit more plug-and-play. Here, just for explanatory purposes,
    we won’t use any neural network libraries or special APIs for building our model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以使用JAX与任何与NumPy兼容的API，以使模型的指定更加即插即用。在这里，仅用于解释目的，我们不会使用任何神经网络库或特殊的API来构建我们的模型。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Hyperparameters
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: Let’s get a few bookkeeping items out of the way.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先处理一些记录事项。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Auto-batching predictions
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动批处理预测
- en: Let us first define our prediction function. Note that we’re defining this for
    a *single* image example. We’re going to use JAX’s `vmap` function to automatically
    handle mini-batches, with no performance penalty.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义我们的预测函数。请注意，我们正在为*单个*图像示例定义这个函数。我们将使用JAX的`vmap`函数自动处理小批量，而无需性能损失。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s check that our prediction function only works on single images.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们的预测函数是否只适用于单个图像。
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point, we have all the ingredients we need to define our neural network
    and train it. We’ve built an auto-batched version of `predict`, which we should
    be able to use in a loss function. We should be able to use `grad` to take the
    derivative of the loss with respect to the neural network parameters. Last, we
    should be able to use `jit` to speed up everything.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一步，我们已经具备了定义和训练神经网络所需的所有要素。我们已经构建了`predict`的自动批处理版本，我们应该能够在损失函数中使用它。我们应该能够使用`grad`来计算损失相对于神经网络参数的导数。最后，我们应该能够使用`jit`来加速整个过程。
- en: Utility and loss functions
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实用工具和损失函数
- en: '[PRE9]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Data Loading with PyTorch
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch进行数据加载
- en: JAX is laser-focused on program transformations and accelerator-backed NumPy,
    so we don’t include data loading or munging in the JAX library. There are already
    a lot of great data loaders out there, so let’s just use them instead of reinventing
    anything. We’ll grab PyTorch’s data loader, and make a tiny shim to make it work
    with NumPy arrays.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: JAX专注于程序转换和支持加速器的NumPy，因此我们不在JAX库中包括数据加载或数据处理。已经有很多出色的数据加载器，所以我们只需使用它们，而不是重新发明轮子。我们将获取PyTorch的数据加载器，并制作一个小的shim以使其与NumPy数组兼容。
- en: '[PRE10]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Training Loop
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练循环
- en: '[PRE17]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We’ve now used the whole of the JAX API: `grad` for derivatives, `jit` for
    speedups and `vmap` for auto-vectorization. We used NumPy to specify all of our
    computation, and borrowed the great data loaders from PyTorch, and ran the whole
    thing on the GPU.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完全使用了 JAX API：`grad` 用于求导，`jit` 用于加速，`vmap` 用于自动向量化。我们使用 NumPy 来指定所有的计算，借用了
    PyTorch 中优秀的数据加载器，并且在 GPU 上运行整个过程。
