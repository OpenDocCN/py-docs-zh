- en: Training a Simple Neural Network, with PyTorch Data Loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html`](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Open in Colab](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)
    ![Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)'
  prefs: []
  type: TYPE_IMG
- en: '**Copyright 2018 The JAX Authors.**'
  prefs: []
  type: TYPE_NORMAL
- en: Licensed under the Apache License, Version 2.0 (the “License”); you may not
    use this file except in compliance with the License. You may obtain a copy of
    the License at
  prefs: []
  type: TYPE_NORMAL
- en: https://www.apache.org/licenses/LICENSE-2.0
  prefs: []
  type: TYPE_NORMAL
- en: Unless required by applicable law or agreed to in writing, software distributed
    under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS
    OF ANY KIND, either express or implied. See the License for the specific language
    governing permissions and limitations under the License.
  prefs: []
  type: TYPE_NORMAL
- en: '![JAX](img/5f620f90762a1045911438d68b694265.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s combine everything we showed in the [quickstart](https://colab.research.google.com/github/google/jax/blob/main/docs/quickstart.html)
    to train a simple neural network. We will first specify and train a simple MLP
    on MNIST using JAX for the computation. We will use PyTorch’s data loading API
    to load images and labels (because it’s pretty great, and the world doesn’t need
    yet another data loading library).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can use JAX with any API that is compatible with NumPy to make
    specifying the model a bit more plug-and-play. Here, just for explanatory purposes,
    we won’t use any neural network libraries or special APIs for building our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s get a few bookkeeping items out of the way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Auto-batching predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us first define our prediction function. Note that we’re defining this for
    a *single* image example. We’re going to use JAX’s `vmap` function to automatically
    handle mini-batches, with no performance penalty.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s check that our prediction function only works on single images.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have all the ingredients we need to define our neural network
    and train it. We’ve built an auto-batched version of `predict`, which we should
    be able to use in a loss function. We should be able to use `grad` to take the
    derivative of the loss with respect to the neural network parameters. Last, we
    should be able to use `jit` to speed up everything.
  prefs: []
  type: TYPE_NORMAL
- en: Utility and loss functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Data Loading with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX is laser-focused on program transformations and accelerator-backed NumPy,
    so we don’t include data loading or munging in the JAX library. There are already
    a lot of great data loaders out there, so let’s just use them instead of reinventing
    anything. We’ll grab PyTorch’s data loader, and make a tiny shim to make it work
    with NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Training Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve now used the whole of the JAX API: `grad` for derivatives, `jit` for
    speedups and `vmap` for auto-vectorization. We used NumPy to specify all of our
    computation, and borrowed the great data loaders from PyTorch, and ran the whole
    thing on the GPU.'
  prefs: []
  type: TYPE_NORMAL
