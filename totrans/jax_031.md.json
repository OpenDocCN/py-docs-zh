["```py\n>>> import jax\n\n>>> def f(x, y): return 2 * x + y\n>>> x, y = 3, 4\n\n>>> lowered = jax.jit(f).lower(x, y)\n\n>>> # Print lowered HLO\n>>> print(lowered.as_text())\nmodule @jit_f attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n func.func public @main(%arg0: tensor<i32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<i32> {mhlo.layout_mode = \"default\"}) -> (tensor<i32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n %c = stablehlo.constant dense<2> : tensor<i32>\n %0 = stablehlo.multiply %c, %arg0 : tensor<i32>\n %1 = stablehlo.add %0, %arg1 : tensor<i32>\n return %1 : tensor<i32>\n }\n}\n\n>>> compiled = lowered.compile()\n\n>>> # Query for cost analysis, print FLOP estimate\n>>> compiled.cost_analysis()[0]['flops']\n2.0\n\n>>> # Execute the compiled function!\n>>> compiled(x, y)\nArray(10, dtype=int32, weak_type=True) \n```", "```py\n>>> i32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n>>> jax.jit(f).lower(i32_scalar, i32_scalar).compile()(x, y)\nArray(10, dtype=int32) \n```", "```py\n>>> x_1d = y_1d = jnp.arange(3)\n>>> jax.jit(f).lower(i32_scalar, i32_scalar).compile()(x_1d, y_1d)  \n...\nTraceback (most recent call last):\nTypeError: Argument types differ from the types for which this computation was compiled. The mismatches are:\nArgument 'x' compiled with int32[] and called with int32[3]\nArgument 'y' compiled with int32[] and called with int32[3]\n\n>>> x_f = y_f = jnp.float32(72.)\n>>> jax.jit(f).lower(i32_scalar, i32_scalar).compile()(x_f, y_f)  \n...\nTraceback (most recent call last):\nTypeError: Argument types differ from the types for which this computation was compiled. The mismatches are:\nArgument 'x' compiled with int32[] and called with float32[]\nArgument 'y' compiled with int32[] and called with float32[] \n```", "```py\n>>> lowered_with_x = jax.jit(f, static_argnums=0).lower(7, 8)\n\n>>> # Lowered HLO, specialized to the *value* of the first argument (7)\n>>> print(lowered_with_x.as_text())\nmodule @jit_f attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n func.func public @main(%arg0: tensor<i32> {mhlo.layout_mode = \"default\"}) -> (tensor<i32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n %c = stablehlo.constant dense<14> : tensor<i32>\n %0 = stablehlo.add %c, %arg0 : tensor<i32>\n return %0 : tensor<i32>\n }\n}\n\n>>> lowered_with_x.compile()(5)\nArray(19, dtype=int32, weak_type=True) \n```", "```py\n>>> jax.jit(f, static_argnums=0).lower(i32_scalar, i32_scalar)  \nTraceback (most recent call last):\nTypeError: unsupported operand type(s) for *: 'int' and 'ShapeDtypeStruct'\n\n>>> jax.jit(f, static_argnums=0).lower(10, i32_scalar).compile()(5)\nArray(25, dtype=int32) \n```", "```py\n>>> def g(x):\n...   assert x.shape == (3, 2)\n...   return x @ jnp.ones(2)\n\n>>> def make_z(*shape):\n...   return jnp.arange(np.prod(shape)).reshape(shape)\n\n>>> z, zs = make_z(3, 2), make_z(4, 3, 2)\n\n>>> g_jit = jax.jit(g)\n>>> g_aot = jax.jit(g).lower(z).compile()\n\n>>> jax.vmap(g_jit)(zs)\nArray([[ 1.,  5.,  9.],\n [13., 17., 21.],\n [25., 29., 33.],\n [37., 41., 45.]], dtype=float32)\n\n>>> jax.vmap(g_aot)(zs)  \nTraceback (most recent call last):\nTypeError: Cannot apply JAX transformations to a function lowered and compiled for a particular signature. Detected argument of Tracer type <class 'jax._src.interpreters.batching.BatchTracer'> \n```"]