["```py\nfrom functools import partial\n\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec as P\nfrom jax.experimental import mesh_utils\nfrom jax.experimental.shard_map import shard_map\n\ndevices = mesh_utils.create_device_mesh((4, 2))\nmesh = Mesh(devices, axis_names=('i', 'j'))\n\na = jnp.arange( 8 * 16.).reshape(8, 16)\nb = jnp.arange(16 * 32.).reshape(16, 32)\n\n@partial(shard_map, mesh=mesh, in_specs=(P('i', 'j'), P('j', None)),\n         out_specs=P('i', None))\ndef matmul_basic(a_block, b_block):\n  # a_block: f32[2, 8]\n  # b_block: f32[8, 32]\n  z_partialsum = jnp.dot(a_block, b_block)\n  z_block = jax.lax.psum(z_partialsum, 'j')\n  return z_block\n\nc = matmul_basic(a, b)  # c: f32[8, 32] \n```", "```py\n@partial(shard_map, mesh=mesh, in_specs=(P('i', 'j'), P('j', None)),\n         out_specs=P('i', 'j'))\ndef matmul_reduce_scatter(a_block, b_block):\n  # c_partialsum: f32[8/X, 32]\n  c_partialsum = jnp.matmul(a_block, b_block)\n  # c_block: f32[8/X, 32/Y]\n  c_block = jax.lax.psum_scatter(c_partialsum, 'j', scatter_dimension=1, tiled=True)\n  return c_block\n\nc = matmul_reduce_scatter(a, b) \n```", "```py\npmap(f, in_axes=[0], out_axes=0)(xs) == jnp.stack([f(x) for x in xs]) \n```", "```py\ndevices = np.array(jax.devices()[:4])\nm = Mesh(devices, ('i',))  # mesh.shape['i'] = 4\n\nshard_map(f, m, in_specs=P('i'), out_specs=P('i'))(y)\n==\njnp.concatenate([f(y_blk) for y_blk in jnp.split(y, 4)]) \n```", "```py\ndevices = np.array(jax.devices())\nm = Mesh(devices.reshape(4, 2), ('i', 'j'))\n\n@partial(shard_map, mesh=m, in_specs=P('i', None), out_specs=P('i', 'j'))\ndef f1(x_block):\n  print(x_block.shape)\n  return x_block\n\nx1 = np.arange(12 * 12).reshape(12, 12)\ny = f1(x1)  # prints (3,12) \n```", "```py\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P('i', 'j'))\ndef f2(x_block):\n  print(x_block.shape)\n  return x_block\n\nx = np.arange(12 * 12).reshape(12, 12)\nx_ = jnp.tile(x, (1, mesh.axis_size['j']))  # x_ has shape (12, 24)\ny = f2(x_)  # prints (3,12), and f1(x) == f2(x_) \n```", "```py\nx = jnp.array([[3.]])\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P('i', 'j'))()\nprint(z)  # prints the same as jnp.tile(x, (4, 2))\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P('i', None))()\nprint(z)  # prints the same as jnp.tile(x, (4, 1)), or just jnp.tile(x, (4,))\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P(None, None))()\nprint(z)  # prints the same as jnp.tile(x, (1, 1)), or just x \n```", "```py\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P('i', None))\ndef f3(x_block):\n  return jax.lax.psum(x_block, 'j')\n\nx = np.arange(12 * 12).reshape(12, 12)\ny3 = f3(x)\nprint(y3.shape)  # (12,6) \n```", "```py\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P(None, 'j'))\ndef f4(x_block):\n  return jax.lax.psum(x_block, 'i')\n\nx = np.arange(12 * 12).reshape(12, 12)\ny4 = f4(x)\nprint(y4.shape)  # (3,12)\n\n@partial(shard_map, mesh=m, in_specs=P('i', 'j'), out_specs=P(None, None))\ndef f5(x_block):\n  return jax.lax.psum(x_block, ('i', 'j'))\n\ny5 = f5(x)\nprint(y5.shape)  # (3,6) \n```", "```py\nfrom jax.sharding import Mesh\nSpecs = PyTree[PartitionSpec]\n\ndef shard_map(f: Callable, mesh: Mesh, in_specs: Specs, out_specs: Specs\n          ) -> Callable:\n  ... \n```", "```py\nM, K, N = 4096, 2048, 1024\nA = jnp.arange(np.prod((M, K))).reshape((M, K))\nB = jnp.arange(np.prod((K, N))).reshape((K, N))\n\nmesh = Mesh(np.array(jax.devices()), axis_names=('i'))\nA_x = jax.device_put(A, NamedSharding(mesh, P('i', None)))\n\n@jax.jit\ndef f(lhs, rhs):\n  return lhs @ rhs\n\nC = f(A_x, B) \n```", "```py\ndef collective_matmul_allgather_lhs_non_contracting(lhs, rhs):\n  # lhs is the looped operand; rhs is the local operand\n  axis_size = jax.lax.psum(1, axis_name='i')\n  axis_index = jax.lax.axis_index(axis_name='i')\n  chunk_size = lhs.shape[0]\n\n  def f(i, carrys):\n    accum, lhs = carrys\n    # matmul for a chunk\n    update = lhs @ rhs\n    # circular shift to the left\n    lhs = jax.lax.ppermute(\n        lhs,\n        axis_name='i',\n        perm=[(j, (j - 1) % axis_size) for j in range(axis_size)]\n    )\n    # device 0 computes chunks 0, 1, ...\n    # device 1 computes chunks 1, 2, ...\n    update_index = (((axis_index + i) % axis_size) * chunk_size, 0)\n    accum = jax.lax.dynamic_update_slice(accum, update, update_index)\n    return accum, lhs\n\n  accum = jnp.zeros((lhs.shape[0] * axis_size, rhs.shape[1]), dtype=lhs.dtype)\n  # fori_loop cause a crash: hlo_sharding.cc:817 Check failed: !IsManual()\n  # accum, lhs = jax.lax.fori_loop(0, axis_size - 1, f, (accum, lhs))\n  for i in range(0, axis_size - 1):\n    accum, lhs = f(i, (accum, lhs))\n\n  # compute the last chunk, without the ppermute\n  update = lhs @ rhs\n  i = axis_size - 1\n  update_index = (((axis_index + i) % axis_size) * chunk_size, 0)\n  accum = jax.lax.dynamic_update_slice(accum, update, update_index)\n\n  return accum \n```", "```py\njit_sharded_f = jax.jit(shard_map(\n  collective_matmul_allgather_lhs_non_contracting, mesh,\n  in_specs=(P('i', None), P()), out_specs=P()))\nC = jit_sharded_f(A_x, B) \n```", "```py\ndef matmul_2D_wg_manual(xnorm, q_wi, layer):\n  '''Calls a custom manual implementation of matmul_reducescatter'''\n  # [batch, maxlen, embed.X] @ [heads.YZ, embed.X, q_wi_per_head]\n  # -> (matmul)\n  # -> [batch, maxlen, heads.YZ, q_wi_per_head]{x unreduced}\n  # -> (reducescatter over x into X heads, B batches)\n  # -> [batch, maxlen, heads.YZX, q_wi_per_head]\n  with jax.named_scope('q_wi'):\n    xnorm = intermediate_dtype(xnorm)\n    q_wi = matmul_reducescatter(\n        'bte,hed->bthd',\n        xnorm,\n        params.q_wi,\n        scatter_dimension=(0, 2),\n        axis_name='i',\n        layer=layer)\n   return q_wi\n\nimport partitioning.logical_to_physical as l2phys\n\ndef pjit_transformer_layer(\n    hparams: HParams, layer: int, params: weights.Layer, sin: jnp.ndarray,\n    cos: jnp.ndarray, kv_caches: Sequence[attention.KVCache],\n    x: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Forward pass through a single layer, returning output, K, V.\"\"\"\n\n  def my_layer(t, axis=0):\n  \"\"\"Gets the parameters corresponding to a given layer.\"\"\"\n    return lax.dynamic_index_in_dim(t, layer, axis=axis, keepdims=False)\n\n  # 2D: [batch.Z, time, embed.XY]\n  x = _with_sharding_constraint(\n      x, ('residual_batch', 'residual_time', 'residual_embed'))\n  xnorm = _layernorm(x)\n  # 2D: [batch, time, embed.X]\n  xnorm = _with_sharding_constraint(\n      xnorm, ('post_norm_batch', 'time', 'post_norm_embed'))\n  # jump into manual mode where you want to optimise\n  if manual:\n    q_wi = shard_map(matmul_2D_wg_manual, mesh\n                in_specs=(l2phys('post_norm_batch', 'time', 'post_norm_embed'),\n                          l2phys('layers', 'heads', 'embed', 'q_wi_per_head')),\n                out_specs=l2phys('post_norm_batch', 'time', 'heads', 'q_wi_per_head'))(xnorm, q_wi, layer)\n  else:\n    q_wi = jnp.einsum('bte,hed->bthd', xnorm, my_layer(params.q_wi))\n    # 2D: [batch, time, heads.YZX, None]\n    q_wi = _with_sharding_constraint(q_wi,\n                                   ('post_norm_batch', 'time', 'heads', 'qkv'))\n  q = q_wi[:, :, :, :hparams.qkv]\n  q = _rope(sin, cos, q)\n  # unlike in https://arxiv.org/pdf/2002.05202.pdf, PaLM implements\n  # swiGLU with full d_ff dimension, rather than 2/3 scaled\n  wi0 = q_wi[:, :, :, hparams.qkv:hparams.qkv + (hparams.ff // hparams.heads)]\n  wi1 = q_wi[:, :, :, hparams.qkv + (hparams.ff // hparams.heads):]\n  kv = jnp.einsum('bte,ezd->btzd', xnorm, my_layer(params.kv))\n  k = kv[:, :, 0, :hparams.qkv]\n  v = kv[:, :, 0, hparams.qkv:]\n  k = _rope(sin, cos, k)\n\n  y_att = jnp.bfloat16(attention.attend(q, k, v, kv_caches, layer))\n\n  y_mlp = special2.swish2(wi0) * wi1\n  # 2D: [batch, time, heads.YZX, None]\n  y_mlp = _with_sharding_constraint(y_mlp,\n                                    ('post_norm_batch', 'time', 'heads', None))\n\n  y_fused = jnp.concatenate([y_att, y_mlp], axis=-1)\n  # do the second half of the mlp and the self-attn projection in parallel\n  y_out = jnp.einsum('bthd,hde->bte', y_fused, my_layer(params.o_wo))\n  # 2D: [batch.Z, time, embed.XY]\n  y_out = _with_sharding_constraint(\n      y_out, ('residual_batch', 'residual_time', 'residual_embed'))\n  z = y_out + x\n  z = _with_sharding_constraint(\n      z, ('residual_batch', 'residual_time', 'residual_embed'))\n  return z, k, v \n```"]