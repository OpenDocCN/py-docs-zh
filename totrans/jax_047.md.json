["```py\nimport functools\nimport itertools\nimport re\nimport sys\nimport time\n\nfrom matplotlib.pyplot import *\n\nimport jax\n\nfrom jax import lax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\nfrom jax import random\n\nimport numpy as np\nimport scipy as sp \n```", "```py\nnp.random.seed(10009)\n\nnum_features = 10\nnum_points = 100\n\ntrue_beta = np.random.randn(num_features).astype(jnp.float32)\nall_x = np.random.randn(num_points, num_features).astype(jnp.float32)\ny = (np.random.rand(num_points) < sp.special.expit(all_x.dot(true_beta))).astype(jnp.int32) \n```", "```py\ny \n```", "```py\narray([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], dtype=int32) \n```", "```py\ndef log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.))\n    result = result + jnp.sum(-jnp.log(1 + jnp.exp(-(2*y-1) * jnp.dot(all_x, beta))))\n    return result \n```", "```py\nlog_joint(np.random.randn(num_features)) \n```", "```py\nArray(-213.2356, dtype=float32) \n```", "```py\n# This doesn't work, because we didn't write `log_prob()` to handle batching.\ntry:\n  batch_size = 10\n  batched_test_beta = np.random.randn(batch_size, num_features)\n\n  log_joint(np.random.randn(batch_size, num_features))\nexcept ValueError as e:\n  print(\"Caught expected exception \" + str(e)) \n```", "```py\nCaught expected exception Incompatible shapes for broadcasting: shapes=[(100,), (100, 10)] \n```", "```py\ndef batched_log_joint(beta):\n    result = 0.\n    # Here (and below) `sum` needs an `axis` parameter. At best, forgetting to set axis\n    # or setting it incorrectly yields an error; at worst, it silently changes the\n    # semantics of the model.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.),\n                           axis=-1)\n    # Note the multiple transposes. Getting this right is not rocket science,\n    # but it's also not totally mindless. (I didn't get it right on the first\n    # try.)\n    result = result + jnp.sum(-jnp.log(1 + jnp.exp(-(2*y-1) * jnp.dot(all_x, beta.T).T)),\n                           axis=-1)\n    return result \n```", "```py\nbatch_size = 10\nbatched_test_beta = np.random.randn(batch_size, num_features)\n\nbatched_log_joint(batched_test_beta) \n```", "```py\nArray([-147.84033 , -207.02205 , -109.26075 , -243.80833 , -163.0291  ,\n       -143.84848 , -160.28773 , -113.771706, -126.60544 , -190.81992 ],      dtype=float32) \n```", "```py\nvmap_batched_log_joint = jax.vmap(log_joint)\nvmap_batched_log_joint(batched_test_beta) \n```", "```py\nArray([-147.84033 , -207.02205 , -109.26075 , -243.80833 , -163.0291  ,\n       -143.84848 , -160.28773 , -113.771706, -126.60544 , -190.81992 ],      dtype=float32) \n```", "```py\n@jax.jit\ndef log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=10.))\n    result = result + jnp.sum(-jnp.log(1 + jnp.exp(-(2*y-1) * jnp.dot(all_x, beta))))\n    return result\n\nbatched_log_joint = jax.jit(jax.vmap(log_joint)) \n```", "```py\ndef elbo(beta_loc, beta_log_scale, epsilon):\n    beta_sample = beta_loc + jnp.exp(beta_log_scale) * epsilon\n    return jnp.mean(batched_log_joint(beta_sample), 0) + jnp.sum(beta_log_scale - 0.5 * np.log(2*np.pi))\n\nelbo = jax.jit(elbo)\nelbo_val_and_grad = jax.jit(jax.value_and_grad(elbo, argnums=(0, 1))) \n```", "```py\ndef normal_sample(key, shape):\n  \"\"\"Convenience function for quasi-stateful RNG.\"\"\"\n    new_key, sub_key = random.split(key)\n    return new_key, random.normal(sub_key, shape)\n\nnormal_sample = jax.jit(normal_sample, static_argnums=(1,))\n\nkey = random.key(10003)\n\nbeta_loc = jnp.zeros(num_features, jnp.float32)\nbeta_log_scale = jnp.zeros(num_features, jnp.float32)\n\nstep_size = 0.01\nbatch_size = 128\nepsilon_shape = (batch_size, num_features)\nfor i in range(1000):\n    key, epsilon = normal_sample(key, epsilon_shape)\n    elbo_val, (beta_loc_grad, beta_log_scale_grad) = elbo_val_and_grad(\n        beta_loc, beta_log_scale, epsilon)\n    beta_loc += step_size * beta_loc_grad\n    beta_log_scale += step_size * beta_log_scale_grad\n    if i % 10 == 0:\n        print('{}\\t{}'.format(i, elbo_val)) \n```", "```py\n0\t-180.8538818359375\n10\t-113.06045532226562\n20\t-102.73727416992188\n30\t-99.787353515625\n40\t-98.90898132324219\n50\t-98.29745483398438\n60\t-98.18632507324219\n70\t-97.57972717285156\n80\t-97.28599548339844\n90\t-97.46996307373047\n100\t-97.4771728515625\n110\t-97.5806655883789\n120\t-97.4943618774414\n130\t-97.50271606445312\n140\t-96.86396026611328\n150\t-97.44197845458984\n160\t-97.06941223144531\n170\t-96.84028625488281\n180\t-97.21336364746094\n190\t-97.56503295898438\n200\t-97.26397705078125\n210\t-97.11979675292969\n220\t-97.39595031738281\n230\t-97.16831970214844\n240\t-97.118408203125\n250\t-97.24345397949219\n260\t-97.29788970947266\n270\t-96.69286346435547\n280\t-96.96438598632812\n290\t-97.30055236816406\n300\t-96.63591766357422\n310\t-97.0351791381836\n320\t-97.52909088134766\n330\t-97.28811645507812\n340\t-97.07321166992188\n350\t-97.15619659423828\n360\t-97.25881958007812\n370\t-97.19515228271484\n380\t-97.13092041015625\n390\t-97.11726379394531\n400\t-96.938720703125\n410\t-97.26676940917969\n420\t-97.35322570800781\n430\t-97.21007537841797\n440\t-97.28434753417969\n450\t-97.1630859375\n460\t-97.2612533569336\n470\t-97.21343994140625\n480\t-97.23997497558594\n490\t-97.14913940429688\n500\t-97.23527526855469\n510\t-96.93419647216797\n520\t-97.21209716796875\n530\t-96.82575988769531\n540\t-97.01284790039062\n550\t-96.94175720214844\n560\t-97.16520690917969\n570\t-97.29165649414062\n580\t-97.42941284179688\n590\t-97.24370574951172\n600\t-97.15222930908203\n610\t-97.49844360351562\n620\t-96.9906997680664\n630\t-96.88956451416016\n640\t-96.89968872070312\n650\t-97.13793182373047\n660\t-97.43705749511719\n670\t-96.99235534667969\n680\t-97.15623474121094\n690\t-97.1869125366211\n700\t-97.11160278320312\n710\t-97.78105163574219\n720\t-97.23226165771484\n730\t-97.16206359863281\n740\t-96.99581909179688\n750\t-96.6672134399414\n760\t-97.16795349121094\n770\t-97.51435089111328\n780\t-97.28900146484375\n790\t-96.91226196289062\n800\t-97.17100524902344\n810\t-97.29047393798828\n820\t-97.16242980957031\n830\t-97.19107055664062\n840\t-97.56382751464844\n850\t-97.00194549560547\n860\t-96.86555480957031\n870\t-96.76338195800781\n880\t-96.83660888671875\n890\t-97.12178039550781\n900\t-97.09554290771484\n910\t-97.0682373046875\n920\t-97.11947631835938\n930\t-96.87930297851562\n940\t-97.45624542236328\n950\t-96.69279479980469\n960\t-97.29376220703125\n970\t-97.3353042602539\n980\t-97.34962463378906\n990\t-97.09675598144531 \n```", "```py\nfigure(figsize=(7, 7))\nplot(true_beta, beta_loc, '.', label='Approximated Posterior Means')\nplot(true_beta, beta_loc + 2*jnp.exp(beta_log_scale), 'r.', label='Approximated Posterior $2\\sigma$ Error Bars')\nplot(true_beta, beta_loc - 2*jnp.exp(beta_log_scale), 'r.')\nplot_scale = 3\nplot([-plot_scale, plot_scale], [-plot_scale, plot_scale], 'k')\nxlabel('True beta')\nylabel('Estimated beta')\nlegend(loc='best') \n```", "```py\n<matplotlib.legend.Legend at 0x7f6a2c3c86a0> \n```"]