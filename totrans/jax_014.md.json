["```py\nimport jax\njax.devices() \n```", "```py\n[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] \n```", "```py\nimport jax.numpy as jnp\narr = jnp.arange(32.0).reshape(4, 8)\narr.devices() \n```", "```py\n{TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)} \n```", "```py\narr.sharding \n```", "```py\nSingleDeviceSharding(device=TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)) \n```", "```py\njax.debug.visualize_array_sharding(arr) \n```", "```py\n\n TPU 0 \n\n```", "```py\n# Pardon the boilerplate; constructing a sharding will become easier in future!\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec\nfrom jax.sharding import NamedSharding\nfrom jax.experimental import mesh_utils\n\nP = jax.sharding.PartitionSpec\ndevices = mesh_utils.create_device_mesh((2, 4))\nmesh = jax.sharding.Mesh(devices, ('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\nprint(sharding) \n```", "```py\nNamedSharding(mesh=Mesh('x': 2, 'y': 4), spec=PartitionSpec('x', 'y')) \n```", "```py\narr_sharded = jax.device_put(arr, sharding)\n\nprint(arr_sharded)\njax.debug.visualize_array_sharding(arr_sharded) \n```", "```py\n[[ 0\\.  1\\.  2\\.  3\\.  4\\.  5\\.  6\\.  7.]\n [ 8\\.  9\\. 10\\. 11\\. 12\\. 13\\. 14\\. 15.]\n [16\\. 17\\. 18\\. 19\\. 20\\. 21\\. 22\\. 23.]\n [24\\. 25\\. 26\\. 27\\. 28\\. 29\\. 30\\. 31.]] \n```", "```py\n\n TPU 0   TPU 1       TPU 2    TPU 3 \n\n TPU 6   TPU 7       TPU 4    TPU 5 \n\n```", "```py\n@jax.jit\ndef f_elementwise(x):\n  return 2 * jnp.sin(x) + 1\n\nresult = f_elementwise(arr_sharded)\n\nprint(\"shardings match:\", result.sharding == arr_sharded.sharding) \n```", "```py\nshardings match: True \n```", "```py\n@jax.jit\ndef f_contract(x):\n  return x.sum(axis=0)\n\nresult = f_contract(arr_sharded)\njax.debug.visualize_array_sharding(result)\nprint(result) \n```", "```py\n TPU 0,6 TPU 1,7  TPU 2,4 TPU 3,5 \n\n```", "```py\n[48\\. 52\\. 56\\. 60\\. 64\\. 68\\. 72\\. 76.] \n```", "```py\n@jax.jit\ndef f_contract_2(x):\n  out = x.sum(axis=0)\n  # mesh = jax.create_mesh((8,), 'x')\n  devices = mesh_utils.create_device_mesh(8)\n  mesh = jax.sharding.Mesh(devices, 'x')\n  sharding = jax.sharding.NamedSharding(mesh, P('x'))\n  return jax.lax.with_sharding_constraint(out, sharding)\n\nresult = f_contract_2(arr_sharded)\njax.debug.visualize_array_sharding(result)\nprint(result) \n```", "```py\n TPU 0  TPU 1    TPU 2    TPU 3    TPU 6    TPU 7    TPU 4  TPU 5 \n\n```", "```py\n[48\\. 52\\. 56\\. 60\\. 64\\. 68\\. 72\\. 76.] \n```", "```py\nfrom jax.experimental.shard_map import shard_map\nP = jax.sharding.PartitionSpec\nmesh = jax.sharding.Mesh(jax.devices(), 'x')\n\nf_elementwise_sharded = shard_map(\n    f_elementwise,\n    mesh=mesh,\n    in_specs=P('x'),\n    out_specs=P('x'))\n\narr = jnp.arange(32)\nf_elementwise_sharded(arr) \n```", "```py\nArray([ 1\\.        ,  2.682942  ,  2.818595  ,  1.28224   , -0.513605  ,\n       -0.9178486 ,  0.44116896,  2.3139732 ,  2.9787164 ,  1.824237  ,\n       -0.08804226, -0.99998045, -0.07314599,  1.8403342 ,  2.9812148 ,\n        2.3005757 ,  0.42419332, -0.92279506, -0.50197446,  1.2997544 ,\n        2.8258905 ,  2.6733112 ,  0.98229736, -0.69244075, -0.81115675,\n        0.7352965 ,  2.525117  ,  2.912752  ,  1.5418116 , -0.32726777,\n       -0.97606325,  0.19192469], dtype=float32) \n```", "```py\nx = jnp.arange(32)\nprint(f\"global shape: {x.shape=}\")\n\ndef f(x):\n  print(f\"device local shape: {x.shape=}\")\n  return x * 2\n\ny = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x) \n```", "```py\nglobal shape: x.shape=(32,)\ndevice local shape: x.shape=(4,) \n```", "```py\ndef f(x):\n  return jnp.sum(x, keepdims=True)\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x) \n```", "```py\nArray([  6,  22,  38,  54,  70,  86, 102, 118], dtype=int32) \n```", "```py\ndef f(x):\n  sum_in_shard = x.sum()\n  return jax.lax.psum(sum_in_shard, 'x')\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P())(x) \n```", "```py\nArray(496, dtype=int32) \n```", "```py\n@jax.jit\ndef layer(x, weights, bias):\n  return jax.nn.sigmoid(x @ weights + bias) \n```", "```py\nimport numpy as np\nrng = np.random.default_rng(0)\n\nx = rng.normal(size=(32,))\nweights = rng.normal(size=(32, 4))\nbias = rng.normal(size=(4,))\n\nlayer(x, weights, bias) \n```", "```py\nArray([0.02138912, 0.893112  , 0.59892005, 0.97742504], dtype=float32) \n```", "```py\nP = jax.sharding.PartitionSpec\nmesh = jax.sharding.Mesh(jax.devices(), 'x')\nsharding = jax.sharding.NamedSharding(mesh, P('x'))\n\nx_sharded = jax.device_put(x, sharding)\nweights_sharded = jax.device_put(weights, sharding)\n\nlayer(x_sharded, weights_sharded, bias) \n```", "```py\nArray([0.02138912, 0.893112  , 0.59892005, 0.97742504], dtype=float32) \n```", "```py\n@jax.jit\ndef layer_auto(x, weights, bias):\n  x = jax.lax.with_sharding_constraint(x, sharding)\n  weights = jax.lax.with_sharding_constraint(weights, sharding)\n  return layer(x, weights, bias)\n\nlayer_auto(x, weights, bias)  # pass in unsharded inputs \n```", "```py\nArray([0.02138914, 0.89311206, 0.5989201 , 0.97742516], dtype=float32) \n```", "```py\nfrom functools import partial\n\n@jax.jit\n@partial(shard_map, mesh=mesh,\n         in_specs=(P('x'), P('x', None), P(None)),\n         out_specs=P(None))\ndef layer_sharded(x, weights, bias):\n  return jax.nn.sigmoid(jax.lax.psum(x @ weights, 'x') + bias)\n\nlayer_sharded(x, weights, bias) \n```", "```py\nArray([0.02138914, 0.89311206, 0.5989201 , 0.97742516], dtype=float32) \n```"]