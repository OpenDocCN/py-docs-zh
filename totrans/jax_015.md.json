["```py\nimport jax\nimport jax.numpy as jnp\n\nclass Counter:\n  \"\"\"A simple counter.\"\"\"\n\n  def __init__(self):\n    self.n = 0\n\n  def count(self) -> int:\n  \"\"\"Increments the counter and returns the new value.\"\"\"\n    self.n += 1\n    return self.n\n\n  def reset(self):\n  \"\"\"Resets the counter to zero.\"\"\"\n    self.n = 0\n\ncounter = Counter()\n\nfor _ in range(3):\n  print(counter.count()) \n```", "```py\n1\n2\n3 \n```", "```py\ncounter.reset()\nfast_count = jax.jit(counter.count)\n\nfor _ in range(3):\n  print(fast_count()) \n```", "```py\n1\n1\n1 \n```", "```py\nself.n += 1 \n```", "```py\nCounterState = int\n\nclass CounterV2:\n\n  def count(self, n: CounterState) -> tuple[int, CounterState]:\n    # You could just return n+1, but here we separate its role as \n    # the output and as the counter state for didactic purposes.\n    return n+1, n+1\n\n  def reset(self) -> CounterState:\n    return 0\n\ncounter = CounterV2()\nstate = counter.reset()\n\nfor _ in range(3):\n  value, state = counter.count(state)\n  print(value) \n```", "```py\n1\n2\n3 \n```", "```py\nstate = counter.reset()\nfast_count = jax.jit(counter.count)\n\nfor _ in range(3):\n  value, state = fast_count(state)\n  print(value) \n```", "```py\n1\n2\n3 \n```", "```py\nclass StatefulClass\n\n  state: State\n\n  def stateful_method(*args, **kwargs) -> Output: \n```", "```py\nclass StatelessClass\n\n  def stateless_method(state: State, *args, **kwargs) -> (Output, State): \n```", "```py\nfrom typing import NamedTuple\n\nclass Params(NamedTuple):\n  weight: jnp.ndarray\n  bias: jnp.ndarray\n\ndef init(rng) -> Params:\n  \"\"\"Returns the initial model params.\"\"\"\n  weights_key, bias_key = jax.random.split(rng)\n  weight = jax.random.normal(weights_key, ())\n  bias = jax.random.normal(bias_key, ())\n  return Params(weight, bias)\n\ndef loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n  pred = params.weight * x + params.bias\n  return jnp.mean((pred - y) ** 2)\n\nLEARNING_RATE = 0.005\n\n@jax.jit\ndef update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:\n  \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n  grad = jax.grad(loss)(params, x, y)\n\n  # If we were using Adam or another stateful optimizer,\n  # we would also do something like\n  #\n  #   updates, new_optimizer_state = optimizer(grad, optimizer_state)\n  # \n  # and then use `updates` instead of `grad` to actually update the params.\n  # (And we'd include `new_optimizer_state` in the output, naturally.)\n\n  new_params = jax.tree_map(\n      lambda param, g: param - g * LEARNING_RATE, params, grad)\n\n  return new_params \n```", "```py\nimport matplotlib.pyplot as plt\n\nrng = jax.random.key(42)\n\n# Generate true data from y = w*x + b + noise\ntrue_w, true_b = 2, -1\nx_rng, noise_rng = jax.random.split(rng)\nxs = jax.random.normal(x_rng, (128, 1))\nnoise = jax.random.normal(noise_rng, (128, 1)) * 0.5\nys = xs * true_w + true_b + noise\n\n# Fit regression\nparams = init(rng)\nfor _ in range(1000):\n  params = update(params, xs, ys)\n\nplt.scatter(xs, ys)\nplt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')\nplt.legend(); \n```", "```py\n/tmp/ipykernel_2992/721844192.py:37: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n  new_params = jax.tree_map( \n```"]