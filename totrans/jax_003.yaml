- en: Quickstart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[`jax.readthedocs.io/en/latest/quickstart.html`](https://jax.readthedocs.io/en/latest/quickstart.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**JAX a library for array-oriented numerical computation (*Ã  la* [NumPy](https://numpy.org/)),
    with automatic differentiation and JIT compilation to enable high-performance
    machine learning research**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This document provides a quick overview of essential JAX features, so you can
    get started with JAX quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: JAX provides a unified NumPy-like interface to computations that run on CPU,
    GPU, or TPU, in local or distributed settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JAX features built-in Just-In-Time (JIT) compilation via [Open XLA](https://github.com/openxla),
    an open-source machine learning compiler ecosystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JAX functions support efficient evaluation of gradients via its automatic differentiation
    transformations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JAX functions can be automatically vectorized to efficiently map them over arrays
    representing batches of inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JAX can be installed for CPU on Linux, Windows, and macOS directly from the
    [Python Package Index](https://pypi.org/project/jax/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'or, for NVIDIA GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For more detailed platform-specific installation information, check out Installing
    JAX.
  prefs: []
  type: TYPE_NORMAL
- en: JAX as NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most JAX usage is through the familiar `jax.numpy` API, which is typically
    imported under the `jnp` alias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With this import, you can immediately use JAX in a similar manner to typical
    NumPy programs, including using NumPy-style array creation functions, Python functions
    and operators, and array attributes and methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Youâ€™ll find a few differences between JAX arrays and NumPy arrays once you begin
    digging-in; these are explored in [ðŸ”ª JAX - The Sharp Bits ðŸ”ª](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).
  prefs: []
  type: TYPE_NORMAL
- en: Just-in-time compilation with `jax.jit()`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX runs transparently on the GPU or TPU (falling back to CPU if you donâ€™t have
    one). However, in the above example, JAX is dispatching kernels to the chip one
    operation at a time. If we have a sequence of operations, we can use the `jax.jit()`
    function to compile this sequence of operations together using XLA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use IPythonâ€™s `%timeit` to quickly benchmark our `selu` function, using
    `block_until_ready()` to account for JAXâ€™s dynamic dispatch (See Asynchronous
    dispatch):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: (notice weâ€™ve used `jax.random` to generate some random numbers; for details
    on how to generate random numbers in JAX, check out Pseudorandom numbers).
  prefs: []
  type: TYPE_NORMAL
- en: We can speed the execution of this function with the `jax.jit()` transformation,
    which will jit-compile the first time `selu` is called and will be cached thereafter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The above timing represent execution on CPU, but the same code can be run on
    GPU or TPU, typically for an even greater speedup.
  prefs: []
  type: TYPE_NORMAL
- en: For more on JIT compilation in JAX, check out Just-in-time compilation.
  prefs: []
  type: TYPE_NORMAL
- en: Taking derivatives with `jax.grad()`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to transforming functions via JIT compilation, JAX also provides
    other transformations. One such transformation is `jax.grad()`, which performs
    [automatic differentiation (autodiff)](https://en.wikipedia.org/wiki/Automatic_differentiation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Letâ€™s verify with finite differences that our result is correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `grad()` and `jit()` transformations compose and can be mixed arbitrarily.
    In the above example we jitted `sum_logistic` and then took its derivative. We
    can go further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Beyond scalar-valued functions, the `jax.jacobian()` transformation can be
    used to compute the full Jacobian matrix for vector-valued functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For more advanced autodiff operations, you can use `jax.vjp()` for reverse-mode
    vector-Jacobian products, and `jax.jvp()` and `jax.linearize()` for forward-mode
    Jacobian-vector products. The two can be composed arbitrarily with one another,
    and with other JAX transformations. For example, `jax.jvp()` and `jax.vjp()` are
    used to define the forward-mode `jax.jacfwd()` and reverse-mode `jax.jacrev()`
    for computing Jacobians in forward- and reverse-mode, respectively. Hereâ€™s one
    way to compose them to make a function that efficiently computes full Hessian
    matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This kind of composition produces efficient code in practice; this is more-or-less
    how JAXâ€™s built-in `jax.hessian()` function is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: For more on automatic differentiation in JAX, check out Automatic differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-vectorization with `jax.vmap()`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another useful transformation is `vmap()`, the vectorizing map. It has the familiar
    semantics of mapping a function along array axes, but instead of explicitly looping
    over function calls, it transforms the function into a natively vectorized version
    for better performance. When composed with `jit()`, it can be just as performant
    as manually rewriting your function operate over an extra batch dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™re going to work with a simple example, and promote matrix-vector products
    into matrix-matrix products using `vmap()`. Although this is easy to do by hand
    in this specific case, the same technique can apply to more complicated functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `apply_matrix` function maps a vector to a vector, but we may want to apply
    it row-wise across a matrix. We could do this by looping over the batch dimension
    in Python, but this usually results in poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'A programmer familiar with the the `jnp.dot` function might recognize that
    `apply_matrix` can be rewritten to avoid explicit looping, using the built-in
    batching semantics of `jnp.dot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'However, as functions become more complicated, this kind of manual batching
    becomes more difficult and error-prone. The `vmap()` transformation is designed
    to automatically transform a function into a batch-aware version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you would expect, `vmap()` can be arbitrarily composed with `jit()`, `grad()`,
    and any other JAX transformation.
  prefs: []
  type: TYPE_NORMAL
- en: For more on automatic vectorization in JAX, check out Automatic vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a taste of what JAX can do. Weâ€™re really excited to see what you
    do with it!
  prefs: []
  type: TYPE_NORMAL
