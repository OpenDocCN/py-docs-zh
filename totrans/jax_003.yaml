- en: Quickstart
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå…¥é—¨
- en: åŸæ–‡ï¼š[`jax.readthedocs.io/en/latest/quickstart.html`](https://jax.readthedocs.io/en/latest/quickstart.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[`jax.readthedocs.io/en/latest/quickstart.html`](https://jax.readthedocs.io/en/latest/quickstart.html)
- en: '**JAX a library for array-oriented numerical computation (*Ã  la* [NumPy](https://numpy.org/)),
    with automatic differentiation and JIT compilation to enable high-performance
    machine learning research**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**JAX æ˜¯ä¸€ä¸ªé¢å‘æ•°ç»„çš„æ•°å€¼è®¡ç®—åº“ï¼ˆ*Ã  la* [NumPy](https://numpy.org/)ï¼‰ï¼Œå…·æœ‰è‡ªåŠ¨å¾®åˆ†å’Œ JIT ç¼–è¯‘åŠŸèƒ½ï¼Œä»¥æ”¯æŒé«˜æ€§èƒ½çš„æœºå™¨å­¦ä¹ ç ”ç©¶**ã€‚'
- en: 'This document provides a quick overview of essential JAX features, so you can
    get started with JAX quickly:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ¡£æä¾›äº† JAX ä¸»è¦åŠŸèƒ½çš„å¿«é€Ÿæ¦‚è¿°ï¼Œè®©æ‚¨å¯ä»¥å¿«é€Ÿå¼€å§‹ä½¿ç”¨ JAXï¼š
- en: JAX provides a unified NumPy-like interface to computations that run on CPU,
    GPU, or TPU, in local or distributed settings.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ç±»ä¼¼äº NumPy çš„æ¥å£ï¼Œç”¨äºåœ¨ CPUã€GPU æˆ– TPU ä¸Šè¿è¡Œçš„è®¡ç®—ï¼Œåœ¨æœ¬åœ°æˆ–åˆ†å¸ƒå¼è®¾ç½®ä¸­ã€‚
- en: JAX features built-in Just-In-Time (JIT) compilation via [Open XLA](https://github.com/openxla),
    an open-source machine learning compiler ecosystem.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX é€šè¿‡ [Open XLA](https://github.com/openxla) å†…ç½®äº†å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰åŠŸèƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„æœºå™¨å­¦ä¹ ç¼–è¯‘å™¨ç”Ÿæ€ç³»ç»Ÿã€‚
- en: JAX functions support efficient evaluation of gradients via its automatic differentiation
    transformations.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX å‡½æ•°æ”¯æŒé€šè¿‡å…¶è‡ªåŠ¨å¾®åˆ†è½¬æ¢æœ‰æ•ˆåœ°è¯„ä¼°æ¢¯åº¦ã€‚
- en: JAX functions can be automatically vectorized to efficiently map them over arrays
    representing batches of inputs.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX å‡½æ•°å¯ä»¥è‡ªåŠ¨å‘é‡åŒ–ï¼Œä»¥æœ‰æ•ˆåœ°å°†å®ƒä»¬æ˜ å°„åˆ°è¡¨ç¤ºè¾“å…¥æ‰¹æ¬¡çš„æ•°ç»„ä¸Šã€‚
- en: Installation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®‰è£…
- en: 'JAX can be installed for CPU on Linux, Windows, and macOS directly from the
    [Python Package Index](https://pypi.org/project/jax/):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ç›´æ¥ä» [Python Package Index](https://pypi.org/project/jax/) å®‰è£… JAX ç”¨äº Linuxã€Windows
    å’Œ macOS ä¸Šçš„ CPUï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'or, for NVIDIA GPU:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¯¹äºNVIDIA GPUï¼š
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For more detailed platform-specific installation information, check out Installing
    JAX.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚éœ€æ›´è¯¦ç»†çš„ç‰¹å®šå¹³å°å®‰è£…ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹å®‰è£… JAXã€‚
- en: JAX as NumPy
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JAX å°±åƒ NumPy ä¸€æ ·
- en: 'Most JAX usage is through the familiar `jax.numpy` API, which is typically
    imported under the `jnp` alias:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•° JAX çš„ä½¿ç”¨æ˜¯é€šè¿‡ç†Ÿæ‚‰çš„ `jax.numpy` API è¿›è¡Œçš„ï¼Œé€šå¸¸åœ¨ `jnp` åˆ«åä¸‹å¯¼å…¥ï¼š
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With this import, you can immediately use JAX in a similar manner to typical
    NumPy programs, including using NumPy-style array creation functions, Python functions
    and operators, and array attributes and methods:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ä¸ªå¯¼å…¥ï¼Œæ‚¨å¯ä»¥ç«‹å³åƒä½¿ç”¨å…¸å‹çš„ NumPy ç¨‹åºä¸€æ ·ä½¿ç”¨ JAXï¼ŒåŒ…æ‹¬ä½¿ç”¨ NumPy é£æ ¼çš„æ•°ç»„åˆ›å»ºå‡½æ•°ã€Python å‡½æ•°å’Œæ“ä½œç¬¦ï¼Œä»¥åŠæ•°ç»„å±æ€§å’Œæ–¹æ³•ï¼š
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Youâ€™ll find a few differences between JAX arrays and NumPy arrays once you begin
    digging-in; these are explored in [ğŸ”ª JAX - The Sharp Bits ğŸ”ª](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å¼€å§‹æ·±å…¥ç ”ç©¶ï¼Œæ‚¨ä¼šå‘ç° JAX æ•°ç»„å’Œ NumPy æ•°ç»„ä¹‹é—´å­˜åœ¨ä¸€äº›å·®å¼‚ï¼›è¿™äº›å·®å¼‚åœ¨ [ğŸ”ª JAX - The Sharp Bits ğŸ”ª](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)
    ä¸­è¿›è¡Œäº†æ¢è®¨ã€‚
- en: Just-in-time compilation with `jax.jit()`
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`jax.jit()`è¿›è¡Œå³æ—¶ç¼–è¯‘
- en: JAX runs transparently on the GPU or TPU (falling back to CPU if you donâ€™t have
    one). However, in the above example, JAX is dispatching kernels to the chip one
    operation at a time. If we have a sequence of operations, we can use the `jax.jit()`
    function to compile this sequence of operations together using XLA.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: JAX å¯ä»¥åœ¨ GPU æˆ– TPU ä¸Šé€æ˜è¿è¡Œï¼ˆå¦‚æœæ²¡æœ‰ï¼Œåˆ™é€€å›åˆ° CPUï¼‰ã€‚ç„¶è€Œï¼Œåœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼ŒJAX æ˜¯ä¸€æ¬¡å°†æ ¸å¿ƒåˆ†æ´¾åˆ°èŠ¯ç‰‡ä¸Šçš„æ“ä½œã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸€ç³»åˆ—æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨
    `jax.jit()` å‡½æ•°å°†è¿™äº›æ“ä½œä¸€èµ·ç¼–è¯‘ä¸º XLAã€‚
- en: 'We can use IPythonâ€™s `%timeit` to quickly benchmark our `selu` function, using
    `block_until_ready()` to account for JAXâ€™s dynamic dispatch (See Asynchronous
    dispatch):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ IPython çš„ `%timeit` å¿«é€Ÿæµ‹è¯•æˆ‘ä»¬çš„ `selu` å‡½æ•°ï¼Œä½¿ç”¨ `block_until_ready()` æ¥è€ƒè™‘ JAX
    çš„åŠ¨æ€åˆ†æ´¾ï¼ˆè¯·å‚é˜…å¼‚æ­¥åˆ†æ´¾ï¼‰ï¼š
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: (notice weâ€™ve used `jax.random` to generate some random numbers; for details
    on how to generate random numbers in JAX, check out Pseudorandom numbers).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆè¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨ `jax.random` ç”Ÿæˆäº†ä¸€äº›éšæœºæ•°ï¼›æœ‰å…³å¦‚ä½•åœ¨ JAX ä¸­ç”Ÿæˆéšæœºæ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¼ªéšæœºæ•°ï¼‰ã€‚
- en: We can speed the execution of this function with the `jax.jit()` transformation,
    which will jit-compile the first time `selu` is called and will be cached thereafter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `jax.jit()` è½¬æ¢æ¥åŠ é€Ÿæ­¤å‡½æ•°çš„æ‰§è¡Œï¼Œè¯¥è½¬æ¢å°†åœ¨é¦–æ¬¡è°ƒç”¨ `selu` æ—¶è¿›è¡Œ JIT ç¼–è¯‘ï¼Œå¹¶åœ¨æ­¤åè¿›è¡Œç¼“å­˜ã€‚
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The above timing represent execution on CPU, but the same code can be run on
    GPU or TPU, typically for an even greater speedup.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°æ—¶é—´è¡¨ç¤ºåœ¨ CPU ä¸Šæ‰§è¡Œï¼Œä½†åŒæ ·çš„ä»£ç å¯ä»¥åœ¨ GPU æˆ– TPU ä¸Šè¿è¡Œï¼Œé€šå¸¸ä¼šæœ‰æ›´å¤§çš„åŠ é€Ÿæ•ˆæœã€‚
- en: For more on JIT compilation in JAX, check out Just-in-time compilation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬²äº†è§£æ›´å¤šå…³äº JAX ä¸­ JIT ç¼–è¯‘çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹å³æ—¶ç¼–è¯‘ã€‚
- en: Taking derivatives with `jax.grad()`
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `jax.grad()` è®¡ç®—å¯¼æ•°
- en: 'In addition to transforming functions via JIT compilation, JAX also provides
    other transformations. One such transformation is `jax.grad()`, which performs
    [automatic differentiation (autodiff)](https://en.wikipedia.org/wiki/Automatic_differentiation):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†é€šè¿‡ JIT ç¼–è¯‘è½¬æ¢å‡½æ•°å¤–ï¼ŒJAX è¿˜æä¾›å…¶ä»–è½¬æ¢åŠŸèƒ½ã€‚å…¶ä¸­ä¸€ç§è½¬æ¢æ˜¯ `jax.grad()`ï¼Œå®ƒæ‰§è¡Œ[è‡ªåŠ¨å¾®åˆ† (autodiff)](https://en.wikipedia.org/wiki/Automatic_differentiation)ï¼š
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Letâ€™s verify with finite differences that our result is correct.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨æœ‰é™å·®åˆ†æ¥éªŒè¯æˆ‘ä»¬çš„ç»“æœæ˜¯å¦æ­£ç¡®ã€‚
- en: '[PRE11]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `grad()` and `jit()` transformations compose and can be mixed arbitrarily.
    In the above example we jitted `sum_logistic` and then took its derivative. We
    can go further:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`grad()` å’Œ `jit()` è½¬æ¢å¯ä»¥ä»»æ„ç»„åˆå¹¶æ··åˆä½¿ç”¨ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯¹ `sum_logistic` è¿›è¡Œäº† JIT ç¼–è¯‘ï¼Œç„¶åå–äº†å®ƒçš„å¯¼æ•°ã€‚æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥è¿›è¡Œï¼š'
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Beyond scalar-valued functions, the `jax.jacobian()` transformation can be
    used to compute the full Jacobian matrix for vector-valued functions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ ‡é‡å€¼å‡½æ•°å¤–ï¼Œ`jax.jacobian()` è½¬æ¢è¿˜å¯ç”¨äºè®¡ç®—å‘é‡å€¼å‡½æ•°çš„å®Œæ•´é›…å¯æ¯”çŸ©é˜µï¼š
- en: '[PRE15]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For more advanced autodiff operations, you can use `jax.vjp()` for reverse-mode
    vector-Jacobian products, and `jax.jvp()` and `jax.linearize()` for forward-mode
    Jacobian-vector products. The two can be composed arbitrarily with one another,
    and with other JAX transformations. For example, `jax.jvp()` and `jax.vjp()` are
    used to define the forward-mode `jax.jacfwd()` and reverse-mode `jax.jacrev()`
    for computing Jacobians in forward- and reverse-mode, respectively. Hereâ€™s one
    way to compose them to make a function that efficiently computes full Hessian
    matrices:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´é«˜çº§çš„è‡ªåŠ¨å¾®åˆ†æ“ä½œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `jax.vjp()` æ¥è¿›è¡Œåå‘æ¨¡å¼å‘é‡-é›…å¯æ¯”ç§¯åˆ†ï¼Œä»¥åŠä½¿ç”¨ `jax.jvp()` å’Œ `jax.linearize()`
    è¿›è¡Œæ­£å‘æ¨¡å¼é›…å¯æ¯”-å‘é‡ç§¯åˆ†ã€‚è¿™ä¸¤è€…å¯ä»¥ä»»æ„ç»„åˆï¼Œä¹Ÿå¯ä»¥ä¸å…¶ä»– JAX è½¬æ¢ç»„åˆä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œ`jax.jvp()` å’Œ `jax.vjp()` ç”¨äºå®šä¹‰æ­£å‘æ¨¡å¼
    `jax.jacfwd()` å’Œåå‘æ¨¡å¼ `jax.jacrev()`ï¼Œç”¨äºè®¡ç®—æ­£å‘å’Œåå‘æ¨¡å¼ä¸‹çš„é›…å¯æ¯”çŸ©é˜µã€‚ä»¥ä¸‹æ˜¯ç»„åˆå®ƒä»¬ä»¥æœ‰æ•ˆè®¡ç®—å®Œæ•´ Hessian
    çŸ©é˜µçš„ä¸€ç§æ–¹æ³•ï¼š
- en: '[PRE17]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This kind of composition produces efficient code in practice; this is more-or-less
    how JAXâ€™s built-in `jax.hessian()` function is implemented.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç»„åˆåœ¨å®è·µä¸­äº§ç”Ÿäº†é«˜æ•ˆçš„ä»£ç ï¼›è¿™åŸºæœ¬ä¸Šæ˜¯ JAX å†…ç½®çš„ `jax.hessian()` å‡½æ•°çš„å®ç°æ–¹å¼ã€‚
- en: For more on automatic differentiation in JAX, check out Automatic differentiation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤šå…³äº JAX ä¸­çš„è‡ªåŠ¨å¾®åˆ†ï¼Œè¯·æŸ¥çœ‹è‡ªåŠ¨å¾®åˆ†ã€‚
- en: Auto-vectorization with `jax.vmap()`
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `jax.vmap()` è¿›è¡Œè‡ªåŠ¨å‘é‡åŒ–
- en: Another useful transformation is `vmap()`, the vectorizing map. It has the familiar
    semantics of mapping a function along array axes, but instead of explicitly looping
    over function calls, it transforms the function into a natively vectorized version
    for better performance. When composed with `jit()`, it can be just as performant
    as manually rewriting your function operate over an extra batch dimension.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰ç”¨çš„è½¬æ¢æ˜¯ `vmap()`ï¼Œå³å‘é‡åŒ–æ˜ å°„ã€‚å®ƒå…·æœ‰æ²¿æ•°ç»„è½´æ˜ å°„å‡½æ•°çš„ç†Ÿæ‚‰è¯­ä¹‰ï¼Œä½†ä¸æ˜¾å¼å¾ªç¯å‡½æ•°è°ƒç”¨ä¸åŒï¼Œå®ƒå°†å‡½æ•°è½¬æ¢ä¸ºæœ¬åœ°å‘é‡åŒ–ç‰ˆæœ¬ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ä¸
    `jit()` ç»„åˆæ—¶ï¼Œå®ƒå¯ä»¥ä¸æ‰‹åŠ¨é‡å†™å‡½æ•°ä»¥å¤„ç†é¢å¤–æ‰¹å¤„ç†ç»´åº¦çš„æ€§èƒ½ç›¸åª²ç¾ã€‚
- en: Weâ€™re going to work with a simple example, and promote matrix-vector products
    into matrix-matrix products using `vmap()`. Although this is easy to do by hand
    in this specific case, the same technique can apply to more complicated functions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¤„ç†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå¹¶ä½¿ç”¨ `vmap()` å°†çŸ©é˜µ-å‘é‡ä¹˜æ³•æå‡ä¸ºçŸ©é˜µ-çŸ©é˜µä¹˜æ³•ã€‚è™½ç„¶åœ¨è¿™ç§ç‰¹å®šæƒ…å†µä¸‹æ‰‹åŠ¨å®Œæˆè¿™ä¸€ç‚¹å¾ˆå®¹æ˜“ï¼Œä½†ç›¸åŒçš„æŠ€æœ¯ä¹Ÿé€‚ç”¨äºæ›´å¤æ‚çš„å‡½æ•°ã€‚
- en: '[PRE19]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `apply_matrix` function maps a vector to a vector, but we may want to apply
    it row-wise across a matrix. We could do this by looping over the batch dimension
    in Python, but this usually results in poor performance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_matrix` å‡½æ•°å°†ä¸€ä¸ªå‘é‡æ˜ å°„åˆ°å¦ä¸€ä¸ªå‘é‡ï¼Œä½†æˆ‘ä»¬å¯èƒ½å¸Œæœ›å°†å…¶é€è¡Œåº”ç”¨äºçŸ©é˜µã€‚åœ¨ Python ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¾ªç¯éå†æ‰¹å¤„ç†ç»´åº¦æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½†é€šå¸¸å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚'
- en: '[PRE20]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'A programmer familiar with the the `jnp.dot` function might recognize that
    `apply_matrix` can be rewritten to avoid explicit looping, using the built-in
    batching semantics of `jnp.dot`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç†Ÿæ‚‰ `jnp.dot` å‡½æ•°çš„ç¨‹åºå‘˜å¯èƒ½ä¼šæ„è¯†åˆ°ï¼Œå¯ä»¥é‡å†™ `apply_matrix` æ¥é¿å…æ˜¾å¼å¾ªç¯ï¼Œåˆ©ç”¨ `jnp.dot` çš„å†…ç½®æ‰¹å¤„ç†è¯­ä¹‰ï¼š
- en: '[PRE22]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'However, as functions become more complicated, this kind of manual batching
    becomes more difficult and error-prone. The `vmap()` transformation is designed
    to automatically transform a function into a batch-aware version:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œéšç€å‡½æ•°å˜å¾—æ›´åŠ å¤æ‚ï¼Œè¿™ç§æ‰‹åŠ¨æ‰¹å¤„ç†å˜å¾—æ›´åŠ å›°éš¾ä¸”å®¹æ˜“å‡ºé”™ã€‚`vmap()` è½¬æ¢æ—¨åœ¨è‡ªåŠ¨å°†å‡½æ•°è½¬æ¢ä¸ºæ”¯æŒæ‰¹å¤„ç†çš„ç‰ˆæœ¬ï¼š
- en: '[PRE24]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you would expect, `vmap()` can be arbitrarily composed with `jit()`, `grad()`,
    and any other JAX transformation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€é¢„æœŸçš„é‚£æ ·ï¼Œ`vmap()` å¯ä»¥ä¸ `jit()`ã€`grad()` å’Œä»»ä½•å…¶ä»– JAX è½¬æ¢ä»»æ„ç»„åˆã€‚
- en: For more on automatic vectorization in JAX, check out Automatic vectorization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤šå…³äº JAX ä¸­çš„è‡ªåŠ¨å‘é‡åŒ–ï¼Œè¯·æŸ¥çœ‹è‡ªåŠ¨å‘é‡åŒ–ã€‚
- en: This is just a taste of what JAX can do. Weâ€™re really excited to see what you
    do with it!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ JAX èƒ½åšçš„ä¸€å°éƒ¨åˆ†ã€‚æˆ‘ä»¬éå¸¸æœŸå¾…çœ‹åˆ°ä½ ç”¨å®ƒåšäº›ä»€ä¹ˆï¼
