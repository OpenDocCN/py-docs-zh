["```py\nimport jax\n\njax.distributed.initialize(coordinator_address=\"192.168.0.1:1234\",\n                           num_processes=2,\n                           process_id=0) \n```", "```py\nimport jax\n\njax.distributed.initialize() \n```", "```py\n# The following is run in parallel on each host on a GPU cluster or TPU pod slice.\n>>> import jax\n>>> jax.distributed.initialize()  # On GPU, see above for the necessary arguments.\n>>> jax.device_count()  # total number of accelerator devices in the cluster\n32\n>>> jax.local_device_count()  # number of accelerator devices attached to this host\n8\n# The psum is performed over all mapped devices across the pod slice\n>>> xs = jax.numpy.ones(jax.local_device_count())\n>>> jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)\nShardedDeviceArray([32., 32., 32., 32., 32., 32., 32., 32.], dtype=float32) \n```"]