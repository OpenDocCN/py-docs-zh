["```py\n    import jax\n    jax.config.update('jax_array', True) \n    ```", "```py\n    import jax\n    jax.config.update('jax_array', False) \n    ```", "```py\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nx = jnp.arange(8)\n\n# Let's say there are 8 devices in jax.devices()\nmesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x'))\n\nsharded_x = jax.device_put(x, sharding)\n\n# `matmul_sharded_x` and `sin_sharded_x` are sharded. `jit` is able to operate over a\n# sharded array without copying data to a single device.\nmatmul_sharded_x = sharded_x @ sharded_x.T\nsin_sharded_x = jnp.sin(sharded_x)\n\n# Even jnp.copy preserves the sharding on the output.\ncopy_sharded_x = jnp.copy(sharded_x)\n\n# double_out is also sharded\ndouble_out = jax.jit(lambda x: x * 2)(sharded_x) \n```", "```py\nValueError: One of pjit arguments was given the sharding of\nNamedSharding(mesh={'x': 2, 'y': 2, 'chips': 2}, partition_spec=PartitionSpec(('x', 'y', 'chips'),)),\nwhich implies that the global size of its dimension 0 should be divisible by 8,\nbut it is equal to 4 \n```", "```py\nfrom jax.experimental import multihost_utils\n\nglobal_inps = multihost_utils.host_local_array_to_global_array(\n    local_inputs, mesh, in_pspecs)\n\nglobal_outputs = pjit(f, in_shardings=in_pspecs,\n                      out_shardings=out_pspecs)(global_inps)\n\nlocal_outs = multihost_utils.global_array_to_host_local_array(\n    global_outputs, mesh, out_pspecs) \n```", "```py\nkey = jax.random.PRNGKey(1)\n\n# As you can see, using host_local_array_to_global_array is not required since in_axis_resources says\n# that the input is fully replicated via P(None)\npjit(f, in_shardings=None, out_shardings=None)(key)\n\n# Mixing inputs\nglobal_inp = multihost_utils.host_local_array_to_global_array(\n    local_inp, mesh, P('data'))\nglobal_out = pjit(f, in_shardings=(P(None), P('data')),\n                  out_shardings=...)(key, global_inp) \n```", "```py\npjit(f, in_shardings=FROM_GDA, out_shardings=...) can be replaced by pjit(f, out_shardings=...) \n```", "```py\npjitted_f = pjit(\n    f, in_shardings=(FROM_GDA, P('x'), FROM_GDA, P(None)),\n    out_shardings=...)\npjitted_f(gda1, np_array1, gda2, np_array2) \n```", "```py\n pjitted_f = pjit(f, out_shardings=...)\n\narray2, array3 = multihost_utils.host_local_array_to_global_array(\n    (np_array1, np_array2), mesh, (P('x'), P(None)))\n\npjitted_f(array1, array2, array3, array4) \n```", "```py\nfrom jax.experimental import multihost_utils\n\nbatch = multihost_utils.host_local_array_to_global_array(\n    batch, mesh, batch_partition_spec) \n```"]