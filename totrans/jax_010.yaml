- en: Automatic differentiation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`jax.readthedocs.io/en/latest/automatic-differentiation.html`](https://jax.readthedocs.io/en/latest/automatic-differentiation.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this section, you will learn about fundamental applications of automatic
    differentiation (autodiff) in JAX. JAX has a pretty general autodiff system. Computing
    gradients is a critical part of modern machine learning methods, and this tutorial
    will walk you through a few introductory autodiff topics, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Taking gradients with jax.grad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Computing gradients in a linear logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Differentiating with respect to nested lists, tuples, and dicts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Evaluating a function and its gradient using jax.value_and_grad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. Checking against numerical differences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure to also check out the Advanced automatic differentiation tutorial
    for more advanced topics.
  prefs: []
  type: TYPE_NORMAL
- en: While understanding how automatic differentiation works “under the hood” isn’t
    crucial for using JAX in most contexts, you are encouraged to check out this quite
    accessible [video](https://www.youtube.com/watch?v=wG_nF1awSSY) to get a deeper
    sense of what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: '## 1\. Taking gradients with `jax.grad`'
  prefs: []
  type: TYPE_NORMAL
- en: 'In JAX, you can differentiate a scalar-valued function with the `jax.grad()`
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`jax.grad()` takes a function and returns a function. If you have a Python
    function `f` that evaluates the mathematical function \(f\), then `jax.grad(f)`
    is a Python function that evaluates the mathematical function \(\nabla f\). That
    means `grad(f)(x)` represents the value \(\nabla f(x)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `jax.grad()` operates on functions, you can apply it to its own output
    to differentiate as many times as you like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'JAX’s autodiff makes it easy to compute higher-order derivatives, because the
    functions that compute derivatives are themselves differentiable. Thus, higher-order
    derivatives are as easy as stacking transformations. This can be illustrated in
    the single-variable case:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of \(f(x) = x³ + 2x² - 3x + 1\) can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The higher-order derivatives of \(f\) are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{array}{l} f'(x) = 3x² + 4x -3\\ f''(x) = 6x + 4\\ f'''(x)
    = 6\\ f^{iv}(x) = 0 \end{array} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing any of these in JAX is as easy as chaining the `jax.grad()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluating the above in \(x=1\) would give you:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{array}{l} f'(1) = 4\\ f''(1) = 10\\ f'''(1) = 6\\ f^{iv}(1)
    = 0 \end{array} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Using JAX:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]  ## 2\. Computing gradients in a linear logistic regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next example shows how to compute gradients with `jax.grad()` in a linear
    logistic regression model. First, the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Use the `jax.grad()` function with its `argnums` argument to differentiate a
    function with respect to positional arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `jax.grad()` API has a direct correspondence to the excellent notation in
    Spivak’s classic *Calculus on Manifolds* (1965), also used in Sussman and Wisdom’s
    [*Structure and Interpretation of Classical Mechanics*](https://mitpress.mit.edu/9780262028967/structure-and-interpretation-of-classical-mechanics)
    (2015) and their [*Functional Differential Geometry*](https://mitpress.mit.edu/9780262019347/functional-differential-geometry)
    (2013). Both books are open-access. See in particular the “Prologue” section of
    *Functional Differential Geometry* for a defense of this notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, when using the `argnums` argument, if `f` is a Python function
    for evaluating the mathematical function \(f\), then the Python expression `jax.grad(f,
    i)` evaluates to a Python function for evaluating \(\partial_i f\).  ## 3\. Differentiating
    with respect to nested lists, tuples, and dicts'
  prefs: []
  type: TYPE_NORMAL
- en: Due to JAX’s PyTree abstraction (see Working with pytrees), differentiating
    with respect to standard Python containers just works, so use tuples, lists, and
    dicts (and arbitrary nesting) however you like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can create Custom pytree nodes to work with not just `jax.grad()` but other
    JAX transformations (`jax.jit()`, `jax.vmap()`, and so on).  ## 4\. Evaluating
    a function and its gradient using `jax.value_and_grad`'
  prefs: []
  type: TYPE_NORMAL
- en: Another convenient function is `jax.value_and_grad()` for efficiently computing
    both a function’s value as well as its gradient’s value in one pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]  ## 5\. Checking against numerical differences'
  prefs: []
  type: TYPE_NORMAL
- en: A great thing about derivatives is that they’re straightforward to check with
    finite differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'JAX provides a simple convenience function that does essentially the same thing,
    but checks up to any order of differentiation that you like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Advanced automatic differentiation tutorial provides more advanced and detailed
    explanations of how the ideas covered in this document are implemented in the
    JAX backend. Some features, such as Custom derivative rules for JAX-transformable
    Python functions, depend on understanding advanced automatic differentiation,
    so do check out that section in the Advanced automatic differentiation tutorial
    if you are interested.
  prefs: []
  type: TYPE_NORMAL
