["```py\n#@title Imports\n\nimport jax\nfrom jax.experimental import pallas as pl\nimport jax.numpy as jnp\nimport numpy as np \n```", "```py\ndef add_matrices_kernel(x_vmem_ref, y_vmem_ref, z_vmem_ref):\n  # Load x and y from VMEM into VREGs\n  x_vregs = x_vmem_ref[:, :]\n  y_vregs = y_vmem_ref[:, :]\n  # Execute a vectorized add\n  z_vregs = x_vregs + y_vregs\n  # Store the output values in VREGs back into VMEM\n  z_vmem_ref[:, :] = z_vregs\n\ndef add_matrices(x: jax.Array, y: jax.Array) -> jax.Array:\n  # pallas_call will first allocate scratch buffers for `x` and `y` in VMEM.\n  # It will then copy `x` and `y` from HBM into VMEM.\n  z = pl.pallas_call(\n      add_matrices_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n  )(x, y)\n  # pallas_call will also copy the output from VMEM back into HBM.\n  return z\n\nx, y = jnp.ones((512, 512)), jnp.ones((512, 512))\nadd_matrices(x, y) \n```", "```py\nArray([[2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       ...,\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.]], dtype=float32) \n```", "```py\npl.pallas_call(some_kernel, grid=n)(...) \n```", "```py\nfor i in range(n):\n  # do HBM -> VMEM copies\n  some_kernel(...)\n  # do VMEM -> HBM copies \n```", "```py\npl.pallas_call(some_kernel, grid=(n, m))(...) \n```", "```py\nfor i in range(n):\n  for j in range(m):\n    # do HBM -> VMEM copies\n    some_kernel(...)\n    # do VMEM -> HBM copies \n```", "```py\ndef x_index_map(i):\n  return (i, 0) \n```", "```py\nblock_spec = pl.BlockSpec(x_index_map, (256, 512)) \n```", "```py\ndef add_matrices_pipelined(x: jax.Array, y: jax.Array) -> jax.Array:\n  block_spec = pl.BlockSpec(lambda i: (i, 0), (256, 512))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(2,))(x, y)\n\nadd_matrices_pipelined(x, y) \n```", "```py\nArray([[2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       ...,\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.]], dtype=float32) \n```", "```py\ndef add_matrices_pipelined_2d(\n    x: jax.Array, y: jax.Array, *, bm: int = 256, bn: int = 256\n) -> jax.Array:\n  m, n = x.shape\n  block_spec = pl.BlockSpec(lambda i, j: (i, j), (bm, bn))\n\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(m // bm, n // bn),\n  )(x, y)\n\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=256, bn=256), x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=128, bn=128), x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=512, bn=512), x + y\n) \n```", "```py\nx = jnp.ones((8, 512, 512))\njnp.sum(x, axis=0) \n```", "```py\nArray([[8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.],\n       ...,\n       [8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.]], dtype=float32) \n```", "```py\n# Warning: this implementation is incorrect!\n\ndef naive_sum_kernel(x_ref, o_ref):\n  o_ref[...] += x_ref[...]\n\ndef naive_sum(x: jax.Array) -> jax.Array:\n  grid, *out_shape = x.shape\n  return pl.pallas_call(\n      naive_sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[pl.BlockSpec(lambda i: (i, 0, 0), (None, *out_shape))],\n      out_specs=pl.BlockSpec(lambda i: (0, 0), out_shape),\n      out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype)\n      )(x)\nnaive_sum(x) \n```", "```py\nArray([[9., 9., 9., ..., 9., 9., 9.],\n       [9., 9., 9., ..., 9., 9., 9.],\n       [9., 9., 9., ..., 9., 9., 9.],\n       ...,\n       [9., 9., 9., ..., 9., 9., 9.],\n       [9., 9., 9., ..., 9., 9., 9.],\n       [9., 9., 9., ..., 9., 9., 9.]], dtype=float32) \n```", "```py\ndef sum_kernel(x_ref, o_ref):\n  @pl.when(pl.program_id(axis=0) == 0)\n  def _():\n    o_ref[...] = jnp.zeros_like(o_ref)\n\n  o_ref[...] += x_ref[...]\n\ndef sum(x: jax.Array) -> jax.Array:\n  grid, *out_shape = x.shape\n  return pl.pallas_call(\n      sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[pl.BlockSpec(lambda i: (i, 0, 0), (None, *out_shape))],\n      out_specs=pl.BlockSpec(lambda i: (0, 0), out_shape),\n      out_shape=jax.ShapeDtypeStruct(out_shape, x.dtype)\n      )(x)\nsum(x) \n```", "```py\nArray([[8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.],\n       ...,\n       [8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.],\n       [8., 8., 8., ..., 8., 8., 8.]], dtype=float32) \n```", "```py\ndef add_matrices_pipelined_megacore(x: jax.Array, y: jax.Array) -> jax.Array:\n  block_spec = pl.BlockSpec(lambda i: (i, 0), (256, 512))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(2,),\n      compiler_params=dict(mosaic=dict(dimension_semantics=(\"parallel\",))))(\n        x, y)\n\nx, y = jnp.ones((512, 512)), jnp.ones((512, 512))\nadd_matrices_pipelined_megacore(x, y) \n```", "```py\nArray([[2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       ...,\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.],\n       [2., 2., 2., ..., 2., 2., 2.]], dtype=float32) \n```"]