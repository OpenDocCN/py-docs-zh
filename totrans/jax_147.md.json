["```py\n>>> from jax.experimental import sparse\n>>> import jax.numpy as jnp\n>>> import numpy as np \n```", "```py\n>>> M = jnp.array([[0., 1., 0., 2.],\n...                [3., 0., 0., 0.],\n...                [0., 0., 4., 0.]]) \n```", "```py\n>>> M_sp = sparse.BCOO.fromdense(M) \n```", "```py\n>>> M_sp\nBCOO(float32[3, 4], nse=4) \n```", "```py\n>>> M_sp.todense()\nArray([[0., 1., 0., 2.],\n [3., 0., 0., 0.],\n [0., 0., 4., 0.]], dtype=float32) \n```", "```py\n>>> M_sp.data  # Explicitly stored data\nArray([1., 2., 3., 4.], dtype=float32) \n```", "```py\n>>> M_sp.indices # Indices of the stored data\nArray([[0, 1],\n [0, 3],\n [1, 0],\n [2, 2]], dtype=int32) \n```", "```py\n>>> M_sp.ndim\n2 \n```", "```py\n>>> M_sp.shape\n(3, 4) \n```", "```py\n>>> M_sp.dtype\ndtype('float32') \n```", "```py\n>>> M_sp.nse  # \"number of specified elements\"\n4 \n```", "```py\n>>> y = jnp.array([3., 6., 5.]) \n```", "```py\n>>> M_sp.T @ y\nArray([18.,  3., 20.,  6.], dtype=float32) \n```", "```py\n>>> M.T @ y  # Compare to dense version\nArray([18.,  3., 20.,  6.], dtype=float32) \n```", "```py\n>>> from jax import grad, jit \n```", "```py\n>>> def f(y):\n...   return (M_sp.T @ y).sum()\n...\n>>> jit(grad(f))(y)\nArray([3., 3., 4.], dtype=float32) \n```", "```py\n>>> def f(M, v):\n...   return 2 * jnp.dot(jnp.log1p(M.T), v) + 1\n...\n>>> f(M, y)\nArray([17.635532,  5.158883, 17.09438 ,  7.591674], dtype=float32) \n```", "```py\n>>> f_sp = sparse.sparsify(f) \n```", "```py\n>>> f_sp(M_sp, y)\nArray([17.635532,  5.158883, 17.09438 ,  7.591674], dtype=float32) \n```", "```py\n>>> import functools\n>>> from sklearn.datasets import make_classification\n>>> from jax.scipy import optimize \n```", "```py\n>>> def sigmoid(x):\n...   return 0.5 * (jnp.tanh(x / 2) + 1)\n...\n>>> def y_model(params, X):\n...   return sigmoid(jnp.dot(X, params[1:]) + params[0])\n...\n>>> def loss(params, X, y):\n...   y_hat = y_model(params, X)\n...   return -jnp.mean(y * jnp.log(y_hat) + (1 - y) * jnp.log(1 - y_hat))\n...\n>>> def fit_logreg(X, y):\n...   params = jnp.zeros(X.shape[1] + 1)\n...   result = optimize.minimize(functools.partial(loss, X=X, y=y),\n...                              x0=params, method='BFGS')\n...   return result.x \n```", "```py\n>>> X, y = make_classification(n_classes=2, random_state=1701)\n>>> params_dense = fit_logreg(X, y)\n>>> print(params_dense)  \n[-0.7298445   0.29893667  1.0248291  -0.44436368  0.8785025  -0.7724008\n -0.62893456  0.2934014   0.82974285  0.16838408 -0.39774987 -0.5071844\n 0.2028872   0.5227761  -0.3739224  -0.7104083   2.4212713   0.6310087\n -0.67060554  0.03139788 -0.05359547] \n```", "```py\n>>> Xsp = sparse.BCOO.fromdense(X)  # Sparse version of the input\n>>> fit_logreg_sp = sparse.sparsify(fit_logreg)  # Sparse-transformed fit function\n>>> params_sparse = fit_logreg_sp(Xsp, y)\n>>> print(params_sparse)  \n[-0.72971725  0.29878938  1.0246326  -0.44430563  0.8784217  -0.77225566\n -0.6288222   0.29335397  0.8293481   0.16820715 -0.39764675 -0.5069753\n 0.202579    0.522672   -0.3740134  -0.7102678   2.4209507   0.6310593\n -0.670236    0.03132951 -0.05356663] \n```"]