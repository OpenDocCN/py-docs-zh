- en: The Autodiff Cookbook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html`](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Open in Colab](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)
    ![Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)'
  prefs: []
  type: TYPE_IMG
- en: '*alexbw@, mattjj@*'
  prefs: []
  type: TYPE_NORMAL
- en: JAX has a pretty general automatic differentiation system. In this notebook,
    we’ll go through a whole bunch of neat autodiff ideas that you can cherry pick
    for your own work, starting with the basics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting with `grad`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can differentiate a function with `grad`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`grad` takes a function and returns a function. If you have a Python function
    `f` that evaluates the mathematical function \(f\), then `grad(f)` is a Python
    function that evaluates the mathematical function \(\nabla f\). That means `grad(f)(x)`
    represents the value \(\nabla f(x)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `grad` operates on functions, you can apply it to its own output to differentiate
    as many times as you like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at computing gradients with `grad` in a linear logistic regression
    model. First, the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Use the `grad` function with its `argnums` argument to differentiate a function
    with respect to positional arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This `grad` API has a direct correspondence to the excellent notation in Spivak’s
    classic *Calculus on Manifolds* (1965), also used in Sussman and Wisdom’s [*Structure
    and Interpretation of Classical Mechanics*](https://mitpress.mit.edu/9780262028967/structure-and-interpretation-of-classical-mechanics)
    (2015) and their [*Functional Differential Geometry*](https://mitpress.mit.edu/9780262019347/functional-differential-geometry)
    (2013). Both books are open-access. See in particular the “Prologue” section of
    *Functional Differential Geometry* for a defense of this notation.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, when using the `argnums` argument, if `f` is a Python function
    for evaluating the mathematical function \(f\), then the Python expression `grad(f,
    i)` evaluates to a Python function for evaluating \(\partial_i f\).
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating with respect to nested lists, tuples, and dicts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differentiating with respect to standard Python containers just works, so use
    tuples, lists, and dicts (and arbitrary nesting) however you like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can [register your own container types](https://github.com/google/jax/issues/446#issuecomment-467105048)
    to work with not just `grad` but all the JAX transformations (`jit`, `vmap`, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate a function and its gradient using `value_and_grad`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another convenient function is `value_and_grad` for efficiently computing both
    a function’s value as well as its gradient’s value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Checking against numerical differences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A great thing about derivatives is that they’re straightforward to check with
    finite differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'JAX provides a simple convenience function that does essentially the same thing,
    but checks up to any order of differentiation that you like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Hessian-vector products with `grad`-of-`grad`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One thing we can do with higher-order `grad` is build a Hessian-vector product
    function. (Later on we’ll write an even more efficient implementation that mixes
    both forward- and reverse-mode, but this one will use pure reverse-mode.)
  prefs: []
  type: TYPE_NORMAL
- en: A Hessian-vector product function can be useful in a [truncated Newton Conjugate-Gradient
    algorithm](https://en.wikipedia.org/wiki/Truncated_Newton_method) for minimizing
    smooth convex functions, or for studying the curvature of neural network training
    objectives (e.g. [1](https://arxiv.org/abs/1406.2572), [2](https://arxiv.org/abs/1811.07062),
    [3](https://arxiv.org/abs/1706.04454), [4](https://arxiv.org/abs/1802.03451)).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a scalar-valued function \(f : \mathbb{R}^n \to \mathbb{R}\) with continuous
    second derivatives (so that the Hessian matrix is symmetric), the Hessian at a
    point \(x \in \mathbb{R}^n\) is written as \(\partial² f(x)\). A Hessian-vector
    product function is then able to evaluate'
  prefs: []
  type: TYPE_NORMAL
- en: \(\qquad v \mapsto \partial² f(x) \cdot v\)
  prefs: []
  type: TYPE_NORMAL
- en: for any \(v \in \mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The trick is not to instantiate the full Hessian matrix: if \(n\) is large,
    perhaps in the millions or billions in the context of neural networks, then that
    might be impossible to store.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, `grad` already gives us a way to write an efficient Hessian-vector
    product function. We just have to use the identity
  prefs: []
  type: TYPE_NORMAL
- en: \(\qquad \partial² f (x) v = \partial [x \mapsto \partial f(x) \cdot v] = \partial
    g(x)\),
  prefs: []
  type: TYPE_NORMAL
- en: where \(g(x) = \partial f(x) \cdot v\) is a new scalar-valued function that
    dots the gradient of \(f\) at \(x\) with the vector \(v\). Notice that we’re only
    ever differentiating scalar-valued functions of vector-valued arguments, which
    is exactly where we know `grad` is efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In JAX code, we can just write this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This example shows that you can freely use lexical closure, and JAX will never
    get perturbed or confused.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll check this implementation a few cells down, once we see how to compute
    dense Hessian matrices. We’ll also write an even better version that uses both
    forward-mode and reverse-mode.
  prefs: []
  type: TYPE_NORMAL
- en: Jacobians and Hessians using `jacfwd` and `jacrev`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can compute full Jacobian matrices using the `jacfwd` and `jacrev` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'These two functions compute the same values (up to machine numerics), but differ
    in their implementation: `jacfwd` uses forward-mode automatic differentiation,
    which is more efficient for “tall” Jacobian matrices, while `jacrev` uses reverse-mode,
    which is more efficient for “wide” Jacobian matrices. For matrices that are near-square,
    `jacfwd` probably has an edge over `jacrev`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use `jacfwd` and `jacrev` with container types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For more details on forward- and reverse-mode, as well as how to implement `jacfwd`
    and `jacrev` as efficiently as possible, read on!
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a composition of two of these functions gives us a way to compute dense
    Hessian matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This shape makes sense: if we start with a function \(f : \mathbb{R}^n \to
    \mathbb{R}^m\), then at a point \(x \in \mathbb{R}^n\) we expect to get the shapes'
  prefs: []
  type: TYPE_NORMAL
- en: \(f(x) \in \mathbb{R}^m\), the value of \(f\) at \(x\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\partial f(x) \in \mathbb{R}^{m \times n}\), the Jacobian matrix at \(x\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\partial² f(x) \in \mathbb{R}^{m \times n \times n}\), the Hessian at \(x\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement `hessian`, we could have used `jacfwd(jacrev(f))` or `jacrev(jacfwd(f))`
    or any other composition of the two. But forward-over-reverse is typically the
    most efficient. That’s because in the inner Jacobian computation we’re often differentiating
    a function wide Jacobian (maybe like a loss function \(f : \mathbb{R}^n \to \mathbb{R}\)),
    while in the outer Jacobian computation we’re differentiating a function with
    a square Jacobian (since \(\nabla f : \mathbb{R}^n \to \mathbb{R}^n\)), which
    is where forward-mode wins out.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How it’s made: two foundational autodiff functions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### Jacobian-Vector products (JVPs, aka forward-mode autodiff)'
  prefs: []
  type: TYPE_NORMAL
- en: JAX includes efficient and general implementations of both forward- and reverse-mode
    automatic differentiation. The familiar `grad` function is built on reverse-mode,
    but to explain the difference in the two modes, and when each can be useful, we
    need a bit of math background.
  prefs: []
  type: TYPE_NORMAL
- en: JVPs in math
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Mathematically, given a function \(f : \mathbb{R}^n \to \mathbb{R}^m\), the
    Jacobian of \(f\) evaluated at an input point \(x \in \mathbb{R}^n\), denoted
    \(\partial f(x)\), is often thought of as a matrix in \(\mathbb{R}^m \times \mathbb{R}^n\):'
  prefs: []
  type: TYPE_NORMAL
- en: \(\qquad \partial f(x) \in \mathbb{R}^{m \times n}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'But we can also think of \(\partial f(x)\) as a linear map, which maps the
    tangent space of the domain of \(f\) at the point \(x\) (which is just another
    copy of \(\mathbb{R}^n\)) to the tangent space of the codomain of \(f\) at the
    point \(f(x)\) (a copy of \(\mathbb{R}^m\)):'
  prefs: []
  type: TYPE_NORMAL
- en: '\(\qquad \partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\).'
  prefs: []
  type: TYPE_NORMAL
- en: This map is called the [pushforward map](https://en.wikipedia.org/wiki/Pushforward_(differential))
    of \(f\) at \(x\). The Jacobian matrix is just the matrix for this linear map
    in a standard basis.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we don’t commit to one specific input point \(x\), then we can think of
    the function \(\partial f\) as first taking an input point and returning the Jacobian
    linear map at that input point:'
  prefs: []
  type: TYPE_NORMAL
- en: '\(\qquad \partial f : \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m\).'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we can uncurry things so that given input point \(x \in \mathbb{R}^n\)
    and a tangent vector \(v \in \mathbb{R}^n\), we get back an output tangent vector
    in \(\mathbb{R}^m\). We call that mapping, from \((x, v)\) pairs to output tangent
    vectors, the *Jacobian-vector product*, and write it as
  prefs: []
  type: TYPE_NORMAL
- en: \(\qquad (x, v) \mapsto \partial f(x) v\)
  prefs: []
  type: TYPE_NORMAL
- en: JVPs in JAX code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Back in Python code, JAX’s `jvp` function models this transformation. Given
    a Python function that evaluates \(f\), JAX’s `jvp` is a way to get a Python function
    for evaluating \((x, v) \mapsto (f(x), \partial f(x) v)\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature),
    we could write
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: where we use `T a` to denote the type of the tangent space for `a`. In words,
    `jvp` takes as arguments a function of type `a -> b`, a value of type `a`, and
    a tangent vector value of type `T a`. It gives back a pair consisting of a value
    of type `b` and an output tangent vector of type `T b`.
  prefs: []
  type: TYPE_NORMAL
- en: The `jvp`-transformed function is evaluated much like the original function,
    but paired up with each primal value of type `a` it pushes along tangent values
    of type `T a`. For each primitive numerical operation that the original function
    would have applied, the `jvp`-transformed function executes a “JVP rule” for that
    primitive that both evaluates the primitive on the primals and applies the primitive’s
    JVP at those primal values.
  prefs: []
  type: TYPE_NORMAL
- en: 'That evaluation strategy has some immediate implications about computational
    complexity: since we evaluate JVPs as we go, we don’t need to store anything for
    later, and so the memory cost is independent of the depth of the computation.
    In addition, the FLOP cost of the `jvp`-transformed function is about 3x the cost
    of just evaluating the function (one unit of work for evaluating the original
    function, for example `sin(x)`; one unit for linearizing, like `cos(x)`; and one
    unit for applying the linearized function to a vector, like `cos_x * v`). Put
    another way, for a fixed primal point \(x\), we can evaluate \(v \mapsto \partial
    f(x) \cdot v\) for about the same marginal cost as evaluating \(f\).'
  prefs: []
  type: TYPE_NORMAL
- en: That memory complexity sounds pretty compelling! So why don’t we see forward-mode
    very often in machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: To answer that, first think about how you could use a JVP to build a full Jacobian
    matrix. If we apply a JVP to a one-hot tangent vector, it reveals one column of
    the Jacobian matrix, corresponding to the nonzero entry we fed in. So we can build
    a full Jacobian one column at a time, and to get each column costs about the same
    as one function evaluation. That will be efficient for functions with “tall” Jacobians,
    but inefficient for “wide” Jacobians.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re doing gradient-based optimization in machine learning, you probably
    want to minimize a loss function from parameters in \(\mathbb{R}^n\) to a scalar
    loss value in \(\mathbb{R}\). That means the Jacobian of this function is a very
    wide matrix: \(\partial f(x) \in \mathbb{R}^{1 \times n}\), which we often identify
    with the Gradient vector \(\nabla f(x) \in \mathbb{R}^n\). Building that matrix
    one column at a time, with each call taking a similar number of FLOPs to evaluate
    the original function, sure seems inefficient! In particular, for training neural
    networks, where \(f\) is a training loss function and \(n\) can be in the millions
    or billions, this approach just won’t scale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do better for functions like this, we just need to use reverse-mode.  ###
    Vector-Jacobian products (VJPs, aka reverse-mode autodiff)'
  prefs: []
  type: TYPE_NORMAL
- en: Where forward-mode gives us back a function for evaluating Jacobian-vector products,
    which we can then use to build Jacobian matrices one column at a time, reverse-mode
    is a way to get back a function for evaluating vector-Jacobian products (equivalently
    Jacobian-transpose-vector products), which we can use to build Jacobian matrices
    one row at a time.
  prefs: []
  type: TYPE_NORMAL
- en: VJPs in math
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s again consider a function \(f : \mathbb{R}^n \to \mathbb{R}^m\). Starting
    from our notation for JVPs, the notation for VJPs is pretty simple:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\qquad (x, v) \mapsto v \partial f(x)\),
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(v\) is an element of the cotangent space of \(f\) at \(x\) (isomorphic
    to another copy of \(\mathbb{R}^m\)). When being rigorous, we should think of
    \(v\) as a linear map \(v : \mathbb{R}^m \to \mathbb{R}\), and when we write \(v
    \partial f(x)\) we mean function composition \(v \circ \partial f(x)\), where
    the types work out because \(\partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\).
    But in the common case we can identify \(v\) with a vector in \(\mathbb{R}^m\)
    and use the two almost interchangeably, just like we might sometimes flip between
    “column vectors” and “row vectors” without much comment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that identification, we can alternatively think of the linear part of
    a VJP as the transpose (or adjoint conjugate) of the linear part of a JVP:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\qquad (x, v) \mapsto \partial f(x)^\mathsf{T} v\).
  prefs: []
  type: TYPE_NORMAL
- en: For a given point \(x\), we can write the signature as
  prefs: []
  type: TYPE_NORMAL
- en: '\(\qquad \partial f(x)^\mathsf{T} : \mathbb{R}^m \to \mathbb{R}^n\).'
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding map on cotangent spaces is often called the [pullback](https://en.wikipedia.org/wiki/Pullback_(differential_geometry))
    of \(f\) at \(x\). The key for our purposes is that it goes from something that
    looks like the output of \(f\) to something that looks like the input of \(f\),
    just like we might expect from a transposed linear function.
  prefs: []
  type: TYPE_NORMAL
- en: VJPs in JAX code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Switching from math back to Python, the JAX function `vjp` can take a Python
    function for evaluating \(f\) and give us back a Python function for evaluating
    the VJP \((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In terms of [Haskell-like type signatures](https://wiki.haskell.org/Type_signature),
    we could write
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: where we use `CT a` to denote the type for the cotangent space for `a`. In words,
    `vjp` takes as arguments a function of type `a -> b` and a point of type `a`,
    and gives back a pair consisting of a value of type `b` and a linear map of type
    `CT b -> CT a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is great because it lets us build Jacobian matrices one row at a time,
    and the FLOP cost for evaluating \((x, v) \mapsto (f(x), v^\mathsf{T} \partial
    f(x))\) is only about three times the cost of evaluating \(f\). In particular,
    if we want the gradient of a function \(f : \mathbb{R}^n \to \mathbb{R}\), we
    can do it in just one call. That’s how `grad` is efficient for gradient-based
    optimization, even for objectives like neural network training loss functions
    on millions or billions of parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a cost, though: though the FLOPs are friendly, memory scales with the
    depth of the computation. Also, the implementation is traditionally more complex
    than that of forward-mode, though JAX has some tricks up its sleeve (that’s a
    story for a future notebook!).'
  prefs: []
  type: TYPE_NORMAL
- en: For more on how reverse-mode works, see [this tutorial video from the Deep Learning
    Summer School in 2017](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/).
  prefs: []
  type: TYPE_NORMAL
- en: Vector-valued gradients with VJPs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you’re interested in taking vector-valued gradients (like `tf.gradients`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Hessian-vector products using both forward- and reverse-mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a previous section, we implemented a Hessian-vector product function just
    using reverse-mode (assuming continuous second derivatives):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: That’s efficient, but we can do even better and save some memory by using forward-mode
    together with reverse-mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, given a function \(f : \mathbb{R}^n \to \mathbb{R}\) to differentiate,
    a point \(x \in \mathbb{R}^n\) at which to linearize the function, and a vector
    \(v \in \mathbb{R}^n\), the Hessian-vector product function we want is'
  prefs: []
  type: TYPE_NORMAL
- en: \((x, v) \mapsto \partial² f(x) v\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the helper function \(g : \mathbb{R}^n \to \mathbb{R}^n\) defined
    to be the derivative (or gradient) of \(f\), namely \(g(x) = \partial f(x)\).
    All we need is its JVP, since that will give us'
  prefs: []
  type: TYPE_NORMAL
- en: \((x, v) \mapsto \partial g(x) v = \partial² f(x) v\).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can translate that almost directly into code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Even better, since we didn’t have to call `jnp.dot` directly, this `hvp` function
    works with arrays of any shape and with arbitrary container types (like vectors
    stored as nested lists/dicts/tuples), and doesn’t even have a dependence on `jax.numpy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way you might consider writing this is using reverse-over-forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s not quite as good, though, because forward-mode has less overhead than
    reverse-mode, and since the outer differentiation operator here has to differentiate
    a larger computation than the inner one, keeping forward-mode on the outside works
    best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Composing VJPs, JVPs, and `vmap`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jacobian-Matrix and Matrix-Jacobian products
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have `jvp` and `vjp` transformations that give us functions to push-forward
    or pull-back single vectors at a time, we can use JAX’s `vmap` [transformation](https://github.com/google/jax#auto-vectorization-with-vmap)
    to push and pull entire bases at once. In particular, we can use that to write
    fast matrix-Jacobian and Jacobian-matrix products.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The implementation of `jacfwd` and `jacrev`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve seen fast Jacobian-matrix and matrix-Jacobian products, it’s
    not hard to guess how to write `jacfwd` and `jacrev`. We just use the same technique
    to push-forward or pull-back an entire standard basis (isomorphic to an identity
    matrix) at once.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, [Autograd](https://github.com/hips/autograd) couldn’t do this.
    Our [implementation](https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/differential_operators.py#L60)
    of reverse-mode `jacobian` in Autograd had to pull back one vector at a time with
    an outer-loop `map`. Pushing one vector at a time through the computation is much
    less efficient than batching it all together with `vmap`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing that Autograd couldn’t do is `jit`. Interestingly, no matter
    how much Python dynamism you use in your function to be differentiated, we could
    always use `jit` on the linear part of the computation. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Complex numbers and differentiation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX is great at complex numbers and differentiation. To support both [holomorphic
    and non-holomorphic differentiation](https://en.wikipedia.org/wiki/Holomorphic_function),
    it helps to think in terms of JVPs and VJPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a complex-to-complex function \(f: \mathbb{C} \to \mathbb{C}\) and
    identify it with a corresponding function \(g: \mathbb{R}² \to \mathbb{R}²\),'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: That is, we’ve decomposed \(f(z) = u(x, y) + v(x, y) i\) where \(z = x + y i\),
    and identified \(\mathbb{C}\) with \(\mathbb{R}²\) to get \(g\).
  prefs: []
  type: TYPE_NORMAL
- en: Since \(g\) only involves real inputs and outputs, we already know how to write
    a Jacobian-vector product for it, say given a tangent vector \((c, d) \in \mathbb{R}²\),
    namely
  prefs: []
  type: TYPE_NORMAL
- en: \(\begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0 v(x,
    y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: To get a JVP for the original function \(f\) applied to a tangent vector \(c
    + di \in \mathbb{C}\), we just use the same definition and identify the result
    as another complex number,
  prefs: []
  type: TYPE_NORMAL
- en: \(\partial f(x + y i)(c + d i) = \begin{matrix} \begin{bmatrix} 1 & i \end{bmatrix}
    \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0
    v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s our definition of the JVP of a \(\mathbb{C} \to \mathbb{C}\) function!
    Notice it doesn’t matter whether or not \(f\) is holomorphic: the JVP is unambiguous.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'What about VJPs? We do something pretty similar: for a cotangent vector \(c
    + di \in \mathbb{C}\) we define the VJP of \(f\) as'
  prefs: []
  type: TYPE_NORMAL
- en: \((c + di)^* \; \partial f(x + y i) = \begin{matrix} \begin{bmatrix} c & -d
    \end{bmatrix} \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1
    u(x, y) \\ \partial_0 v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix}
    1 \\ -i \end{bmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: What’s with the negatives? They’re just to take care of complex conjugation,
    and the fact that we’re working with covectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a check of the VJP rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: What about convenience wrappers like `grad`, `jacfwd`, and `jacrev`?
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(\mathbb{R} \to \mathbb{R}\) functions, recall we defined `grad(f)(x)`
    as being `vjp(f, x)1`, which works because applying a VJP to a `1.0` value reveals
    the gradient (i.e. Jacobian, or derivative). We can do the same thing for \(\mathbb{C}
    \to \mathbb{R}\) functions: we can still use `1.0` as the cotangent vector, and
    we just get out a complex number result summarizing the full Jacobian:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: For general \(\mathbb{C} \to \mathbb{C}\) functions, the Jacobian has 4 real-valued
    degrees of freedom (as in the 2x2 Jacobian matrices above), so we can’t hope to
    represent all of them within a complex number. But we can for holomorphic functions!
    A holomorphic function is precisely a \(\mathbb{C} \to \mathbb{C}\) function with
    the special property that its derivative can be represented as a single complex
    number. (The [Cauchy-Riemann equations](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations)
    ensure that the above 2x2 Jacobians have the special form of a scale-and-rotate
    matrix in the complex plane, i.e. the action of a single complex number under
    multiplication.) And we can reveal that one complex number using a single call
    to `vjp` with a covector of `1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because this only works for holomorphic functions, to use this trick we need
    to promise JAX that our function is holomorphic; otherwise, JAX will raise an
    error when `grad` is used for a complex-output function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'All the `holomorphic=True` promise does is disable the error when the output
    is complex-valued. We can still write `holomorphic=True` when the function isn’t
    holomorphic, but the answer we get out won’t represent the full Jacobian. Instead,
    it’ll be the Jacobian of the function where we just discard the imaginary part
    of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some useful upshots for how `grad` works here:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use `grad` on holomorphic \(\mathbb{C} \to \mathbb{C}\) functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can use `grad` to optimize \(f : \mathbb{C} \to \mathbb{R}\) functions,
    like real-valued loss functions of complex parameters `x`, by taking steps in
    the direction of the conjugate of `grad(f)(x)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we have an \(\mathbb{R} \to \mathbb{R}\) function that just happens to use
    some complex-valued operations internally (some of which must be non-holomorphic,
    e.g. FFTs used in convolutions) then `grad` still works and we get the same result
    that an implementation using only real values would have given.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In any case, JVPs and VJPs are always unambiguous. And if we wanted to compute
    the full Jacobian matrix of a non-holomorphic \(\mathbb{C} \to \mathbb{C}\) function,
    we can do it with JVPs or VJPs!
  prefs: []
  type: TYPE_NORMAL
- en: 'You should expect complex numbers to work everywhere in JAX. Here’s differentiating
    through a Cholesky decomposition of a complex matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: More advanced autodiff
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this notebook, we worked through some easy, and then progressively more complicated,
    applications of automatic differentiation in JAX. We hope you now feel that taking
    derivatives in JAX is easy and powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a whole world of other autodiff tricks and functionality out there.
    Topics we didn’t cover, but hope to in an “Advanced Autodiff Cookbook” include:'
  prefs: []
  type: TYPE_NORMAL
- en: Gauss-Newton Vector Products, linearizing once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom VJPs and JVPs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient derivatives at fixed-points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the trace of a Hessian using random Hessian-vector products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward-mode autodiff using only reverse-mode autodiff.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking derivatives with respect to custom data types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpointing (binomial checkpointing for efficient reverse-mode, not model
    snapshotting).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing VJPs with Jacobian pre-accumulation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
