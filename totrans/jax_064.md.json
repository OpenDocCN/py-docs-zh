["```py\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z \n```", "```py\nfrom typing import NamedTuple\n\nclass Primitive(NamedTuple):\n  name: str\n\nadd_p = Primitive('add')\nmul_p = Primitive('mul')\nneg_p = Primitive(\"neg\")\nsin_p = Primitive(\"sin\")\ncos_p = Primitive(\"cos\")\nreduce_sum_p = Primitive(\"reduce_sum\")\ngreater_p = Primitive(\"greater\")\nless_p = Primitive(\"less\")\ntranspose_p = Primitive(\"transpose\")\nbroadcast_p = Primitive(\"broadcast\")\n\ndef add(x, y): return bind1(add_p, x, y)\ndef mul(x, y): return bind1(mul_p, x, y)\ndef neg(x): return bind1(neg_p, x)\ndef sin(x): return bind1(sin_p, x)\ndef cos(x): return bind1(cos_p, x)\ndef greater(x, y): return bind1(greater_p, x, y)\ndef less(x, y): return bind1(less_p, x, y)\ndef transpose(x, perm): return bind1(transpose_p, x, perm=perm)\ndef broadcast(x, shape, axes): return bind1(broadcast_p, x, shape=shape, axes=axes)\ndef reduce_sum(x, axis=None):\n  if axis is None:\n    axis = tuple(range(np.ndim(x)))\n  if type(axis) is int:\n    axis = (axis,)\n  return bind1(reduce_sum_p, x, axis=axis)\n\ndef bind1(prim, *args, **params):\n  out, = bind(prim, *args, **params)\n  return out \n```", "```py\nfrom collections.abc import Sequence\nfrom contextlib import contextmanager\nfrom typing import Optional, Any\n\nclass MainTrace(NamedTuple):\n  level: int\n  trace_type: type['Trace']\n  global_data: Optional[Any]\n\ntrace_stack: list[MainTrace] = []\ndynamic_trace: Optional[MainTrace] = None  # to be employed in Part 3\n\n@contextmanager\ndef new_main(trace_type: type['Trace'], global_data=None):\n  level = len(trace_stack)\n  main = MainTrace(level, trace_type, global_data)\n  trace_stack.append(main)\n\n  try:\n    yield main\n  finally:\n    trace_stack.pop() \n```", "```py\nclass Trace:\n  main: MainTrace\n\n  def __init__(self, main: MainTrace) -> None:\n    self.main = main\n\n  def pure(self, val): assert False  # must override\n  def lift(self, val): assert False  # must override\n\n  def process_primitive(self, primitive, tracers, params):\n    assert False  # must override \n```", "```py\nimport numpy as np\n\nclass Tracer:\n  _trace: Trace\n\n  __array_priority__ = 1000\n\n  @property\n  def aval(self):\n    assert False  # must override\n\n  def full_lower(self):\n    return self  # default implementation\n\n  def __neg__(self): return self.aval._neg(self)\n  def __add__(self, other): return self.aval._add(self, other)\n  def __radd__(self, other): return self.aval._radd(self, other)\n  def __mul__(self, other): return self.aval._mul(self, other)\n  def __rmul__(self, other): return self.aval._rmul(self, other)\n  def __gt__(self, other): return self.aval._gt(self, other)\n  def __lt__(self, other): return self.aval._lt(self, other)\n  def __bool__(self): return self.aval._bool(self)\n  def __nonzero__(self): return self.aval._nonzero(self)\n\n  def __getattr__(self, name):\n    try:\n      return getattr(self.aval, name)\n    except AttributeError:\n      raise AttributeError(f\"{self.__class__.__name__} has no attribute {name}\")\n\ndef swap(f): return lambda x, y: f(y, x) \n```", "```py\nclass ShapedArray:\n  array_abstraction_level = 1\n  shape: tuple[int, ...]\n  dtype: np.dtype\n\n  def __init__(self, shape, dtype):\n    self.shape = shape\n    self.dtype = dtype\n\n  @property\n  def ndim(self):\n    return len(self.shape)\n\n  _neg = staticmethod(neg)\n  _add = staticmethod(add)\n  _radd = staticmethod(swap(add))\n  _mul = staticmethod(mul)\n  _rmul = staticmethod(swap(mul))\n  _gt = staticmethod(greater)\n  _lt = staticmethod(less)\n\n  @staticmethod\n  def _bool(tracer):\n    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n\n  @staticmethod\n  def _nonzero(tracer):\n    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n\n  def str_short(self):\n    return f'{self.dtype.name}[{\",\".join(str(d)  for  d  in  self.shape)}]'\n\n  def __hash__(self):\n    return hash((self.shape, self.dtype))\n\n  def __eq__(self, other):\n    return (type(self) is type(other) and\n            self.shape == other.shape and self.dtype == other.dtype)\n\n  def __repr__(self):\n    return f\"ShapedArray(shape={self.shape}, dtype={self.dtype})\"\n\nclass ConcreteArray(ShapedArray):\n  array_abstraction_level = 2\n  val: np.ndarray\n\n  def __init__(self, val):\n    self.val = val\n    self.shape = val.shape\n    self.dtype = val.dtype\n\n  @staticmethod\n  def _bool(tracer):\n    return bool(tracer.aval.val)\n\n  @staticmethod\n  def _nonzero(tracer):\n    return bool(tracer.aval.val)\n\ndef get_aval(x):\n  if isinstance(x, Tracer):\n    return x.aval\n  elif type(x) in jax_types:\n    return ConcreteArray(np.asarray(x))\n  else:\n    raise TypeError(x)\n\njax_types = {bool, int, float,\n             np.bool_, np.int32, np.int64, np.float32, np.float64, np.ndarray} \n```", "```py\ndef bind(prim, *args, **params):\n  top_trace = find_top_trace(args)\n  tracers = [full_raise(top_trace, arg) for arg in args]\n  outs = top_trace.process_primitive(prim, tracers, params)\n  return [full_lower(out) for out in outs] \n```", "```py\nimport operator as op\n\ndef find_top_trace(xs) -> Trace:\n  top_main = max((x._trace.main for x in xs if isinstance(x, Tracer)),\n                 default=trace_stack[0], key=op.attrgetter('level'))\n  if dynamic_trace and dynamic_trace.level > top_main.level:\n    top_main = dynamic_trace\n  return top_main.trace_type(top_main) \n```", "```py\ndef full_lower(val: Any):\n  if isinstance(val, Tracer):\n    return val.full_lower()\n  else:\n    return val\n\ndef full_raise(trace: Trace, val: Any) -> Tracer:\n  if not isinstance(val, Tracer):\n    assert type(val) in jax_types\n    return trace.pure(val)\n  level = trace.main.level\n  if val._trace.main is trace.main:\n    return val\n  elif val._trace.main.level < level:\n    return trace.lift(val)\n  elif val._trace.main.level > level:\n    raise Exception(f\"Can't lift level {val._trace.main.level} to {level}.\")\n  else:  # val._trace.level == level\n    raise Exception(f\"Different traces at same level: {val._trace}, {trace}.\") \n```", "```py\nclass EvalTrace(Trace):\n  pure = lift = lambda self, x: x  # no boxing in Tracers needed\n\n  def process_primitive(self, primitive, tracers, params):\n    return impl_rules[primitive](*tracers, **params)\n\ntrace_stack.append(MainTrace(0, EvalTrace, None))  # special bottom of the stack\n\n# NB: in JAX, instead of a dict we attach impl rules to the Primitive instance\nimpl_rules = {}\n\nimpl_rules[add_p] = lambda x, y: [np.add(x, y)]\nimpl_rules[mul_p] = lambda x, y: [np.multiply(x, y)]\nimpl_rules[neg_p] = lambda x: [np.negative(x)]\nimpl_rules[sin_p] = lambda x: [np.sin(x)]\nimpl_rules[cos_p] = lambda x: [np.cos(x)]\nimpl_rules[reduce_sum_p] = lambda x, *, axis: [np.sum(x, axis)]\nimpl_rules[greater_p] = lambda x, y: [np.greater(x, y)]\nimpl_rules[less_p] = lambda x, y: [np.less(x, y)]\nimpl_rules[transpose_p] = lambda x, *, perm: [np.transpose(x, perm)]\n\ndef broadcast_impl(x, *, shape, axes):\n  for axis in sorted(axes):\n    x = np.expand_dims(x, axis)\n  return [np.broadcast_to(x, shape)]\nimpl_rules[broadcast_p] = broadcast_impl \n```", "```py\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nprint(f(3.0)) \n```", "```py\n2.7177599838802657 \n```", "```py\nimport builtins\n\ndef zeros_like(val):\n  aval = get_aval(val)\n  return np.zeros(aval.shape, aval.dtype)\n\ndef unzip2(pairs):\n  lst1, lst2 = [], []\n  for x1, x2 in pairs:\n    lst1.append(x1)\n    lst2.append(x2)\n  return lst1, lst2\n\ndef map(f, *xs):\n  return list(builtins.map(f, *xs))\n\ndef zip(*args):\n  fst, *rest = args = map(list, args)\n  n = len(fst)\n  for arg in rest:\n    assert len(arg) == n\n  return list(builtins.zip(*args)) \n```", "```py\nclass JVPTracer(Tracer):\n  def __init__(self, trace, primal, tangent):\n    self._trace = trace\n    self.primal = primal\n    self.tangent = tangent\n\n  @property\n  def aval(self):\n    return get_aval(self.primal)\n\nclass JVPTrace(Trace):\n  pure = lift = lambda self, val: JVPTracer(self, val, zeros_like(val))\n\n  def process_primitive(self, primitive, tracers, params):\n    primals_in, tangents_in = unzip2((t.primal, t.tangent) for t in tracers)\n    jvp_rule = jvp_rules[primitive]\n    primal_outs, tangent_outs = jvp_rule(primals_in, tangents_in, **params)\n    return [JVPTracer(self, x, t) for x, t in zip(primal_outs, tangent_outs)]\n\njvp_rules = {} \n```", "```py\ndef add_jvp(primals, tangents):\n  (x, y), (x_dot, y_dot) = primals, tangents\n  return [x + y], [x_dot + y_dot]\njvp_rules[add_p] = add_jvp\n\ndef mul_jvp(primals, tangents):\n  (x, y), (x_dot, y_dot) = primals, tangents\n  return [x * y], [x_dot * y + x * y_dot]\njvp_rules[mul_p] = mul_jvp\n\ndef sin_jvp(primals, tangents):\n  (x,), (x_dot,) = primals, tangents\n  return [sin(x)], [cos(x) * x_dot]\njvp_rules[sin_p] = sin_jvp\n\ndef cos_jvp(primals, tangents):\n  (x,), (x_dot,) = primals, tangents\n  return [cos(x)], [-sin(x) * x_dot]\njvp_rules[cos_p] = cos_jvp\n\ndef neg_jvp(primals, tangents):\n  (x,), (x_dot,) = primals, tangents\n  return [neg(x)], [neg(x_dot)]\njvp_rules[neg_p] = neg_jvp\n\ndef reduce_sum_jvp(primals, tangents, *, axis):\n  (x,), (x_dot,) = primals, tangents\n  return [reduce_sum(x, axis)], [reduce_sum(x_dot, axis)]\njvp_rules[reduce_sum_p] = reduce_sum_jvp\n\ndef greater_jvp(primals, tangents):\n  (x, y), _ = primals, tangents\n  out_primal = greater(x, y)\n  return [out_primal], [zeros_like(out_primal)]\njvp_rules[greater_p] = greater_jvp\n\ndef less_jvp(primals, tangents):\n  (x, y), _ = primals, tangents\n  out_primal = less(x, y)\n  return [out_primal], [zeros_like(out_primal)]\njvp_rules[less_p] = less_jvp \n```", "```py\ndef jvp_v1(f, primals, tangents):\n  with new_main(JVPTrace) as main:\n    trace = JVPTrace(main)\n    tracers_in = [JVPTracer(trace, x, t) for x, t in zip(primals, tangents)]\n    out = f(*tracers_in)\n    tracer_out = full_raise(trace, out)\n    primal_out, tangent_out = tracer_out.primal, tracer_out.tangent\n  return primal_out, tangent_out \n```", "```py\nx = 3.0\ny, sin_deriv_at_3 = jvp_v1(sin, (x,), (1.0,))\nprint(sin_deriv_at_3)\nprint(cos(3.0)) \n```", "```py\n-0.9899924966004454\n-0.9899924966004454 \n```", "```py\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nx, xdot = 3., 1.\ny, ydot = jvp_v1(f, (x,), (xdot,))\nprint(y)\nprint(ydot) \n```", "```py\n2.7177599838802657\n2.979984993200891 \n```", "```py\ndef deriv(f):\n  return lambda x: jvp_v1(f, (x,), (1.,))[1]\n\nprint(deriv(sin)(3.))\nprint(deriv(deriv(sin))(3.))\nprint(deriv(deriv(deriv(sin)))(3.))\nprint(deriv(deriv(deriv(deriv(sin))))(3.)) \n```", "```py\n-0.9899924966004454\n-0.1411200080598672\n0.9899924966004454\n0.1411200080598672 \n```", "```py\ndef f(x):\n  if x > 0.:  # Python control flow\n    return 2. * x\n  else:\n    return x\n\nprint(deriv(f)(3.))\nprint(deriv(f)(-3.)) \n```", "```py\n2.0\n1.0 \n```", "```py\ndef jvp_flat(f, primals, tangents):\n  with new_main(JVPTrace) as main:\n    trace = JVPTrace(main)\n    tracers_in = [JVPTracer(trace, x, t) for x, t in zip(primals, tangents)]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    primals_out, tangents_out = unzip2((t.primal, t.tangent) for t in tracers_out)\n  return primals_out, tangents_out \n```", "```py\ndef jvp(f, primals, tangents):\n  primals_flat, in_tree = tree_flatten(primals)\n  tangents_flat, in_tree2 = tree_flatten(tangents)\n  if in_tree != in_tree2: raise TypeError\n  f, out_tree = flatten_fun(f, in_tree)\n  primals_out_flat, tangents_out_flat = jvp_flat(f, primals_flat, tangents_flat)\n  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n  tangents_out = tree_unflatten(out_tree(), tangents_out_flat)\n  return primals_out, tangents_out \n```", "```py\ndef flatten_fun(f, in_tree):\n  store = Store()\n\n  def flat_fun(*args_flat):\n    pytree_args = tree_unflatten(in_tree, args_flat)\n    out = f(*pytree_args)\n    out_flat, out_tree = tree_flatten(out)\n    store.set_value(out_tree)\n    return out_flat\n\n  return flat_fun, store\n\nclass Empty: pass\nempty = Empty()\n\nclass Store:\n  val = empty\n\n  def set_value(self, val):\n    assert self.val is empty\n    self.val = val\n\n  def __call__(self):\n    return self.val \n```", "```py\nfrom collections.abc import Hashable, Iterable, Iterator\nimport itertools as it\nfrom typing import Callable\n\nclass NodeType(NamedTuple):\n  name: str\n  to_iterable: Callable\n  from_iterable: Callable\n\ndef register_pytree_node(ty: type, to_iter: Callable, from_iter: Callable\n                         ) -> None:\n  node_types[ty] = NodeType(str(ty), to_iter, from_iter)\n\nnode_types: dict[type, NodeType] = {}\nregister_pytree_node(tuple, lambda t: (None, t), lambda _, xs: tuple(xs))\nregister_pytree_node(list,  lambda l: (None, l), lambda _, xs:  list(xs))\nregister_pytree_node(dict,\n                     lambda d: map(tuple, unzip2(sorted(d.items()))),\n                     lambda keys, vals: dict(zip(keys, vals)))\n\nclass PyTreeDef(NamedTuple):\n  node_type: NodeType\n  node_metadata: Hashable\n  child_treedefs: tuple['PyTreeDef', ...]\n\nclass Leaf: pass\nleaf = Leaf()\n\ndef tree_flatten(x: Any) -> tuple[list[Any], PyTreeDef]:\n  children_iter, treedef = _tree_flatten(x)\n  return list(children_iter), treedef\n\ndef _tree_flatten(x: Any) -> tuple[Iterable, PyTreeDef]:\n  node_type = node_types.get(type(x))\n  if node_type:\n    node_metadata, children = node_type.to_iterable(x)\n    children_flat, child_trees = unzip2(map(_tree_flatten, children))\n    flattened = it.chain.from_iterable(children_flat)\n    return flattened, PyTreeDef(node_type, node_metadata, tuple(child_trees))\n  else:\n    return [x], leaf\n\ndef tree_unflatten(treedef: PyTreeDef, xs: list[Any]) -> Any:\n  return _tree_unflatten(treedef, iter(xs))\n\ndef _tree_unflatten(treedef: PyTreeDef, xs: Iterator) -> Any:\n  if treedef is leaf:\n    return next(xs)\n  else:\n    children = (_tree_unflatten(t, xs) for t in treedef.child_treedefs)\n    return treedef.node_type.from_iterable(treedef.node_metadata, children) \n```", "```py\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return {'hi': z, 'there': [x, y]}\n\nx, xdot = 3., 1.\ny, ydot = jvp(f, (x,), (xdot,))\nprint(y)\nprint(ydot) \n```", "```py\n{'hi': np.float64(2.7177599838802657), 'there': [3.0, np.float64(0.2822400161197344)]}\n{'hi': np.float64(2.979984993200891), 'there': [1.0, np.float64(-1.9799849932008908)]} \n```", "```py\ndef mapped_aval(batch_dim, aval):\n  shape = list(aval.shape)\n  del shape[batch_dim]\n  return ShapedArray(tuple(shape), aval.dtype)\n\ndef move_batch_axis(axis_size, src, dst, x):\n  if src is not_mapped:\n    target_shape = list(np.shape(x))\n    target_shape.insert(dst, axis_size)\n    return broadcast(x, target_shape, [dst])\n  elif src == dst:\n    return x\n  else:\n    return moveaxis(x, src, dst)\n\ndef moveaxis(x, src: int, dst: int):\n  perm = [i for i in range(np.ndim(x)) if i != src]\n  perm.insert(dst, src)\n  return transpose(x, perm) \n```", "```py\nfrom typing import Union\n\nclass NotMapped: pass\nnot_mapped = NotMapped()\n\nBatchAxis = Union[NotMapped, int]\n\nclass BatchTracer(Tracer):\n  def __init__(self, trace, val, batch_dim: BatchAxis):\n    self._trace = trace\n    self.val = val\n    self.batch_dim = batch_dim\n\n  @property\n  def aval(self):\n    if self.batch_dim is not_mapped:\n      return get_aval(self.val)\n    else:\n      return mapped_aval(self.batch_dim, get_aval(self.val))\n\n  def full_lower(self):\n    if self.batch_dim is not_mapped:\n      return full_lower(self.val)\n    else:\n      return self\n\nclass BatchTrace(Trace):\n  pure = lift = lambda self, val: BatchTracer(self, val, not_mapped)\n\n  def process_primitive(self, primitive, tracers, params):\n    vals_in, bdims_in = unzip2((t.val, t.batch_dim) for t in tracers)\n    vmap_rule = vmap_rules[primitive]\n    val_outs, bdim_outs = vmap_rule(self.axis_size, vals_in, bdims_in, **params)\n    return [BatchTracer(self, x, bd) for x, bd in zip(val_outs, bdim_outs)]\n\n  @property\n  def axis_size(self):\n    return self.main.global_data\n\nvmap_rules = {} \n```", "```py\nfrom functools import partial\n\ndef binop_batching_rule(op, axis_size, vals_in, dims_in):\n  (x, y), (x_bdim, y_bdim) = vals_in, dims_in\n  if x_bdim != y_bdim:\n    if x_bdim is not_mapped:\n      x = move_batch_axis(axis_size, x_bdim, y_bdim, x)\n      x_bdim = y_bdim\n    else:\n      y = move_batch_axis(axis_size, y_bdim, x_bdim, y)\n  return [op(x, y)], [x_bdim]\nvmap_rules[add_p] = partial(binop_batching_rule, add)\nvmap_rules[mul_p] = partial(binop_batching_rule, mul)\n\ndef vectorized_unop_batching_rule(op, axis_size, vals_in, dims_in):\n  (x,), (x_bdim,) = vals_in, dims_in\n  return [op(x)], [x_bdim]\nvmap_rules[sin_p] = partial(vectorized_unop_batching_rule, sin)\nvmap_rules[cos_p] = partial(vectorized_unop_batching_rule, cos)\nvmap_rules[neg_p] = partial(vectorized_unop_batching_rule, neg)\n\ndef reduce_sum_batching_rule(axis_size, vals_in, dims_in, *, axis):\n  (x,), (x_bdim,) = vals_in, dims_in\n  new_axis = tuple(ax + (x_bdim <= ax) for ax in axis)\n  out_bdim = x_bdim - sum(ax < x_bdim for ax in axis)\n  return [reduce_sum(x, new_axis)], [out_bdim]\nvmap_rules[reduce_sum_p] = reduce_sum_batching_rule \n```", "```py\ndef vmap_flat(f, in_axes, *args):\n  axis_size, = {x.shape[ax] for x, ax in zip(args, in_axes)\n                if ax is not not_mapped}\n  with new_main(BatchTrace, axis_size) as main:\n    trace = BatchTrace(main)\n    tracers_in = [BatchTracer(trace, x, ax) if ax is not None else x\n                  for x, ax in zip(args, in_axes)]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    vals_out, bdims_out = unzip2((t.val, t.batch_dim) for t in tracers_out)\n  outs_transposed = [move_batch_axis(axis_size, bdim, 0, val_out)\n                     for val_out, bdim in zip(vals_out, bdims_out)]\n  return outs_transposed\n\ndef vmap(f, in_axes):\n  def batched_f(*args):\n    args_flat, in_tree = tree_flatten(args)\n    in_axes_flat, in_tree2 = tree_flatten(in_axes)\n    if in_tree != in_tree2: raise TypeError\n    f_flat, out_tree = flatten_fun(f, in_tree)\n    outs_flat = vmap_flat(f_flat, in_axes_flat, *args_flat)\n    return tree_unflatten(out_tree(), outs_flat)\n  return batched_f \n```", "```py\ndef add_one_to_a_scalar(scalar):\n  assert np.ndim(scalar) == 0\n  return 1 + scalar\n\nvector_in = np.arange(3.)\nvector_out = vmap(add_one_to_a_scalar, (0,))(vector_in)\n\nprint(vector_in)\nprint(vector_out) \n```", "```py\n[0\\. 1\\. 2.]\n[1\\. 2\\. 3.] \n```", "```py\ndef jacfwd(f, x):\n  pushfwd = lambda v: jvp(f, (x,), (v,))[1]\n  vecs_in = np.eye(np.size(x)).reshape(np.shape(x) * 2)\n  return vmap(pushfwd, (0,))(vecs_in)\n\ndef f(x):\n  return sin(x)\n\njacfwd(f, np.arange(3.)) \n```", "```py\narray([[ 1\\.        ,  0\\.        , -0\\.        ],\n       [ 0\\.        ,  0.54030231, -0\\.        ],\n       [ 0\\.        ,  0\\.        , -0.41614684]]) \n```", "```py\njaxpr ::=\n  { lambda <binder> , ... .\n    let <eqn>\n        ...\n    in ( <atom> , ... ) }\n\nbinder ::= <var>:<array_type>\nvar ::= a | b | c | ...\natom ::= <var> | <literal>\nliteral ::= <int32> | <int64> | <float32> | <float64>\n\neqn ::= <binder> , ... = <primitive> [ <params> ] <atom> , ... \n```", "```py\njaxpr_type ::= [ <array_type> , ... ] -> [ <array_type> , ... ]\narray_type ::= <dtype>[<shape>]\ndtype ::= f32 | f64 | i32 | i64\nshape ::= <int> , ... \n```", "```py\nclass Var:\n  aval: ShapedArray\n  def __init__(self, aval): self.aval = aval\n\nclass Lit:\n  val: Any\n  aval: ShapedArray\n\n  def __init__(self, val):\n    self.aval = aval = raise_to_shaped(get_aval(val))\n    self.val = np.array(val, aval.dtype)\n\nAtom = Union[Var, Lit]\n\nclass JaxprEqn(NamedTuple):\n  primitive: Primitive\n  inputs: list[Atom]\n  params: dict[str, Any]\n  out_binders: list[Var]\n\nclass Jaxpr(NamedTuple):\n  in_binders: list[Var]\n  eqns: list[JaxprEqn]\n  outs: list[Atom]\n\n  def __hash__(self): return id(self)\n  __eq__ = op.is_\n\ndef raise_to_shaped(aval):\n  return ShapedArray(aval.shape, aval.dtype) \n```", "```py\nclass JaxprType(NamedTuple):\n  in_types:  list[ShapedArray]\n  out_types: list[ShapedArray]\n\n  def __repr__(self):\n    in_types = ', '.join(aval.str_short() for aval in self.in_types)\n    out_types = ', '.join(aval.str_short() for aval in self.out_types)\n    return f'({in_types}) -> ({out_types})'\n\ndef typecheck_jaxpr(jaxpr: Jaxpr) -> JaxprType:\n  env: set[Var] = set()\n\n  for v in jaxpr.in_binders:\n    if v in env: raise TypeError\n    env.add(v)\n\n  for eqn in jaxpr.eqns:\n    in_types = [typecheck_atom(env, x) for x in eqn.inputs]\n    out_types = abstract_eval_rules[eqn.primitive](*in_types, **eqn.params)\n    for out_binder, out_type in zip(eqn.out_binders, out_types):\n      if not out_type == out_binder.aval: raise TypeError\n    for out_binder in eqn.out_binders:\n      if out_binder in env: raise TypeError\n      env.add(out_binder)\n\n  in_types = [v.aval for v in jaxpr.in_binders]\n  out_types = [typecheck_atom(env, x) for x in jaxpr.outs]\n  return JaxprType(in_types, out_types)\n\ndef typecheck_atom(env: set[Var], x: Atom) -> ShapedArray:\n  if isinstance(x, Var):\n    if x not in env: raise TypeError(\"unbound variable\")\n    return x.aval\n  elif isinstance(x, Lit):\n    return raise_to_shaped(get_aval(x.val))\n  else:\n    assert False \n```", "```py\ndef eval_jaxpr(jaxpr: Jaxpr, args: list[Any]) -> list[Any]:\n  env: dict[Var, Any] = {}\n\n  def read(x: Atom) -> Any:\n    return env[x] if type(x) is Var else x.val\n\n  def write(v: Var, val: Any) -> None:\n    assert v not in env  # single-assignment\n    env[v] = val\n\n  map(write, jaxpr.in_binders, args)\n  for eqn in jaxpr.eqns:\n    in_vals = map(read, eqn.inputs)\n    outs = bind(eqn.primitive, *in_vals, **eqn.params)\n    map(write, eqn.out_binders, outs)\n  return map(read, jaxpr.outs)\n\ndef jaxpr_as_fun(jaxpr: Jaxpr):\n  return lambda *args: eval_jaxpr(jaxpr, args) \n```", "```py\ndef split_list(lst: list[Any], n: int) -> tuple[list[Any], list[Any]]:\n  assert 0 <= n <= len(lst)\n  return lst[:n], lst[n:]\n\ndef partition_list(bs: list[bool], l: list[Any]) -> tuple[list[Any], list[Any]]:\n  assert len(bs) == len(l)\n  lists = lst1, lst2 = [], []\n  for b, x in zip(bs, l):\n    lists[b].append(x)\n  return lst1, lst2 \n```", "```py\n# NB: the analogous class in JAX is called 'DynamicJaxprTracer'\nclass JaxprTracer(Tracer):\n  __slots__ = ['aval']\n  aval: ShapedArray\n\n  def __init__(self, trace, aval):\n    self._trace = trace\n    self.aval = aval\n\n# NB: the analogous class in JAX is called 'DynamicJaxprTrace'\nclass JaxprTrace(Trace):\n  def new_arg(self, aval: ShapedArray) -> JaxprTracer:\n    aval = raise_to_shaped(aval)\n    tracer = self.builder.new_tracer(self, aval)\n    self.builder.tracer_to_var[id(tracer)] = Var(aval)\n    return tracer\n\n  def get_or_make_const_tracer(self, val: Any) -> JaxprTracer:\n    tracer = self.builder.const_tracers.get(id(val))\n    if tracer is None:\n      tracer = self.builder.new_tracer(self, raise_to_shaped(get_aval(val)))\n      self.builder.add_const(tracer, val)\n    return tracer\n  pure = lift = get_or_make_const_tracer\n\n  def process_primitive(self, primitive, tracers, params):\n    avals_in = [t.aval for t in tracers]\n    avals_out = abstract_eval_rules[primitive](*avals_in, **params)\n    out_tracers = [self.builder.new_tracer(self, a) for a in avals_out]\n    inputs = [self.builder.getvar(t) for t in tracers]\n    outvars = [self.builder.add_var(t) for t in out_tracers]\n    self.builder.add_eqn(JaxprEqn(primitive, inputs, params, outvars))\n    return out_tracers\n\n  @property\n  def builder(self):\n    return self.main.global_data\n\n# NB: in JAX, we instead attach abstract eval rules to Primitive instances\nabstract_eval_rules = {} \n```", "```py\nclass JaxprBuilder:\n  eqns: list[JaxprEqn]\n  tracer_to_var: dict[int, Var]\n  const_tracers: dict[int, JaxprTracer]\n  constvals: dict[Var, Any]\n  tracers: list[JaxprTracer]\n\n  def __init__(self):\n    self.eqns = []\n    self.tracer_to_var = {}\n    self.const_tracers = {}\n    self.constvals = {}\n    self.tracers = []\n\n  def new_tracer(self, trace: JaxprTrace, aval: ShapedArray) -> JaxprTracer:\n    tracer = JaxprTracer(trace, aval)\n    self.tracers.append(tracer)\n    return tracer\n\n  def add_eqn(self, eqn: JaxprEqn) -> None:\n    self.eqns.append(eqn)\n\n  def add_var(self, tracer: JaxprTracer) -> Var:\n    assert id(tracer) not in self.tracer_to_var\n    var = self.tracer_to_var[id(tracer)] = Var(tracer.aval)\n    return var\n\n  def getvar(self, tracer: JaxprTracer) -> Var:\n    var = self.tracer_to_var.get(id(tracer))\n    assert var is not None\n    return var\n\n  def add_const(self, tracer: JaxprTracer, val: Any) -> Var:\n    var = self.add_var(tracer)\n    self.const_tracers[id(val)] = tracer\n    self.constvals[var] = val\n    return var\n\n  def build(self, in_tracers: list[JaxprTracer], out_tracers: list[JaxprTracer]\n            ) -> tuple[Jaxpr, list[Any]]:\n    constvars, constvals = unzip2(self.constvals.items())\n    t2v = lambda t: self.tracer_to_var[id(t)]\n    in_binders = constvars + [t2v(t) for t in in_tracers]\n    out_vars = [t2v(t) for t in out_tracers]\n    jaxpr = Jaxpr(in_binders, self.eqns, out_vars)\n    typecheck_jaxpr(jaxpr)\n    jaxpr, constvals = _inline_literals(jaxpr, constvals)\n    return jaxpr, constvals \n```", "```py\ndef _inline_literals(jaxpr: Jaxpr, consts: list[Any]) -> tuple[Jaxpr, list[Any]]:\n  const_binders, other_binders = split_list(jaxpr.in_binders, len(consts))\n  scalars = [type(x) in jax_types and not get_aval(x).shape for x in consts]\n  new_const_binders, lit_binders = partition_list(scalars, const_binders)\n  new_consts, lit_vals = partition_list(scalars, consts)\n  literals = dict(zip(lit_binders, map(Lit, lit_vals)))\n  new_eqns = [JaxprEqn(eqn.primitive, [literals.get(x, x) for x in eqn.inputs],\n                       eqn.params, eqn.out_binders) for eqn in jaxpr.eqns]\n  new_outs = [literals.get(x, x) for x in jaxpr.outs]\n  new_jaxpr = Jaxpr(new_const_binders + other_binders, new_eqns, new_outs)\n  typecheck_jaxpr(new_jaxpr)\n  return new_jaxpr, new_consts \n```", "```py\ndef binop_abstract_eval(x: ShapedArray, y: ShapedArray) -> list[ShapedArray]:\n  if not isinstance(x, ShapedArray) or not isinstance(y, ShapedArray):\n    raise TypeError\n  if raise_to_shaped(x) != raise_to_shaped(y): raise TypeError\n  return [ShapedArray(x.shape, x.dtype)]\n\nabstract_eval_rules[add_p] = binop_abstract_eval\nabstract_eval_rules[mul_p] = binop_abstract_eval\n\ndef compare_abstract_eval(x: ShapedArray, y: ShapedArray) -> list[ShapedArray]:\n  if not isinstance(x, ShapedArray) or not isinstance(y, ShapedArray):\n    raise TypeError\n  if x.shape != y.shape: raise TypeError\n  return [ShapedArray(x.shape, np.dtype('bool'))]\nabstract_eval_rules[greater_p] = compare_abstract_eval\nabstract_eval_rules[less_p] = compare_abstract_eval\n\ndef vectorized_unop_abstract_eval(x: ShapedArray) -> list[ShapedArray]:\n  return [ShapedArray(x.shape, x.dtype)]\n\nabstract_eval_rules[sin_p] = vectorized_unop_abstract_eval\nabstract_eval_rules[cos_p] = vectorized_unop_abstract_eval\nabstract_eval_rules[neg_p] = vectorized_unop_abstract_eval\n\ndef reduce_sum_abstract_eval(x: ShapedArray, *, axis: tuple[int, ...]\n                             ) -> list[ShapedArray]:\n  axis_ = set(axis)\n  new_shape = [d for i, d in enumerate(x.shape) if i not in axis_]\n  return [ShapedArray(tuple(new_shape), x.dtype)]\nabstract_eval_rules[reduce_sum_p] = reduce_sum_abstract_eval\n\ndef broadcast_abstract_eval(x: ShapedArray, *, shape: Sequence[int],\n                            axes: Sequence[int]) -> list[ShapedArray]:\n  return [ShapedArray(tuple(shape), x.dtype)]\nabstract_eval_rules[broadcast_p] = broadcast_abstract_eval \n```", "```py\nfrom functools import lru_cache\n\n@lru_cache()  # ShapedArrays are hashable\ndef make_jaxpr_v1(f, *avals_in):\n  avals_in, in_tree = tree_flatten(avals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n\n  builder = JaxprBuilder()\n  with new_main(JaxprTrace, builder) as main:\n    trace = JaxprTrace(main)\n    tracers_in = [trace.new_arg(aval) for aval in avals_in]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    jaxpr, consts = builder.build(tracers_in, tracers_out)\n  return jaxpr, consts, out_tree() \n```", "```py\nfrom collections import defaultdict\nimport string\n\nclass PPrint:\n  lines: list[tuple[int, str]]\n\n  def __init__(self, lines):\n    self.lines = lines\n\n  def indent(self, indent: int) -> 'PPrint':\n    return PPrint([(indent + orig_indent, s) for orig_indent, s in self.lines])\n\n  def __add__(self, rhs: 'PPrint') -> 'PPrint':\n    return PPrint(self.lines + rhs.lines)\n\n  def __rshift__(self, rhs: 'PPrint') -> 'PPrint':\n    if not rhs.lines: return self\n    if not self.lines: return rhs\n    indent, s = self.lines[-1]\n    indented_block = rhs.indent(indent + len(s))\n    common_line = s + ' ' * rhs.lines[0][0] + rhs.lines[0][1]\n    return PPrint(self.lines[:-1]\n                  + [(indent, common_line)]\n                  + indented_block.lines[1:])\n\n  def __str__(self) -> str:\n    return '\\n'.join(' ' * indent + s for indent, s in self.lines)\n\ndef pp(s: Any) -> PPrint:\n  return PPrint([(0, line) for line in str(s).splitlines()])\n\ndef vcat(ps: list[PPrint]) -> PPrint:\n  return sum(ps, pp(''))\n\ndef pp_jaxpr(jaxpr: Jaxpr) -> PPrint:\n  namegen = (''.join(s) for r in it.count(1)\n             for s in it.permutations(string.ascii_lowercase, r))\n  names = defaultdict(lambda: next(namegen))\n  in_binders = ', '.join(var_str(names, x) for x in jaxpr.in_binders)\n  eqns = vcat([pp_eqn(names, e) for e in jaxpr.eqns])\n  outs = ', '.join(names[v] if isinstance(v, Var) else str(v.val)\n                   for v in jaxpr.outs)\n  return (pp(f'{{ lambda {in_binders} .') +\n          ((pp('let ') >> eqns) + pp(f'in ( {outs} ) }}')).indent(2))\n\ndef var_str(names: defaultdict[Var, str], v: Var) -> str:\n  return f'{names[v]}:{v.aval.str_short()}'\n\ndef pp_eqn(names: defaultdict[Var, str], eqn: JaxprEqn) -> PPrint:\n  rule = pp_rules.get(eqn.primitive)\n  if rule:\n    return rule(names, eqn)\n  else:\n    lhs = pp(' '.join(var_str(names, v) for v in eqn.out_binders))\n    rhs = (pp(eqn.primitive.name) >> pp_params(eqn.params) >>\n           pp(' '.join(names[x] if isinstance(x, Var) else str(x.val)\n                       for x in eqn.inputs)))\n    return lhs >> pp(' = ') >> rhs\n\ndef pp_params(params: dict[str, Any]) -> PPrint:\n  items = sorted(params.items())\n  if items:\n    return pp(' [ ') >> vcat([pp(f'{k}={v}') for k, v in items]) >> pp(' ] ')\n  else:\n    return pp(' ')\n\nJaxpr.__repr__ = lambda self: str(pp_jaxpr(self))\npp_rules: dict[Primitive, Callable[..., PPrint]] = {} \n```", "```py\njaxpr, consts, _ = make_jaxpr_v1(lambda x: 2. * x, raise_to_shaped(get_aval(3.)))\nprint(jaxpr)\nprint(typecheck_jaxpr(jaxpr)) \n```", "```py\n{ lambda a:float64[] .\n  let b:float64[] = mul 2.0 a\n  in ( b ) }\n(float64[]) -> (float64[]) \n```", "```py\njaxpr, consts, _ = make_jaxpr_v1(lambda: mul(2., 2.))\nprint(jaxpr) \n```", "```py\n{ lambda  .\n  let \n  in ( 4.0 ) } \n```", "```py\n@contextmanager\ndef new_dynamic(main: MainTrace):\n  global dynamic_trace\n  prev_dynamic_trace, dynamic_trace = dynamic_trace, main\n  try:\n    yield\n  finally:\n    dynamic_trace = prev_dynamic_trace\n\n@lru_cache()\ndef make_jaxpr(f: Callable, *avals_in: ShapedArray,\n               ) -> tuple[Jaxpr, list[Any], PyTreeDef]:\n  avals_in, in_tree = tree_flatten(avals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n\n  builder = JaxprBuilder()\n  with new_main(JaxprTrace, builder) as main:\n    with new_dynamic(main):\n      trace = JaxprTrace(main)\n      tracers_in = [trace.new_arg(aval) for aval in avals_in]\n      outs = f(*tracers_in)\n      tracers_out = [full_raise(trace, out) for out in outs]\n      jaxpr, consts = builder.build(tracers_in, tracers_out)\n  return jaxpr, consts, out_tree()\n\njaxpr, consts, _ = make_jaxpr(lambda: mul(2., 2.))\nprint(jaxpr) \n```", "```py\n{ lambda  .\n  let a:float64[] = mul 2.0 2.0\n  in ( a ) } \n```", "```py\ndef jit(f):\n  def f_jitted(*args):\n    avals_in = [raise_to_shaped(get_aval(x)) for x in args]\n    jaxpr, consts, out_tree = make_jaxpr(f, *avals_in)\n    outs = bind(xla_call_p, *consts, *args, jaxpr=jaxpr, num_consts=len(consts))\n    return tree_unflatten(out_tree, outs)\n  return f_jitted\n\nxla_call_p = Primitive('xla_call') \n```", "```py\nclass IDHashable:\n  val: Any\n\n  def __init__(self, val):\n    self.val = val\n\n  def __hash__(self) -> int:\n    return id(self.val)\n\n  def __eq__(self, other):\n    return type(other) is IDHashable and id(self.val) == id(other.val) \n```", "```py\nfrom jax._src import xla_bridge as xb\nfrom jax._src.lib import xla_client as xc\nxe = xc._xla\nxops = xc._xla.ops\n\ndef xla_call_impl(*args, jaxpr: Jaxpr, num_consts: int):\n  consts, args = args[:num_consts], args[num_consts:]\n  hashable_consts = tuple(map(IDHashable, consts))\n  execute = xla_callable(IDHashable(jaxpr), hashable_consts)\n  return execute(*args)\nimpl_rules[xla_call_p] = xla_call_impl\n\n@lru_cache()\ndef xla_callable(hashable_jaxpr: IDHashable,\n                 hashable_consts: tuple[IDHashable, ...]):\n  jaxpr: Jaxpr = hashable_jaxpr.val\n  typecheck_jaxpr(jaxpr)\n  consts = [x.val for x in hashable_consts]\n  in_avals = [v.aval for v in jaxpr.in_binders[len(consts):]]\n  c = xc.XlaBuilder('xla_call')\n  xla_consts = _xla_consts(c, consts)\n  xla_params = _xla_params(c, in_avals)\n  outs = jaxpr_subcomp(c, jaxpr, xla_consts + xla_params)\n  out = xops.Tuple(c, outs)\n  compiled = xb.get_backend(None).compile(\n    xc._xla.mlir.xla_computation_to_mlir_module(c.build(out)))\n  return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n\ndef _xla_consts(c: xe.XlaBuilder, consts: list[Any]) -> list[xe.XlaOp]:\n  unique_consts = {id(cnst): cnst for cnst in consts}\n  xla_consts = {\n      id_: xops.ConstantLiteral(c, cnst) for id_, cnst in unique_consts.items()}\n  return [xla_consts[id(cnst)] for cnst in consts]\n\ndef _xla_params(c: xe.XlaBuilder, avals_in: list[ShapedArray]) -> list[xe.XlaOp]:\n  return [xops.Parameter(c, i, _xla_shape(a)) for i, a in enumerate(avals_in)]\n\ndef _xla_shape(aval: ShapedArray) -> xe.Shape:\n  return xc.Shape.array_shape(xc.dtype_to_etype(aval.dtype), aval.shape) \n```", "```py\ndef jaxpr_subcomp(c: xe.XlaBuilder, jaxpr: Jaxpr, args: list[xe.XlaOp]\n                  ) -> list[xe.XlaOp]:\n  env: dict[Var, xe.XlaOp] = {}\n\n  def read(x: Atom) -> xe.XlaOp:\n    return env[x] if type(x) is Var else xops.Constant(c, np.asarray(x.val))\n\n  def write(v: Var, val: xe.XlaOp) -> None:\n    env[v] = val\n\n  map(write, jaxpr.in_binders, args)\n  for eqn in jaxpr.eqns:\n    in_avals = [x.aval for x in eqn.inputs]\n    in_vals = map(read, eqn.inputs)\n    rule = xla_translations[eqn.primitive]\n    out_vals = rule(c, in_avals, in_vals, **eqn.params)\n    map(write, eqn.out_binders, out_vals)\n  return map(read, jaxpr.outs)\n\ndef execute_compiled(compiled, out_avals, *args):\n  input_bufs = [input_handlers[type(x)](x) for x in args]\n  out_bufs = compiled.execute(input_bufs)\n  return [handle_result(aval, buf) for aval, buf in zip(out_avals, out_bufs)]\n\ndefault_input_handler = xb.get_backend(None).buffer_from_pyval\ninput_handlers = {ty: default_input_handler for ty in\n                  [bool, int, float, np.ndarray, np.float64, np.float32]}\n\ndef handle_result(aval: ShapedArray, buf):\n  del aval  # Unused for now\n  return np.asarray(buf)\n\nxla_translations = {} \n```", "```py\ndef direct_translation(op, c, in_avals, in_vals):\n  del c, in_avals\n  return [op(*in_vals)]\n\nxla_translations[add_p] = partial(direct_translation, xops.Add)\nxla_translations[mul_p] = partial(direct_translation, xops.Mul)\nxla_translations[neg_p] = partial(direct_translation, xops.Neg)\nxla_translations[sin_p] = partial(direct_translation, xops.Sin)\nxla_translations[cos_p] = partial(direct_translation, xops.Cos)\nxla_translations[greater_p] = partial(direct_translation, xops.Gt)\nxla_translations[less_p] = partial(direct_translation, xops.Lt)\n\ndef reduce_sum_translation(c, in_avals, in_vals, *, axis):\n  (x_aval,), (x,) = in_avals, in_vals\n  zero = xops.ConstantLiteral(c, np.array(0, x_aval.dtype))\n  subc = xc.XlaBuilder('add')\n  shape = _xla_shape(ShapedArray((), x_aval.dtype))\n  xops.Add(xops.Parameter(subc, 0, shape), xops.Parameter(subc, 1, shape))\n  return [xops.Reduce(c, [x], [zero], subc.build(), axis)]\nxla_translations[reduce_sum_p] = reduce_sum_translation\n\ndef broadcast_translation(c, in_avals, in_vals, *, shape, axes):\n  x, = in_vals\n  dims_complement = [i for i in range(len(shape)) if i not in axes]\n  return [xops.BroadcastInDim(x, shape, dims_complement)]\nxla_translations[broadcast_p] = broadcast_translation \n```", "```py\n@jit\ndef f(x, y):\n  print('tracing!')\n  return sin(x) * cos(y) \n```", "```py\nz = f(3., 4.)  # 'tracing!' prints the first time\nprint(z) \n```", "```py\ntracing!\n-0.09224219304455371 \n```", "```py\nz = f(4., 5.)  # 'tracing!' doesn't print, compilation cache hit!\nprint(z) \n```", "```py\n-0.21467624978306993 \n```", "```py\n@jit\ndef f(x):\n  return reduce_sum(x, axis=0)\n\nprint(f(np.array([1., 2., 3.]))) \n```", "```py\n6.0 \n```", "```py\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\ndef deriv(f):\n  return lambda x: jvp(f, (x,), (1.,))[1]\n\nprint(    deriv(deriv(f))(3.))\nprint(jit(deriv(deriv(f)))(3.)) \n```", "```py\n0.2822400161197344\n0.2822400161197344 \n```", "```py\ndef xla_call_jvp_rule(primals, tangents, *, jaxpr, num_consts):\n  del num_consts  # Unused\n  new_jaxpr, new_consts = jvp_jaxpr(jaxpr)\n  outs = bind(xla_call_p, *new_consts, *primals, *tangents, jaxpr=new_jaxpr,\n              num_consts=len(new_consts))\n  n = len(outs) // 2\n  primals_out, tangents_out = outs[:n], outs[n:]\n  return primals_out, tangents_out\njvp_rules[xla_call_p] = xla_call_jvp_rule\n\n@lru_cache()\ndef jvp_jaxpr(jaxpr: Jaxpr) -> tuple[Jaxpr, list[Any]]:\n  def jvp_traceable(*primals_and_tangents):\n    n = len(primals_and_tangents) // 2\n    primals, tangents = primals_and_tangents[:n], primals_and_tangents[n:]\n    return jvp(jaxpr_as_fun(jaxpr), primals, tangents)\n\n  in_avals = [v.aval for v in jaxpr.in_binders]\n  new_jaxpr, new_consts, _ = make_jaxpr(jvp_traceable, *in_avals, *in_avals)\n  return new_jaxpr, new_consts \n```", "```py\ndef xla_call_vmap_rule(axis_size, vals_in, dims_in, *, jaxpr, num_consts):\n  del num_consts  # Unused\n  new_jaxpr, new_consts = vmap_jaxpr(jaxpr, axis_size, tuple(dims_in))\n  outs = bind(xla_call_p, *new_consts, *vals_in, jaxpr=new_jaxpr,\n              num_consts=len(new_consts))\n  return outs, [0] * len(outs)\nvmap_rules[xla_call_p] = xla_call_vmap_rule\n\n@lru_cache()\ndef vmap_jaxpr(jaxpr: Jaxpr, axis_size: int, bdims_in: tuple[BatchAxis, ...]\n               ) -> tuple[Jaxpr, list[Any]]:\n  vmap_traceable = vmap(jaxpr_as_fun(jaxpr), tuple(bdims_in))\n  in_avals = [unmapped_aval(axis_size, d, v.aval)\n              for v, d in zip(jaxpr.in_binders, bdims_in)]\n  new_jaxpr, new_consts, _ = make_jaxpr(vmap_traceable, *in_avals)\n  return new_jaxpr, new_consts\n\ndef unmapped_aval(axis_size: int, batch_dim: BatchAxis, aval: ShapedArray\n                  ) -> ShapedArray:\n  if batch_dim is not_mapped:\n    return aval\n  else:\n    shape = list(aval.shape)\n    shape.insert(batch_dim, axis_size)\n    return ShapedArray(tuple(shape), aval.dtype) \n```", "```py\ndef xla_call_abstract_eval_rule(*in_types, jaxpr, num_consts):\n  del num_consts  # Unused\n  jaxpr_type = typecheck_jaxpr(jaxpr)\n  if not all(t1 == t2 for t1, t2 in zip(jaxpr_type.in_types, in_types)):\n    raise TypeError\n  return jaxpr_type.out_types\nabstract_eval_rules[xla_call_p] = xla_call_abstract_eval_rule\n\ndef xla_call_translation(c, in_avals, in_vals, *, jaxpr, num_consts):\n  del num_consts  # Only used at top-level.\n  # Calling jaxpr_subcomp directly would inline. We generate a Call HLO instead.\n  subc = xc.XlaBuilder('inner xla_call')\n  xla_params = _xla_params(subc, in_avals)\n  outs = jaxpr_subcomp(subc, jaxpr, xla_params)\n  subc = subc.build(xops.Tuple(subc, outs))\n  return destructure_tuple(c, xops.Call(c, subc, in_vals))\nxla_translations[xla_call_p] = xla_call_translation\n\ndef destructure_tuple(c, tup):\n  num_elements = len(c.get_shape(tup).tuple_shapes())\n  return [xops.GetTupleElement(tup, i) for i in range(num_elements)] \n```", "```py\n@jit\ndef f(x):\n  print('tracing!')\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nx, xdot = 3., 1.\ny, ydot = jvp(f, (x,), (xdot,))\nprint(y)\nprint(ydot) \n```", "```py\ntracing!\n2.7177599838802657\n2.979984993200891 \n```", "```py\ny, ydot = jvp(f, (x,), (xdot,))  # 'tracing!' not printed \n```", "```py\nys = vmap(f, (0,))(np.arange(3.))\nprint(ys) \n```", "```py\n[ 0\\.         -0.68294197  0.18140515] \n```", "```py\ndef handle_result(aval: ShapedArray, buf):  # noqa: F811\n  return Array(aval, buf)\n\nclass Array:\n  buf: Any\n  aval: ShapedArray\n\n  def __init__(self, aval, buf):\n    self.aval = aval\n    self.buf = buf\n\n  dtype = property(lambda self: self.aval.dtype)\n  shape = property(lambda self: self.aval.shape)\n  ndim  = property(lambda self: self.aval.ndim)\n\n  def __array__(self): return np.asarray(self.buf)\n  def __repr__(self):  return repr(np.asarray(self.buf))\n  def __str__(self):   return str(np.asarray(self.buf))\n\n  _neg = staticmethod(neg)\n  _add = staticmethod(add)\n  _radd = staticmethod(add)\n  _mul = staticmethod(mul)\n  _rmul = staticmethod(mul)\n  _gt = staticmethod(greater)\n  _lt = staticmethod(less)\ninput_handlers[Array] = lambda x: x.buf\n\njax_types.add(Array) \n```", "```py\n@jit\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nx, xdot = 3., 1.\ny, ydot = jvp(f, (x,), (xdot,))\nprint(y)\nprint(ydot) \n```", "```py\n2.7177599838802657\n2.979984993200891 \n```", "```py\ndef pprint_xla_call(names: defaultdict[Var, str], eqn: JaxprEqn) -> PPrint:\n  lhs = pp(' '.join(var_str(names, v) for v in eqn.out_binders))\n  params_without_jaxpr = {k:v for k, v in eqn.params.items() if k != 'jaxpr'}\n  rhs = (pp(eqn.primitive.name) >> pp_params(params_without_jaxpr) >>\n         pp(' '.join(names[x] if isinstance(x, Var) else str(x.val)\n                     for x in eqn.inputs)))\n  return vcat([lhs >> pp(' = ') >> rhs,\n               pp_jaxpr(eqn.params['jaxpr']).indent(2)])\npp_rules[xla_call_p] = pprint_xla_call \n```", "```py\ny, f_lin = linearize(f, x)\ny_dot = f_lin(x_dot) \n```", "```py\ny, y_dot = jvp(f, (x,), (x_dot,)) \n```", "```py\njvp : (a -> b) -> (UnrestrictedUse a, T a) -o (UnrestrictedUse b, T b) \n```", "```py\ndef split_half(lst: list[Any]) -> tuple[list[Any], list[Any]]:\n  assert not len(lst) % 2\n  return split_list(lst, len(lst) // 2)\n\ndef merge_lists(which: list[bool], l1: list[Any], l2: list[Any]) -> list[Any]:\n  l1, l2 = iter(l1), iter(l2)\n  out = [next(l2) if b else next(l1) for b in which]\n  assert next(l1, None) is next(l2, None) is None\n  return out \n```", "```py\ndef linearize_flat(f, *primals_in):\n  pvals_in = ([PartialVal.known(x) for x in primals_in] +\n              [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n  def f_jvp(*primals_tangents_in):\n    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n    return [*primals_out, *tangents_out]\n  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)\n  primal_pvals, _ = split_half(pvals_out)\n  assert all(pval.is_known for pval in primal_pvals)\n  primals_out = [pval.const for pval in primal_pvals]\n  f_lin = lambda *tangents: eval_jaxpr(jaxpr, [*consts, *tangents])\n  return primals_out, f_lin\n\ndef linearize(f, *primals_in):\n  primals_in_flat, in_tree = tree_flatten(primals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n  primals_out_flat, f_lin_flat = linearize_flat(f, *primals_in_flat)\n  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n\n  def f_lin(*tangents_in):\n    tangents_in_flat, in_tree2 = tree_flatten(tangents_in)\n    if in_tree != in_tree2: raise TypeError\n    tangents_out_flat = f_lin_flat(*tangents_in_flat)\n    return tree_unflatten(out_tree(), tangents_out_flat)\n\n  return primals_out, f_lin\n\ndef vspace(aval: ShapedArray) -> ShapedArray:\n  return raise_to_shaped(aval)  # TODO handle integers? \n```", "```py\npartial_eval : ((a1, a2) -> (b1, b2)) -> a1 -> exists r. (b1, r, (r, a2) -> b2) \n```", "```py\n{ lambda a:float64[] .\n  let b:float64[] = sin a\n      c:float64[] = neg b\n  in ( c ) } \n```", "```py\n{ lambda a:float64[] b:float64[] .\n  let c:float64[] = sin a\n      d:float64[] = cos a\n      e:float64[] = mul d b\n      f:float64[] = neg c\n      g:float64[] = neg e\n  in ( f, g ) } \n```", "```py\n{ lambda a:float64[] .\n  let c:float64[] = sin a\n      d:float64[] = cos a\n      f:float64[] = neg c\n  in ( f, d ) } \n```", "```py\n{ lambda d:float64[] b:float64[] .\n  let e:float64[] = mul d b\n      g:float64[] = neg e\n  in ( g ) } \n```", "```py\nclass PartialVal(NamedTuple):\n  aval: ShapedArray\n  const: Optional[Any]\n\n  @classmethod\n  def known(cls, val: Any):\n    return PartialVal(get_aval(val), val)\n\n  @classmethod\n  def unknown(cls, aval: ShapedArray):\n    return PartialVal(aval, None)\n\n  is_known   = property(lambda self: self.const is not None)\n  is_unknown = property(lambda self: self.const is     None) \n```", "```py\ndef partial_eval_flat(f: Callable, pvals_in: list[PartialVal]\n                      ) -> tuple[Jaxpr, list[PartialVal], list[Any]]:\n  with new_main(PartialEvalTrace) as main:\n    trace = PartialEvalTrace(main)\n    tracers_in = [trace.new_arg(pval) for pval in pvals_in]\n    outs = f(*tracers_in)\n    tracers_out = [full_raise(trace, out) for out in outs]\n    pvals_out = [t.pval for t in tracers_out]\n    unk_tracers_in  = [t for t in tracers_in  if t.pval.is_unknown]\n    unk_tracers_out = [t for t in tracers_out if t.pval.is_unknown]\n    jaxpr, consts = tracers_to_jaxpr(unk_tracers_in, unk_tracers_out)\n  return jaxpr, pvals_out, consts \n```", "```py\nfrom weakref import ref, ReferenceType\n\nclass LambdaBindingRecipe(NamedTuple):\n  pass\n\nclass ConstRecipe(NamedTuple):\n  val: Any\n\nclass JaxprEqnRecipe(NamedTuple):\n  prim: Primitive\n  tracers_in: list['PartialEvalTracer']\n  params: dict[str, Any]\n  avals_out: list[ShapedArray]\n  tracer_refs_out: list['ReferenceType[PartialEvalTracer]']\n\nJaxprRecipe = Union[LambdaBindingRecipe, ConstRecipe, JaxprEqnRecipe] \n```", "```py\nclass PartialEvalTracer(Tracer):\n  pval: PartialVal\n  recipe: Optional[JaxprRecipe]\n\n  def __init__(self, trace, pval, recipe):\n    self._trace = trace\n    self.pval = pval\n    self.recipe = recipe\n\n  aval = property(lambda self: self.pval.aval)\n\n  def full_lower(self):\n    if self.pval.is_known:\n      return full_lower(self.pval.const)\n    return self \n```", "```py\nclass PartialEvalTrace(Trace):\n  def new_arg(self, pval: PartialVal) -> Any:\n    return PartialEvalTracer(self, pval, LambdaBindingRecipe())\n\n  def lift(self, val: Any) -> PartialEvalTracer:\n    return PartialEvalTracer(self, PartialVal.known(val), None)\n  pure = lift\n\n  def instantiate_const(self, tracer: PartialEvalTracer) -> PartialEvalTracer:\n    if tracer.pval.is_unknown:\n      return tracer\n    else:\n      pval = PartialVal.unknown(raise_to_shaped(tracer.aval))\n      return PartialEvalTracer(self, pval, ConstRecipe(tracer.pval.const))\n\n  def process_primitive(self, primitive, tracers, params):\n    if all(t.pval.is_known for t in tracers):\n      return bind(primitive, *map(full_lower, tracers), **params)\n    rule = partial_eval_rules.get(primitive)\n    if rule: return rule(self, tracers, **params)\n    tracers_in = [self.instantiate_const(t) for t in tracers]\n    avals_in = [t.aval for t in tracers_in]\n    avals_out = abstract_eval_rules[primitive](*avals_in, **params)\n    tracers_out = [PartialEvalTracer(self, PartialVal.unknown(aval), None)\n                   for aval in avals_out]\n    eqn = JaxprEqnRecipe(primitive, tracers_in, params, avals_out,\n                         map(ref, tracers_out))\n    for t in tracers_out: t.recipe = eqn\n    return tracers_out\n\npartial_eval_rules = {} \n```", "```py\ndef tracers_to_jaxpr(tracers_in: list[PartialEvalTracer],\n                     tracers_out: list[PartialEvalTracer]):\n  tracer_to_var: dict[int, Var] = {id(t): Var(raise_to_shaped(t.aval))\n                                   for t in tracers_in}\n  constvar_to_val: dict[int, Any] = {}\n  constid_to_var: dict[int, Var] = {}\n  processed_eqns: set[int] = set()\n  eqns: list[JaxprEqn] = []\n  for t in toposort(tracers_out, tracer_parents):\n    if isinstance(t.recipe, LambdaBindingRecipe):\n      assert id(t) in set(map(id, tracers_in))\n    elif isinstance(t.recipe, ConstRecipe):\n      val = t.recipe.val\n      var = constid_to_var.get(id(val))\n      if var is None:\n        aval = raise_to_shaped(get_aval(val))\n        var = constid_to_var[id(val)] = Var(aval)\n        constvar_to_val[var] = val\n      tracer_to_var[id(t)] = var\n    elif isinstance(t.recipe, JaxprEqnRecipe):\n      if id(t.recipe) not in processed_eqns:\n        eqns.append(recipe_to_eqn(tracer_to_var, t.recipe))\n        processed_eqns.add(id(t.recipe))\n    else:\n      raise TypeError(t.recipe)\n\n  constvars, constvals = unzip2(constvar_to_val.items())\n  in_binders = constvars + [tracer_to_var[id(t)] for t in tracers_in]\n  out_vars = [tracer_to_var[id(t)] for t in tracers_out]\n  jaxpr = Jaxpr(in_binders, eqns, out_vars)\n  typecheck_jaxpr(jaxpr)\n  return jaxpr, constvals\n\ndef recipe_to_eqn(tracer_to_var: dict[int, Var], recipe: JaxprEqnRecipe\n                  ) -> JaxprEqn:\n  inputs = [tracer_to_var[id(t)] for t in recipe.tracers_in]\n  out_binders = [Var(aval) for aval in recipe.avals_out]\n  for t_ref, var in zip(recipe.tracer_refs_out, out_binders):\n    if t_ref() is not None: tracer_to_var[id(t_ref())] = var\n  return JaxprEqn(recipe.prim, inputs, recipe.params, out_binders)\n\ndef tracer_parents(t: PartialEvalTracer) -> list[PartialEvalTracer]:\n  return t.recipe.tracers_in if isinstance(t.recipe, JaxprEqnRecipe) else [] \n```", "```py\ndef toposort(out_nodes: list[Any], parents: Callable[[Any], list[Any]]):\n  if not out_nodes: return []\n  out_nodes = remove_duplicates(out_nodes)\n\n  child_counts = {}\n  stack = list(out_nodes)\n  while stack:\n    node = stack.pop()\n    if id(node) in child_counts:\n      child_counts[id(node)] += 1\n    else:\n      child_counts[id(node)] = 1\n      stack.extend(parents(node))\n  for node in out_nodes:\n    child_counts[id(node)] -= 1\n\n  sorted_nodes = []\n  childless_nodes = [node for node in out_nodes if not child_counts[id(node)]]\n  while childless_nodes:\n    node = childless_nodes.pop()\n    sorted_nodes.append(node)\n    for parent in parents(node):\n      if child_counts[id(parent)] == 1:\n        childless_nodes.append(parent)\n      else:\n        child_counts[id(parent)] -= 1\n\n  sorted_nodes = sorted_nodes[::-1]\n  check_toposort(sorted_nodes, parents)\n  return sorted_nodes\n\ndef remove_duplicates(lst):\n  seen = set()\n  return [x for x in lst if id(x) not in seen and not seen.add(id(x))]\n\ndef check_toposort(nodes: list[Any], parents: Callable[[Any], list[Any]]):\n  seen = set()\n  for node in nodes:\n    assert all(id(parent) in seen for parent in parents(node))\n    seen.add(id(node)) \n```", "```py\ny, sin_lin = linearize(sin, 3.)\nprint(y, sin(3.))\nprint(sin_lin(1.), cos(3.)) \n```", "```py\n0.1411200080598672 0.1411200080598672\n-0.9899924966004454 -0.9899924966004454 \n```", "```py\ndef xla_call_partial_eval(trace, tracers, *, jaxpr, num_consts):\n  del num_consts  # Unused\n  in_unknowns = [not t.pval.is_known for t in tracers]\n  jaxpr1, jaxpr2, out_unknowns, num_res = partial_eval_jaxpr(jaxpr, in_unknowns)\n  known_tracers, unknown_tracers = partition_list(in_unknowns, tracers)\n  known_vals = [t.pval.const for t in known_tracers]\n  outs1_res = bind(xla_call_p, *known_vals, jaxpr=jaxpr1, num_consts=0)\n  outs1, res = split_list(outs1_res, len(jaxpr1.outs) - num_res)\n  res_tracers = [trace.instantiate_const(full_raise(trace, x)) for x in res]\n  outs2 = [PartialEvalTracer(trace, PartialVal.unknown(v.aval), None)\n           for v in jaxpr2.outs]\n  eqn = JaxprEqnRecipe(xla_call_p, res_tracers + unknown_tracers,\n                       dict(jaxpr=jaxpr2, num_consts=0),\n                       [v.aval for v in jaxpr2.outs], map(ref, outs2))\n  for t in outs2: t.recipe = eqn\n  return merge_lists(out_unknowns, outs1, outs2)\npartial_eval_rules[xla_call_p] = xla_call_partial_eval\n\ndef partial_eval_jaxpr(jaxpr: Jaxpr, in_unknowns: list[bool],\n                       instantiate: Optional[list[bool]] = None,\n                       ) -> tuple[Jaxpr, Jaxpr, list[bool], int]:\n  env: dict[Var, bool] = {}\n  residuals: set[Var] = set()\n\n  def read(x: Atom) -> bool:\n    return type(x) is Var and env[x]\n\n  def write(unk: bool, v: Var) -> None:\n    env[v] = unk\n\n  def new_res(x: Atom) -> Atom:\n    if type(x) is Var: residuals.add(x)\n    return x\n\n  eqns1, eqns2 = [], []\n  map(write, in_unknowns, jaxpr.in_binders)\n  for eqn in jaxpr.eqns:\n    unks_in = map(read, eqn.inputs)\n    rule = partial_eval_jaxpr_rules.get(eqn.primitive)\n    if rule:\n      eqn1, eqn2, unks_out, res = rule(unks_in, eqn)\n      eqns1.append(eqn1); eqns2.append(eqn2); residuals.update(res)\n      map(write, unks_out, eqn.out_binders)\n    elif any(unks_in):\n      inputs = [v if unk else new_res(v) for unk, v in zip(unks_in, eqn.inputs)]\n      eqns2.append(JaxprEqn(eqn.primitive, inputs, eqn.params, eqn.out_binders))\n      map(partial(write, True), eqn.out_binders)\n    else:\n      eqns1.append(eqn)\n      map(partial(write, False), eqn.out_binders)\n  out_unknowns = map(read, jaxpr.outs)\n  if instantiate is not None:\n    for v, uk, inst in zip(jaxpr.outs, out_unknowns, instantiate):\n      if inst and not uk: new_res(v)\n    out_unknowns = map(op.or_, out_unknowns, instantiate)\n\n  residuals, num_res = list(residuals), len(residuals)\n  assert all(type(v) is Var for v in residuals), residuals\n\n  ins1, ins2 = partition_list(in_unknowns, jaxpr.in_binders)\n  outs1, outs2 = partition_list(out_unknowns, jaxpr.outs)\n\n  jaxpr1 = Jaxpr(ins1, eqns1, outs1 + residuals)\n  jaxpr2 = Jaxpr(residuals + ins2, eqns2, outs2)\n  typecheck_partial_eval_jaxpr(jaxpr, in_unknowns, out_unknowns, jaxpr1, jaxpr2)\n\n  return jaxpr1, jaxpr2, out_unknowns, num_res\n\ndef typecheck_partial_eval_jaxpr(jaxpr, unks_in, unks_out, jaxpr1, jaxpr2):\n  jaxprty = typecheck_jaxpr(jaxpr)    # (a1,  a2) -> (b1, b2 )\n  jaxpr1ty = typecheck_jaxpr(jaxpr1)  #  a1       -> (b1, res)\n  jaxpr2ty = typecheck_jaxpr(jaxpr2)  # (res, a2) -> b2\n\n  a1, a2 = partition_list(unks_in, jaxprty.in_types)\n  b1, b2 = partition_list(unks_out, jaxprty.out_types)\n  b1_, res = split_list(jaxpr1ty.out_types, len(b1))\n  res_, a2_ = split_list(jaxpr2ty.in_types, len(res))\n  b2_ = jaxpr2ty.out_types\n\n  if jaxpr1ty.in_types != a1: raise TypeError\n  if jaxpr2ty.out_types != b2: raise TypeError\n  if b1 != b1_: raise TypeError\n  if res != res_: raise TypeError\n  if a2 != a2_: raise TypeError\n  if b2 != b2_: raise TypeError\n\npartial_eval_jaxpr_rules = {}\n\ndef xla_call_peval_eqn(unks_in: list[bool], eqn: JaxprEqn,\n                       ) -> tuple[JaxprEqn, JaxprEqn, list[bool], list[Var]]:\n  jaxpr = eqn.params['jaxpr']\n  jaxpr1, jaxpr2, unks_out, num_res = partial_eval_jaxpr(jaxpr, unks_in)\n  ins1, ins2 = partition_list(unks_in, eqn.inputs)\n  out_binders1, out_binders2 = partition_list(unks_out, eqn.out_binders)\n  residuals = [Var(v.aval) for v in jaxpr2.in_binders[:num_res]]\n  eqn1 = JaxprEqn(xla_call_p, ins1, dict(jaxpr=jaxpr1, num_consts=0),\n                  out_binders1 + residuals)\n  eqn2 = JaxprEqn(xla_call_p, residuals + ins2,\n                  dict(jaxpr=jaxpr2, num_consts=0), out_binders2)\n  return eqn1, eqn2, unks_out, residuals\npartial_eval_jaxpr_rules[xla_call_p] = xla_call_peval_eqn \n```", "```py\n@jit\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\ny, f_lin = linearize(f, 3.)\ny_dot = f_lin(1.)\nprint(y, y_dot) \n```", "```py\n2.7177599838802657 2.979984993200891 \n```", "```py\n@jit\ndef f(x):\n  y = sin(x) * 2.\n  z = g(x, y)\n  return z\n\n@jit\ndef g(x, y):\n  return cos(x) + y\n\ny, f_lin = linearize(f, 3.)\ny_dot = f_lin(1.)\nprint(y, y_dot) \n```", "```py\n-0.7077524804807109 -2.121105001260758 \n```", "```py\nlinearize : (a -> b) -> a -> (b, T a -o T b)\nvjp       : (a -> b) -> a -> (b, T b -o T a) \n```", "```py\ndef vjp(f, x):\n  y, f_lin = linearize(f, x)\n  f_vjp = lambda y_bar: transpose(f_lin)(y_bar)\n  return y, f_vjp \n```", "```py\ndef vjp_flat(f, *primals_in):\n  pvals_in = ([PartialVal.known(x) for x in primals_in] +\n              [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n  primal_pvals_in, tangent_pvals_in = split_half(pvals_in)\n  def f_jvp(*primals_tangents_in):\n    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n    return [*primals_out, *tangents_out]\n  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)  # linearize\n  primal_pvals, _ = split_half(pvals_out)\n  assert all(pval.is_known for pval in primal_pvals)\n  primals_out = [pval.const for pval in primal_pvals]\n  transpose_inputs = consts + [UndefPrimal(p.aval) for p in tangent_pvals_in]\n  f_vjp = lambda *cts: eval_jaxpr_transposed(jaxpr, transpose_inputs, cts)\n  return primals_out, f_vjp\n\ndef vjp(f, *primals_in):\n  primals_in_flat, in_tree = tree_flatten(primals_in)\n  f, out_tree = flatten_fun(f, in_tree)\n  primals_out_flat, f_vjp_flat = vjp_flat(f, *primals_in_flat)\n  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n\n  def f_vjp(*cotangents_out):\n    cotangents_out_flat, _ = tree_flatten(cotangents_out)\n    cotangents_in_flat = f_vjp_flat(*cotangents_out_flat)\n    return tree_unflatten(in_tree, cotangents_in_flat)\n\n  return primals_out, f_vjp\n\nclass UndefPrimal(NamedTuple):\n  aval: ShapedArray\n\nregister_pytree_node(UndefPrimal,\n                     lambda u: (u.aval, ()),\n                     lambda aval, _: UndefPrimal(aval)) \n```", "```py\n# NB: the analogous function in JAX is called 'backward_pass'\ndef eval_jaxpr_transposed(jaxpr: Jaxpr, args: list[Any], cotangents: list[Any]\n                          ) -> list[Any]:\n  primal_env: dict[Var, Any] = {}\n  ct_env: dict[Var, Any] = {}\n\n  def read_primal(x: Atom) -> Any:\n    return primal_env.get(x, UndefPrimal(x.aval)) if type(x) is Var else x.val\n\n  def write_primal(v: Var, val: Any) -> None:\n    if type(val) is not UndefPrimal:\n      primal_env[v] = val\n\n  def read_cotangent(v: Var) -> Any:\n    return ct_env.pop(v, np.zeros(v.aval.shape, v.aval.dtype))\n\n  def write_cotangent(x: Atom, val: Any):\n    if type(x) is Var and val is not None:\n      ct_env[x] = add(ct_env[x], val) if x in ct_env else val\n\n  map(write_primal, jaxpr.in_binders, args)\n  map(write_cotangent, jaxpr.outs, cotangents)\n  for eqn in jaxpr.eqns[::-1]:\n    primals_in = map(read_primal, eqn.inputs)\n    cts_in = map(read_cotangent, eqn.out_binders)\n    rule = transpose_rules[eqn.primitive]\n    cts_out = rule(cts_in, *primals_in, **eqn.params)\n    map(write_cotangent, eqn.inputs, cts_out)\n\n  return [read_cotangent(v) for v, x in zip(jaxpr.in_binders, args)\n          if type(x) is UndefPrimal]\n\ntranspose_rules = {} \n```", "```py\ndef mul_transpose_rule(cts, x, y):\n  z_bar, = cts\n  assert (type(x) is UndefPrimal) ^ (type(y) is UndefPrimal)\n  return [mul(z_bar, y), None] if type(x) is UndefPrimal else [None, mul(x, z_bar)]\ntranspose_rules[mul_p] = mul_transpose_rule\n\ndef neg_transpose_rule(cts, x):\n  ybar, = cts\n  assert type(x) is UndefPrimal\n  return [neg(ybar)]\ntranspose_rules[neg_p] = neg_transpose_rule\n\ndef add_transpose_rule(cts, x, y):\n  z_bar, = cts\n  return [z_bar, z_bar]\ntranspose_rules[add_p] = add_transpose_rule\n\ndef reduce_sum_transpose_rule(cts, x, *, axis):\n  y_bar, = cts\n  return [broadcast(y_bar, x.aval.shape, axis)]\ntranspose_rules[reduce_sum_p] = reduce_sum_transpose_rule\n\ndef xla_call_transpose_rule(cts, *invals, jaxpr, num_consts):\n  del num_consts  # Unused\n  undef_primals = [type(x) is UndefPrimal for x in invals]\n  transposed_jaxpr, new_consts = transpose_jaxpr(jaxpr, tuple(undef_primals))\n  residuals, _ = partition_list(undef_primals, invals)\n  outs = bind(xla_call_p, *new_consts, *residuals, *cts,\n              jaxpr=transposed_jaxpr, num_consts=len(new_consts))\n  outs = iter(outs)\n  return [next(outs) if undef else None for undef in undef_primals]\ntranspose_rules[xla_call_p] = xla_call_transpose_rule\n\n@lru_cache()\ndef transpose_jaxpr(jaxpr: Jaxpr, undef_primals: tuple[bool, ...]\n                    ) -> tuple[Jaxpr, list[Any]]:\n  avals_in, avals_out = typecheck_jaxpr(jaxpr)\n  traceable = partial(eval_jaxpr_transposed, jaxpr)\n  args = [UndefPrimal(a) if u else a for a, u in zip(avals_in, undef_primals)]\n  trans_jaxpr, consts, _ = make_jaxpr(traceable, tuple(args), tuple(avals_out))\n  typecheck_jaxpr(trans_jaxpr)\n  return trans_jaxpr, consts \n```", "```py\ndef grad(f):\n  def gradfun(x, *xs):\n    y, f_vjp = vjp(f, x, *xs)\n    if np.shape(y) != (): raise TypeError\n    x_bar, *_ = f_vjp(np.ones(np.shape(y), np.result_type(y)))\n    return x_bar\n  return gradfun \n```", "```py\ny, f_vjp = vjp(sin, 3.)\nprint(f_vjp(1.), cos(3.)) \n```", "```py\n(np.float64(-0.9899924966004454),) -0.9899924966004454 \n```", "```py\ndef f(x):\n  y = sin(x) * 2.\n  z = - y + x\n  return z\n\nprint(grad(f)(3.)) \n```", "```py\n2.979984993200891 \n```", "```py\n@jit\ndef f(x):\n  y = x * 2.\n  z = g(y)\n  return z\n\n@jit\ndef g(x):\n  return cos(x) * 2.\n\nprint(grad(f)(3.)) \n```", "```py\n1.1176619927957034 \n```", "```py\n# from core_test.py fun_with_nested_calls_2\ndef foo(x):\n  @jit\n  def bar(y):\n    def baz(w):\n      q = jit(lambda x: y)(x)\n      q = q + jit(lambda: y)()\n      q = q + jit(lambda y: w + y)(y)\n      q = jit(lambda w: jit(sin)(x) * y)(1.0) + q\n      return q\n    p, t = jvp(baz, (x + 1.0,), (y,))\n    return t + (x * p)\n  return bar(x)\n\ndef assert_allclose(*vals):\n  for v1, v2 in zip(vals[:-1], vals[1:]):\n    np.testing.assert_allclose(v1, v2)\n\nans1 = f(3.)\nans2 = jit(f)(3.)\nans3, _ = jvp(f, (3.,), (5.,))\nans4, _ = jvp(jit(f), (3.,), (5.,))\nassert_allclose(ans1, ans2, ans3, ans4)\n\nderiv1 = grad(f)(3.)\nderiv2 = grad(jit(f))(3.)\nderiv3 = jit(grad(jit(f)))(3.)\n_, deriv4 = jvp(f, (3.,), (1.,))\n_, deriv5 = jvp(jit(f), (3.,), (1.,))\nassert_allclose(deriv1, deriv2, deriv3, deriv4, deriv5)\n\nhess1 = grad(grad(f))(3.)\nhess2 = grad(grad(jit(f)))(3.)\nhess3 = grad(jit(grad(f)))(3.)\nhess4 = jit(grad(grad(f)))(3.)\n_, hess5 = jvp(grad(f), (3.,), (1.,))\n_, hess6 = jvp(jit(grad(f)), (3.,), (1.,))\n_, hess7 = jvp(jit(grad(f)), (3.,), (1.,))\nassert_allclose(hess1, hess2, hess3, hess4, hess5, hess6, hess7) \n```", "```py\ndef cond(pred, true_fn, false_fn, *operands):\n  avals_in = [raise_to_shaped(get_aval(x)) for x in operands]\n  true_jaxpr, true_consts, out_tree = make_jaxpr(true_fn, *avals_in)\n  false_jaxpr, false_consts, out_tree_ = make_jaxpr(false_fn, *avals_in)\n  if out_tree != out_tree_: raise TypeError\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  if typecheck_jaxpr(true_jaxpr) != typecheck_jaxpr(false_jaxpr):\n    raise TypeError\n  outs = bind_cond(pred, *true_consts, *false_consts, *operands,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  return tree_unflatten(out_tree, outs)\ncond_p = Primitive('cond')\n\ndef _join_jaxpr_consts(jaxpr1: Jaxpr, jaxpr2: Jaxpr, n1: int, n2: int\n                       ) -> tuple[Jaxpr, Jaxpr]:\n  jaxpr1_type, jaxpr2_type = typecheck_jaxpr(jaxpr1), typecheck_jaxpr(jaxpr2)\n  assert jaxpr1_type.in_types[n1:] == jaxpr2_type.in_types[n2:]\n  consts1, rest1 = split_list(jaxpr1.in_binders, n1)\n  consts2, rest2 = split_list(jaxpr2.in_binders, n2)\n  new_jaxpr1 = Jaxpr(consts1 + consts2 + rest1, jaxpr1.eqns, jaxpr1.outs)\n  new_jaxpr2 = Jaxpr(consts1 + consts2 + rest2, jaxpr2.eqns, jaxpr2.outs)\n  return new_jaxpr1, new_jaxpr2\n\ndef bind_cond(pred, *args, true_jaxpr, false_jaxpr):\n  assert len(args) == len(true_jaxpr.in_binders) == len(false_jaxpr.in_binders)\n  return bind(cond_p, pred, *args, true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr) \n```", "```py\ndef cond_impl(pred, *operands, true_jaxpr, false_jaxpr):\n  if pred:\n    return eval_jaxpr(true_jaxpr, operands)\n  else:\n    return eval_jaxpr(false_jaxpr, operands)\nimpl_rules[cond_p] = cond_impl \n```", "```py\nout = cond(True, lambda: 3, lambda: 4)\nprint(out) \n```", "```py\n3 \n```", "```py\ndef cond_jvp_rule(primals, tangents, *, true_jaxpr, false_jaxpr):\n  pred, *primals = primals\n  _   , *tangents = tangents\n  true_jaxpr , true_consts  = jvp_jaxpr(true_jaxpr)\n  false_jaxpr, false_consts = jvp_jaxpr(false_jaxpr)\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  assert typecheck_jaxpr(true_jaxpr) == typecheck_jaxpr(false_jaxpr)\n  outs = bind_cond(pred, *true_consts, *false_consts, *primals, *tangents,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  primals_out, tangents_out = split_half(outs)\n  return primals_out, tangents_out\njvp_rules[cond_p] = cond_jvp_rule \n```", "```py\nout, out_tan = jvp(lambda x: cond(True, lambda: x * x, lambda: 0.), (1.,), (1.,))\nprint(out_tan) \n```", "```py\n2.0 \n```", "```py\ndef cond_vmap_rule(axis_size, vals_in, dims_in, *, true_jaxpr, false_jaxpr):\n  pred    , *vals_in = vals_in\n  pred_dim, *dims_in = dims_in\n  if pred_dim is not not_mapped: raise NotImplementedError  # TODO\n  true_jaxpr, true_consts = vmap_jaxpr(true_jaxpr, axis_size, tuple(dims_in))\n  false_jaxpr, false_consts = vmap_jaxpr(false_jaxpr, axis_size, tuple(dims_in))\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  assert typecheck_jaxpr(true_jaxpr) == typecheck_jaxpr(false_jaxpr)\n  outs = bind_cond(pred, *true_consts, *false_consts, *vals_in,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  return outs, [0] * len(outs)\nvmap_rules[cond_p] = cond_vmap_rule \n```", "```py\nxs = np.array([1., 2., 3])\nout = vmap(lambda x: cond(True, lambda: x + 1., lambda: 0.), (0,))(xs)\nprint(out) \n```", "```py\n[2\\. 3\\. 4.] \n```", "```py\n{ lambda a:float32[] .\n  let\n  in ( a ) } \n```", "```py\n{ lambda a:float32[] .\n  let\n  in ( 0. ) } \n```", "```py\ndef cond_abstract_eval(pred_type, *in_types, true_jaxpr, false_jaxpr):\n  if pred_type != ShapedArray((), np.dtype('bool')): raise TypeError\n  jaxpr_type = typecheck_jaxpr(true_jaxpr)\n  if jaxpr_type != typecheck_jaxpr(false_jaxpr):\n    raise TypeError\n  if not all(t1 == t2 for t1, t2 in zip(jaxpr_type.in_types, in_types)):\n    raise TypeError\n  return jaxpr_type.out_types\nabstract_eval_rules[cond_p] = cond_abstract_eval\n\ndef cond_translation(c, in_avals, in_vals, *, true_jaxpr, false_jaxpr):\n  del in_avals  # Unused\n  pred, *in_vals = in_vals\n  flat_vals, in_tree = tree_flatten(in_vals)\n  operand = xops.Tuple(c, flat_vals)\n  operand_shape = c.get_shape(operand)\n\n  def make_comp(name: str, jaxpr: Jaxpr) -> xe.XlaComputation:\n    c = xc.XlaBuilder(name)\n    operand = xops.Parameter(c, 0, operand_shape)\n    operands = tree_unflatten(in_tree, destructure_tuple(c, operand))\n    outs = jaxpr_subcomp(c, jaxpr, operands)\n    return c.build(xops.Tuple(c, outs))\n\n  true_comp = make_comp('true_fn', true_jaxpr)\n  false_comp = make_comp('false_fn', false_jaxpr)\n\n  int_etype = xc.dtype_to_etype(np.dtype('int32'))\n  out = xops.Conditional(xops.ConvertElementType(pred, int_etype),\n                         [false_comp, true_comp], [operand] * 2)\n  return destructure_tuple(c, out)\nxla_translations[cond_p] = cond_translation \n```", "```py\nout = jit(lambda: cond(False, lambda: 1, lambda: 2))()\nprint(out) \n```", "```py\n2 \n```", "```py\ndef cond_partial_eval(trace, tracers, *, true_jaxpr, false_jaxpr):\n  pred_tracer, *tracers = tracers\n  assert pred_tracer.pval.is_known\n  pred = pred_tracer.pval.const\n  in_uks = [not t.pval.is_known for t in tracers]\n\n  *jaxprs, out_uks, num_res = _cond_partial_eval(true_jaxpr, false_jaxpr, in_uks)\n  t_jaxpr1, f_jaxpr1, t_jaxpr2, f_jaxpr2 = jaxprs\n\n  known_tracers, unknown_tracers = partition_list(in_uks, tracers)\n  known_vals = [t.pval.const for t in known_tracers]\n  outs1_res = bind_cond(pred, *known_vals,\n                        true_jaxpr=t_jaxpr1, false_jaxpr=f_jaxpr1)\n  outs1, res = split_list(outs1_res, len(outs1_res) - num_res)\n  pred_tracer_ = trace.instantiate_const(full_raise(trace, pred_tracer))\n  res_tracers = [trace.instantiate_const(full_raise(trace, x)) for x in res]\n  outs2 = [PartialEvalTracer(trace, PartialVal.unknown(v.aval), None)\n           for v in t_jaxpr2.outs]\n  eqn = JaxprEqnRecipe(cond_p, [pred_tracer_, *res_tracers, *unknown_tracers],\n                       dict(true_jaxpr=t_jaxpr2, false_jaxpr=f_jaxpr2),\n                       [v.aval for v in t_jaxpr2.outs], map(ref, outs2))\n  for t in outs2: t.recipe = eqn\n  return merge_lists(out_uks, outs1, outs2)\npartial_eval_rules[cond_p] = cond_partial_eval\n\ndef _cond_partial_eval(true_jaxpr: Jaxpr, false_jaxpr: Jaxpr, in_uks: list[bool]\n                       ) -> tuple[Jaxpr, Jaxpr, Jaxpr, Jaxpr, list[bool], int]:\n  _, _, t_out_uks, _ = partial_eval_jaxpr(true_jaxpr , in_uks)\n  _, _, f_out_uks, _ = partial_eval_jaxpr(false_jaxpr, in_uks)\n  out_uks = map(op.or_, t_out_uks, f_out_uks)\n\n  t_jaxpr1, t_jaxpr2, _, t_nres = partial_eval_jaxpr(true_jaxpr , in_uks, out_uks)\n  f_jaxpr1, f_jaxpr2, _, f_nres = partial_eval_jaxpr(false_jaxpr, in_uks, out_uks)\n\n  t_jaxpr1, f_jaxpr1 = _join_jaxpr_res(t_jaxpr1, f_jaxpr1, t_nres, f_nres)\n  t_jaxpr2, f_jaxpr2 = _join_jaxpr_consts(t_jaxpr2, f_jaxpr2, t_nres, f_nres)\n  assert typecheck_jaxpr(t_jaxpr1) == typecheck_jaxpr(f_jaxpr1)\n  assert typecheck_jaxpr(t_jaxpr2) == typecheck_jaxpr(f_jaxpr2)\n  num_res = t_nres + f_nres\n\n  return t_jaxpr1, f_jaxpr1, t_jaxpr2, f_jaxpr2, out_uks, num_res\n\ndef _join_jaxpr_res(jaxpr1: Jaxpr, jaxpr2: Jaxpr, n1: int, n2: int\n                    ) -> tuple[Jaxpr, Jaxpr]:\n  jaxpr1_type, jaxpr2_type = typecheck_jaxpr(jaxpr1), typecheck_jaxpr(jaxpr2)\n  out_types1, _ = split_list(jaxpr1_type.out_types, len(jaxpr1.outs) - n1)\n  out_types2, _ = split_list(jaxpr2_type.out_types, len(jaxpr2.outs) - n2)\n  assert out_types1 == out_types2\n  outs1, res1 = split_list(jaxpr1.outs, len(jaxpr1.outs) - n1)\n  outs2, res2 = split_list(jaxpr2.outs, len(jaxpr2.outs) - n2)\n  zeros_like1 = [Lit(np.zeros(v.aval.shape, v.aval.dtype)) for v in res1]\n  zeros_like2 = [Lit(np.zeros(v.aval.shape, v.aval.dtype)) for v in res2]\n  new_jaxpr1 = Jaxpr(jaxpr1.in_binders, jaxpr1.eqns, outs1 + res1 + zeros_like2)\n  new_jaxpr2 = Jaxpr(jaxpr2.in_binders, jaxpr2.eqns, outs2 + zeros_like1 + res2)\n  return new_jaxpr1, new_jaxpr2 \n```", "```py\n_, f_lin = linearize(lambda x: cond(True, lambda: x, lambda: 0.), 1.)\nout = f_lin(3.14)\nprint(out) \n```", "```py\n3.14 \n```", "```py\ndef cond_peval_eqn(unks_in: list[bool], eqn: JaxprEqn,\n                   ) -> tuple[JaxprEqn, JaxprEqn, list[bool], list[Atom]]:\n  pred_unk, *unks_in = unks_in\n  assert not pred_unk\n  true_jaxpr, false_jaxpr = eqn.params['true_jaxpr'], eqn.params['false_jaxpr']\n  *jaxprs, unks_out, num_res = _cond_partial_eval(true_jaxpr, false_jaxpr, unks_in)\n  t_jaxpr1, f_jaxpr1, t_jaxpr2, f_jaxpr2 = jaxprs\n  ins1, ins2 = partition_list(unks_in, eqn.inputs[1:])\n  outs1, outs2 = partition_list(unks_out, eqn.out_binders)\n  residuals, _ = split_list(t_jaxpr2.in_binders, num_res)\n  eqn1 = JaxprEqn(cond_p, [eqn.inputs[0], *ins1],\n                  dict(true_jaxpr=t_jaxpr1, false_jaxpr=f_jaxpr1),\n                  outs1 + residuals)\n  eqn2 = JaxprEqn(cond_p, [eqn.inputs[0], *residuals, *ins2],\n                  dict(true_jaxpr=t_jaxpr2, false_jaxpr=f_jaxpr2),\n                  outs2)\n  res = [eqn.inputs[0], *residuals] if type(eqn.inputs[0]) is Var else residuals\n  return eqn1, eqn2, unks_out, res\npartial_eval_jaxpr_rules[cond_p] = cond_peval_eqn \n```", "```py\n_, f_lin = linearize(jit(lambda x: cond(True, lambda: x, lambda: 0.)), 1.)\nout = f_lin(3.14)\nprint(out) \n```", "```py\n3.14 \n```", "```py\ndef cond_transpose_rule(cts, pred, *invals, true_jaxpr, false_jaxpr):\n  undef_primals = tuple(type(x) is UndefPrimal for x in invals)\n  true_jaxpr, true_consts = transpose_jaxpr(true_jaxpr, undef_primals)\n  false_jaxpr, false_consts = transpose_jaxpr(false_jaxpr, undef_primals)\n  true_jaxpr, false_jaxpr = _join_jaxpr_consts(\n      true_jaxpr, false_jaxpr, len(true_consts), len(false_consts))\n  res = [x for x in invals if type(x) is not UndefPrimal]\n  outs = bind_cond(pred, *true_consts, *false_consts, *res, *cts,\n                   true_jaxpr=true_jaxpr, false_jaxpr=false_jaxpr)\n  outs = iter(outs)\n  return [None] + [next(outs) if type(x) is UndefPrimal else None for x in invals]\ntranspose_rules[cond_p] = cond_transpose_rule \n```", "```py\nout = grad(lambda x: cond(True, lambda: x * x, lambda: 0.))(1.)\nprint(out) \n```", "```py\n2.0 \n```", "```py\ndef pprint_cond(names: defaultdict[Var, str], eqn: JaxprEqn) -> PPrint:\n  true_jaxpr, false_jaxpr = eqn.params['true_jaxpr'], eqn.params['false_jaxpr']\n  new_params = {k:v for k, v in eqn.params.items() if not k.endswith('jaxpr')}\n  lhs = pp(' '.join(var_str(names, v) for v in eqn.out_binders))\n  rhs = (pp(eqn.primitive.name) >> pp_params(new_params) >>\n         pp(' '.join(names[x] if isinstance(x, Var) else str(x.val)\n                     for x in eqn.inputs)))\n  return vcat([lhs >> pp(' = ') >> rhs,\n               pp_jaxpr(true_jaxpr).indent(2),\n               pp_jaxpr(false_jaxpr).indent(2)])\npp_rules[cond_p] = pprint_cond \n```"]