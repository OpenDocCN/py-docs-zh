["```py\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad\n\ngrad_tanh = grad(jnp.tanh)\nprint(grad_tanh(2.0)) \n```", "```py\n0.070650816 \n```", "```py\nprint(grad(grad(jnp.tanh))(2.0))\nprint(grad(grad(grad(jnp.tanh)))(2.0)) \n```", "```py\n-0.13621868\n0.25265405 \n```", "```py\nf = lambda x: x**3 + 2*x**2 - 3*x + 1\n\ndfdx = jax.grad(f) \n```", "```py\nd2fdx = jax.grad(dfdx)\nd3fdx = jax.grad(d2fdx)\nd4fdx = jax.grad(d3fdx) \n```", "```py\nprint(dfdx(1.))\nprint(d2fdx(1.))\nprint(d3fdx(1.))\nprint(d4fdx(1.)) \n```", "```py\n4.0\n10.0\n6.0\n0.0 \n```", "```py\nkey = jax.random.key(0)\n\ndef sigmoid(x):\n  return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n  return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12,  0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n  preds = predict(W, b, inputs)\n  label_probs = preds * targets + (1 - preds) * (1 - targets)\n  return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = jax.random.split(key, 3)\nW = jax.random.normal(W_key, (3,))\nb = jax.random.normal(b_key, ()) \n```", "```py\n# Differentiate `loss` with respect to the first positional argument:\nW_grad = grad(loss, argnums=0)(W, b)\nprint(f'{W_grad=}')\n\n# Since argnums=0 is the default, this does the same thing:\nW_grad = grad(loss)(W, b)\nprint(f'{W_grad=}')\n\n# But you can choose different values too, and drop the keyword:\nb_grad = grad(loss, 1)(W, b)\nprint(f'{b_grad=}')\n\n# Including tuple values\nW_grad, b_grad = grad(loss, (0, 1))(W, b)\nprint(f'{W_grad=}')\nprint(f'{b_grad=}') \n```", "```py\nW_grad=Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32)\nW_grad=Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32)\nb_grad=Array(-0.29227245, dtype=float32)\nW_grad=Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32)\nb_grad=Array(-0.29227245, dtype=float32) \n```", "```py\ndef loss2(params_dict):\n    preds = predict(params_dict['W'], params_dict['b'], inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nprint(grad(loss2)({'W': W, 'b': b})) \n```", "```py\n{'W': Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32), 'b': Array(-0.29227245, dtype=float32)} \n```", "```py\nloss_value, Wb_grad = jax.value_and_grad(loss, (0, 1))(W, b)\nprint('loss value', loss_value)\nprint('loss value', loss(W, b)) \n```", "```py\nloss value 3.0519385\nloss value 3.0519385 \n```", "```py\n# Set a step size for finite differences calculations\neps = 1e-4\n\n# Check b_grad with scalar finite differences\nb_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\nprint('b_grad_numerical', b_grad_numerical)\nprint('b_grad_autodiff', grad(loss, 1)(W, b))\n\n# Check W_grad with finite differences in a random direction\nkey, subkey = jax.random.split(key)\nvec = jax.random.normal(subkey, W.shape)\nunitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\nW_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\nprint('W_dirderiv_numerical', W_grad_numerical)\nprint('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec)) \n```", "```py\nb_grad_numerical -0.29325485\nb_grad_autodiff -0.29227245\nW_dirderiv_numerical -0.2002716\nW_dirderiv_autodiff -0.19909117 \n```", "```py\nfrom jax.test_util import check_grads\n\ncheck_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives \n```"]