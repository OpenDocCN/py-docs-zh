["```py\n>>> import jax\n>>> from jax import export\n>>> from jax import numpy as jnp\n>>> def f(x):  # f: f32[a, b]\n...   return jnp.concatenate([x, x], axis=1)\n\n>>> # We construct symbolic dimension variables.\n>>> a, b = export.symbolic_shape(\"a, b\")\n\n>>> # We can use the symbolic dimensions to construct shapes.\n>>> x_shape = (a, b)\n>>> x_shape\n(a, b)\n\n>>> # Then we export with symbolic shapes:\n>>> exp: export.Exported = export.export(jax.jit(f))(\n...     jax.ShapeDtypeStruct(x_shape, jnp.int32))\n>>> exp.in_avals\n(ShapedArray(int32[a,b]),)\n>>> exp.out_avals\n(ShapedArray(int32[a,2*b]),)\n\n>>> # We can later call with concrete shapes (with a=3 and b=4), without re-tracing `f`.\n>>> res = exp.call(np.ones((3, 4), dtype=np.int32))\n>>> res.shape\n(3, 8) \n```", "```py\n>>> def f1(x, y): # x: f32[a, 1], y : f32[a, 4]\n...  return x + y\n\n>>> # Assuming you have some actual args with concrete shapes\n>>> x = np.ones((3, 1), dtype=np.int32)\n>>> y = np.ones((3, 4), dtype=np.int32)\n>>> args_specs = export.symbolic_args_specs((x, y), \"a, ...\")\n>>> exp = export.export(jax.jit(f1))(* args_specs)\n>>> exp.in_avals\n(ShapedArray(int32[a,1]), ShapedArray(int32[a,4])) \n```", "```py\n>>> f = lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],))\n>>> arg_spec = jax.ShapeDtypeStruct(export.symbolic_shape(\"b, 4\"), jnp.int32)\n>>> exp = export.export(jax.jit(f))(arg_spec)\n>>> exp.out_avals\n(ShapedArray(int32[4*b]),) \n```", "```py\n>>> exp = export.export(jax.jit(lambda x: jnp.array(x.shape[0]) + x))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"b\"), np.int32))\n>>> exp.call(jnp.arange(3, dtype=np.int32))\nArray([3, 4, 5], dtype=int32)\n\n>>> exp = export.export(jax.jit(lambda x: x.reshape(jnp.array(x.shape[0]) + 2)))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"b\"), np.int32))  \nTraceback (most recent call last):\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>]. \n```", "```py\n>>> exp = export.export(jax.jit(\n...     lambda x: (5. + x.shape[0],\n...                x.shape[0] - np.arange(5, dtype=jnp.int32),\n...                x + x.shape[0] + jnp.sin(x.shape[0]))))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"b\"), jnp.int32))\n>>> exp.out_avals\n(ShapedArray(float32[], weak_type=True),\n ShapedArray(int32[5]),\n ShapedArray(float32[b], weak_type=True))\n\n>>> exp.call(jnp.ones((3,), jnp.int32))\n (Array(8., dtype=float32, weak_type=True),\n Array([ 3, 2, 1, 0, -1], dtype=int32),\n Array([4.14112, 4.14112, 4.14112], dtype=float32, weak_type=True)) \n```", "```py\n>>> exp = export.export(jax.jit(\n...     lambda x: jnp.sum(x, axis=0) / x.shape[0]))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"b, c\"), jnp.int32))\n>>> exp.call(jnp.arange(12, dtype=jnp.int32).reshape((3, 4)))\nArray([4., 5., 6., 7.], dtype=float32) \n```", "```py\n>>> v, = export.symbolic_shape(\"v,\")\n>>> export.export(jax.jit(lambda x, y: x + y))(\n...     jax.ShapeDtypeStruct((v,), dtype=np.int32),\n...     jax.ShapeDtypeStruct((4,), dtype=np.int32))\nTraceback (most recent call last):\nTypeError: add got incompatible shapes for broadcasting: (v,), (4,).\n\n>>> export.export(jax.jit(lambda x: jnp.matmul(x, x)))(\n...     jax.ShapeDtypeStruct((v, 4), dtype=np.int32))\nTraceback (most recent call last):\nTypeError: dot_general requires contracting dimensions to have the same shape, got (4,) and (v,). \n```", "```py\nimport jax\n>>> export.export(jax.jit(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"), dtype=np.int32))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\njax._src.export.shape_poly.InconclusiveDimensionOperation: Symbolic dimension comparison 'a + 1' >= 'b' is inconclusive.\nThis error arises for comparison operations with shapes that\nare non-constant, and the result of the operation cannot be represented as\na boolean value for all values of the symbolic dimensions involved. \n```", "```py\n>>> _ = export.export(jax.jit(lambda x: x[0:16]))(\n...    jax.ShapeDtypeStruct(export.symbolic_shape(\"b + 15\"), dtype=np.int32)) \n```", "```py\n>>> # Introduce dimension variable with constraints.\n>>> a, b = export.symbolic_shape(\"a, b\",\n...                              constraints=(\"a >= b\", \"b >= 16\"))\n>>> _ = export.export(jax.jit(lambda x: x[:x.shape[1], :16]))(\n...    jax.ShapeDtypeStruct((a, b), dtype=np.int32)) \n```", "```py\nfrom jax import lax\n>>> b, = export.symbolic_shape(\"b\")\n>>> f = lambda x: lax.slice_in_dim(x, 0, x.shape[0] % 3)\n>>> export.export(jax.jit(f))(\n...     jax.ShapeDtypeStruct((b,), dtype=np.int32))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\njax._src.export.shape_poly.InconclusiveDimensionOperation: Symbolic dimension comparison 'b' >= 'mod(b, 3)' is inconclusive.\nThis error arises for comparison operations with shapes that\nare non-constant, and the result of the operation cannot be represented as\na boolean value for all values of the symbolic dimensions involved. \n```", "```py\n>>> b, = export.symbolic_shape(\"b\",\n...                            constraints=[\"b >= mod(b, 3)\"])\n>>> f = lambda x: lax.slice_in_dim(x, 0, x.shape[0] % 3)\n>>> _ = export.export(jax.jit(f))(\n...     jax.ShapeDtypeStruct((b,), dtype=np.int32)) \n```", "```py\n>>> a1, = export.symbolic_shape(\"a,\")\n>>> a2, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\n\n>>> a1 + a2  \nTraceback (most recent call last):\nValueError: Invalid mixing of symbolic scopes for linear combination.\nExpected  scope 4776451856 created at <doctest shape_poly.md[31]>:1:6 (<module>)\nand found for 'a' (unknown) scope 4776979920 created at <doctest shape_poly.md[32]>:1:6 (<module>) with constraints:\n a >= 8 \n```", "```py\n>>> a, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\n>>> b, = export.symbolic_shape(\"b,\", scope=a.scope)  # Reuse the scope of `a`\n\n>>> a + b  # Allowed\nb + a \n```", "```py\n>>> my_scope = export.SymbolicScope()\n>>> c, = export.symbolic_shape(\"c\", scope=my_scope)\n>>> d, = export.symbolic_shape(\"d\", scope=my_scope)\n>>> c + d  # Allowed\nd + c \n```", "```py\n>>> def my_top_k(k, x):  # x: i32[4, 10], k <= 10\n...   return lax.top_k(x, k)[0]  # : i32[4, 3]\n>>> x = np.arange(40, dtype=np.int32).reshape((4, 10))\n\n>>> # Export with static `k=3`. Since `k` appears in shapes it must be in `static_argnums`.\n>>> exp_static_k = export.export(jax.jit(my_top_k, static_argnums=0))(3, x)\n>>> exp_static_k.in_avals[0]\nShapedArray(int32[4,10])\n\n>>> exp_static_k.out_avals[0]\nShapedArray(int32[4,3])\n\n>>> # When calling the exported function we pass only the non-static arguments\n>>> exp_static_k.call(x)\nArray([[ 9,  8,  7],\n [19, 18, 17],\n [29, 28, 27],\n [39, 38, 37]], dtype=int32)\n\n>>> # Now attempt to export with symbolic `k` so that we choose `k` after export.\n>>> k, = export.symbolic_shape(\"k\", constraints=[\"k <= 10\"])\n>>> export.export(jax.jit(my_top_k, static_argnums=0))(k, x)  \nTraceback (most recent call last):\nKeyError: \"Encountered dimension variable 'k' that is not appearing in the shapes of the function arguments \n```", "```py\n>>> def my_top_k_with_dimensions(dimensions, x):  # dimensions: i32[0, k], x: i32[4, 10]\n...   return my_top_k(dimensions.shape[1], x)\n>>> exp = export.export(jax.jit(my_top_k_with_dimensions))(\n...     jax.ShapeDtypeStruct((0, k), dtype=np.int32),\n...     x)\n>>> exp.in_avals\n(ShapedArray(int32[0,k]), ShapedArray(int32[4,10]))\n\n>>> exp.out_avals[0]\nShapedArray(int32[4,k])\n\n>>> # When we invoke `exp` we must construct and pass an array of shape (0, k)\n>>> exp.call(np.zeros((0, 3), dtype=np.int32), x)\nArray([[ 9,  8,  7],\n [19, 18, 17],\n [29, 28, 27],\n [39, 38, 37]], dtype=int32) \n```", "```py\n>>> a, = export.symbolic_shape(\"a\")\n>>> export.export(jax.jit(lambda x: x.shape[0]))(\n...    jax.ShapeDtypeStruct((a * a,), dtype=np.int32))  \nTraceback (most recent call last):\nValueError: Cannot solve for values of dimension variables {'a'}.\nWe can only solve linear uni-variate constraints.\nUsing the following polymorphic shapes specifications: args[0].shape = (a^2,).\nUnprocessed specifications: 'a^2' for dimension size args[0].shape[0]. \n```", "```py\n>>> def f(x):  # x: f32[b, b, 2*d]\n...   return x\n>>> exp = export.export(jax.jit(f))(\n...     jax.ShapeDtypeStruct(export.symbolic_shape(\"b, b, 2*d\"), dtype=np.int32))   \n>>> exp.call(np.ones((3, 3, 5), dtype=np.int32))  \nTraceback (most recent call last):\nValueError: Input shapes do not match the polymorphic shapes specification.\nDivision had remainder 1 when computing the value of 'd'.\nUsing the following polymorphic shapes specifications:\n args[0].shape = (b, b, 2*d).\nObtained dimension variables: 'b' = 3 from specification 'b' for dimension args[0].shape[0] (= 3), .\nPlease see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details. \n```", "```py\n>>> b, = export.symbolic_shape(\"b\")\n>>> export.export(jax.jit(lambda x: x.reshape((2, -1))))(\n...     jax.ShapeDtypeStruct((b,), dtype=np.int32))\nTraceback (most recent call last):\njax._src.core.InconclusiveDimensionOperation: Cannot divide evenly the sizes of shapes (b,) and (2, -1).\nThe remainder mod(b, - 2) should be 0. \n```", "```py\n>>> b, = export.symbolic_shape(\"b\")\n>>> # We specify that the first dimension is a multiple of 4\n>>> exp = export.export(jax.jit(lambda x: x.reshape((2, -1))))(\n...     jax.ShapeDtypeStruct((4*b,), dtype=np.int32))\n>>> exp.out_avals\n(ShapedArray(int32[2,2*b]),)\n\n>>> # We specify that some other dimension is even\n>>> exp = export.export(jax.jit(lambda x: x.reshape((2, -1))))(\n...     jax.ShapeDtypeStruct((b, 5, 6), dtype=np.int32))\n>>> exp.out_avals\n(ShapedArray(int32[2,15*b]),) \n```"]