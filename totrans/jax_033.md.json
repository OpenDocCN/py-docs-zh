["```py\n>>> import re\n>>> import numpy as np\n>>> import jax\n>>> from jax import export\n\n>>> def f(x): return 2 * x * x\n\n>>> exported: export.Exported = export.export(jax.jit(f))(\n...    jax.ShapeDtypeStruct((), np.float32))\n\n>>> # You can inspect the Exported object\n>>> exported.fun_name\n'f'\n\n>>> exported.in_avals\n(ShapedArray(float32[]),)\n\n>>> print(re.search(r\".*@main.*\", exported.mlir_module()).group(0))\n func.func public @main(%arg0: tensor<f32> {mhlo.layout_mode = \"default\"} loc(\"x\")) -> (tensor<f32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n\n>>> # And you can serialize the Exported to a bytearray.\n>>> serialized: bytearray = exported.serialize()\n\n>>> # The serialized function can later be rehydrated and called from\n>>> # another JAX computation, possibly in another process.\n>>> rehydrated_exp: export.Exported = export.deserialize(serialized)\n>>> rehydrated_exp.in_avals\n(ShapedArray(float32[]),)\n\n>>> def callee(y):\n...  return 3. * rehydrated_exp.call(y * 4.)\n\n>>> callee(1.)\nArray(96., dtype=float32) \n```", "```py\n>>> import jax\n>>> from jax import export\n>>> from typing import Callable\n\n>>> def f(x): return 7 * x * x * x\n\n>>> # Serialize 3 levels of VJP along with the primal function\n>>> blob: bytearray = export.export(jax.jit(f))(1.).serialize(vjp_order=3)\n>>> rehydrated_f: Callable = export.deserialize(blob).call\n\n>>> rehydrated_f(0.1)  # 7 * 0.1^3\nArray(0.007, dtype=float32)\n\n>>> jax.grad(rehydrated_f)(0.1)  # 7*3 * 0.1^2\nArray(0.21000001, dtype=float32)\n\n>>> jax.grad(jax.grad(rehydrated_f))(0.1)  # 7*3*2 * 0.1\nArray(4.2, dtype=float32)\n\n>>> jax.grad(jax.grad(jax.grad(rehydrated_f)))(0.1)  # 7*3*2\nArray(42., dtype=float32)\n\n>>> jax.grad(jax.grad(jax.grad(jax.grad(rehydrated_f))))(0.1)  \nTraceback (most recent call last):\nValueError: No VJP is available \n```", "```py\n>>> import jax\n>>> from jax import export\n>>> from jax import lax\n>>> from jax._src import core\n>>> from jax._src.interpreters import mlir\n>>> # Define a new primitive backed by a custom call\n>>> new_prim = core.Primitive(\"new_prim\")\n>>> _ = new_prim.def_abstract_eval(lambda x: x)\n>>> _ = mlir.register_lowering(new_prim, lambda ctx, o: mlir.custom_call(\"my_new_prim\", operands=[o], result_types=[o.type]).results)\n>>> print(jax.jit(new_prim.bind).lower(1.).compiler_ir())\nmodule @jit_bind attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n func.func public @main(%arg0: tensor<f32> {mhlo.layout_mode = \"default\"}) -> (tensor<f32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n %0 = stablehlo.custom_call @my_new_prim(%arg0) {api_version = 2 : i32} : (tensor<f32>) -> tensor<f32>\n return %0 : tensor<f32>\n }\n}\n\n>>> # If we try to export, we get an error\n>>> export.export(jax.jit(new_prim.bind))(1.)  \nTraceback (most recent call last):\nValueError: Cannot serialize code with custom calls whose targets have no compatibility guarantees: my_new_bind\n\n>>> # We can avoid the error if we pass a `DisabledSafetyCheck.custom_call`\n>>> exp = export.export(\n...    jax.jit(new_prim.bind),\n...    disabled_checks=[export.DisabledSafetyCheck.custom_call(\"my_new_prim\")])(1.) \n```", "```py\n>>> from jax import export\n>>> export.default_export_platform()\n'cpu' \n```", "```py\n>>> import jax\n>>> from jax import export\n>>> from jax import lax\n\n>>> # You can specify the export platform, e.g., `tpu`, `cpu`, `cuda`, `rocm`\n>>> # even if the current machine does not have that accelerator.\n>>> exp = export.export(jax.jit(lax.cos), platforms=['tpu'])(1.)\n\n>>> # But you will get an error if you try to compile `exp`\n>>> # on a machine that does not have TPUs.\n>>> exp.call(1.)  \nTraceback (most recent call last):\nValueError: Function 'cos' was lowered for platforms '('tpu',)' but it is used on '('cpu',)'.\n\n>>> # We can avoid the error if we pass a `DisabledSafetyCheck.platform`\n>>> # parameter to `export`, e.g., because you have reasons to believe\n>>> # that the code lowered will run adequately on the current\n>>> # compilation platform (which is the case for `cos` in this\n>>> # example):\n>>> exp_unsafe = export.export(jax.jit(lax.cos),\n...    lowering_platforms=['tpu'],\n...    disabled_checks=[export.DisabledSafetyCheck.platform()])(1.)\n\n>>> exp_unsafe.call(1.)\nArray(0.5403023, dtype=float32, weak_type=True)\n\n# and similarly with multi-platform lowering\n>>> exp_multi = export.export(jax.jit(lax.cos),\n...    lowering_platforms=['tpu', 'cpu', 'cuda'])(1.)\n>>> exp_multi.call(1.)\nArray(0.5403023, dtype=float32, weak_type=True) \n```", "```py\n>>> import jax\n>>> from jax import export\n>>> from jax import lax\n>>> # A largish function\n>>> def f(x):\n...   for i in range(1000):\n...     x = jnp.cos(x)\n...   return x\n\n>>> exp_single = export.export(jax.jit(f))(1.)\n>>> len(exp_single.mlir_module_serialized)  \n9220\n\n>>> exp_multi = export.export(jax.jit(f),\n...                           lowering_platforms=[\"cpu\", \"tpu\", \"cuda\"])(1.)\n>>> len(exp_multi.mlir_module_serialized)  \n9282 \n```", "```py\n>>> import jax\n>>> from jax import export\n>>> from jax.sharding import Mesh, NamedSharding\n>>> from jax.sharding import PartitionSpec as P\n\n>>> # Use the first 4 devices for exporting.\n>>> export_devices = jax.local_devices()[:4]\n>>> export_mesh = Mesh(export_devices, (\"a\",))\n>>> def f(x):\n...   return x.T\n\n>>> arg = jnp.arange(8 * len(export_devices))\n>>> exp = export.export(jax.jit(f, in_shardings=(NamedSharding(export_mesh, P(\"a\")),)))(arg)\n\n>>> # `exp` knows for how many devices it was exported.\n>>> exp.nr_devices\n4\n\n>>> # and it knows the shardings for the inputs. These will be applied\n>>> # when the exported is called.\n>>> exp.in_shardings_hlo\n({devices=[4]<=[4]},)\n\n>>> res1 = exp.call(jax.device_put(arg,\n...                                NamedSharding(export_mesh, P(\"a\"))))\n\n>>> # Check out the first 2 shards of the result\n>>> [f\"device={s.device} index={s.index}\" for s in res1.addressable_shards[:2]]\n['device=TFRT_CPU_0 index=(slice(0, 8, None),)',\n 'device=TFRT_CPU_1 index=(slice(8, 16, None),)']\n\n>>> # We can call `exp` with some other 4 devices and another\n>>> # mesh with a different shape, as long as the number of devices is\n>>> # the same.\n>>> other_mesh = Mesh(np.array(jax.local_devices()[2:6]).reshape((2, 2)), (\"b\", \"c\"))\n>>> res2 = exp.call(jax.device_put(arg,\n...                                NamedSharding(other_mesh, P(\"b\"))))\n\n>>> # Check out the first 2 shards of the result. Notice that the output is\n>>> # sharded similarly; this means that the input was resharded according to the\n>>> # exp.in_shardings.\n>>> [f\"device={s.device} index={s.index}\" for s in res2.addressable_shards[:2]]\n['device=TFRT_CPU_2 index=(slice(0, 8, None),)',\n 'device=TFRT_CPU_3 index=(slice(8, 16, None),)'] \n```", "```py\n>>> import jax\n>>> from jax import export\n>>> from jax.sharding import Mesh, NamedSharding\n>>> from jax.sharding import PartitionSpec as P\n\n>>> export_devices = jax.local_devices()\n>>> export_mesh = Mesh(np.array(export_devices), (\"a\",))\n>>> def f(x):\n...   return x.T\n\n>>> arg = jnp.arange(4 * len(export_devices))\n>>> exp = export.export(jax.jit(f, in_shardings=(NamedSharding(export_mesh, P(\"a\")),)))(arg)\n\n>>> exp.call(arg)  \nTraceback (most recent call last):\nValueError: Exported module f was lowered for 8 devices and is called in a context with 1 devices. This is disallowed because: the module was lowered for more than 1 device. \n```", "```py\n>>> import jax\n>>> from jax import export\n>>> from jax.sharding import Mesh, NamedSharding\n>>> from jax.sharding import PartitionSpec as P\n\n>>> export_devices = jax.local_devices()\n>>> export_mesh = Mesh(np.array(export_devices), (\"a\",))\n>>> def f(x):\n...   return x.T\n\n>>> arg = jnp.arange(4 * len(export_devices))\n>>> exp = export.export(jax.jit(f, in_shardings=(NamedSharding(export_mesh, P(\"a\")),)))(arg)\n\n>>> # Prepare the mesh for calling `exp`.\n>>> calling_mesh = Mesh(np.array(export_devices[::-1]), (\"b\",))\n\n>>> # Shard the arg according to what `exp` expects.\n>>> sharded_arg = jax.device_put(arg, exp.in_shardings_jax(calling_mesh)[0])\n>>> res = exp.call(sharded_arg) \n```", "```py\n```", "```py\n\n## Calling convention versions\n\nThe JAX export support has evolved over time, e.g., to support effects. In order to support compatibility (see [compatibility guarantees](#compatibility-guarantees)) we maintain a calling convention version for each `Exported`. As of June 2024, all function exported with version 9 (the latest, see [all calling convention versions](#calling-convention-versions)):\n\n```", "```py\n\nAt any given time, the export APIs may support a range of calling convention versions. You can control which calling convention version to use using the `--jax-export-calling-convention-version` flag or the `JAX_EXPORT_CALLING_CONVENTION_VERSION` environment variable:\n\n```", "```py\n\nWe reserve the right to remove support for generating or consuming calling convention versions older than 6 months.\n\n### Module calling convention\n\nThe `Exported.mlir_module` has a `main` function that takes an optional first platform index argument if the module supports multiple platforms (`len(platforms) > 1`), followed by the token arguments corresponding to the ordered effects, followed by the kept array arguments (corresponding to `module_kept_var_idx` and `in_avals`). The platform index is a i32 or i64 scalar encoding the index of the current compilation platform into the `platforms` sequence.\n\nInner functions use a different calling convention: an optional platform index argument, optional dimension variable arguments (scalar tensors of type i32 or i64), followed by optional token arguments (in presence of ordered effects), followed by the regular array arguments. The dimension arguments correspond to the dimension variables appearing in the `args_avals`, in sorted order of their names.\n\nConsider the lowering of a function with one array argument of type `f32[w, 2 * h]`, where `w` and `h` are two dimension variables. Assume that we use multi-platform lowering, and we have one ordered effect. The `main` function will be as follows:\n\n```", "```py\n\nThe actual computation is in `_wrapped_jax_export_main`, taking also the values of `h` and `w` dimension variables.\n\nThe signature of the `_wrapped_jax_export_main` is:\n\n```", "```py\n\nPrior to calling convention version 9 the calling convention for effects was different: the `main` function does not take or return a token. Instead the function creates dummy tokens of type `i1[0]` and passes them to the `_wrapped_jax_export_main`. The `_wrapped_jax_export_main` takes dummy tokens of type `i1[0]` and will create internally real tokens to pass to the inner functions. The inner functions use real tokens (both before and after calling convention version 9)\n\nAlso starting with calling convention version 9, function arguments that contain the platform index or the dimension variable values have a `jax.global_constant` string attribute whose value is the name of the global constant, either `_platform_index` or a dimension variable name. The global constant name may be empty if it is not known. Some global constant computations use inner functions, e.g., for `floor_divide`. The arguments of such functions have a `jax.global_constant` attribute for all attributes, meaning that the result of the function is also a global constant.\n\nNote that `main` contains a call to `_check_shape_assertions`. JAX tracing assumes that `arg.shape[1]` is even, and that both `w` and `h` have values >= 1\\. We must check these constraints when we invoke the module. We use a special custom call `@shape_assertion` that takes a boolean first operand, a string `error_message` attribute that may contain format specifiers `{0}`, `{1}`, â€¦, and a variadic number of integer scalar operands corresponding to the format specifiers.\n\n```"]