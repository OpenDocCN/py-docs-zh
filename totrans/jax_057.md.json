["```py\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, grad, vmap\nfrom jax import random \n```", "```py\nx = random.normal(random.key(0), (5000, 5000))\ndef f(w, b, x):\n  return jnp.tanh(jnp.dot(x, w) + b)\nfast_f = jit(f) \n```", "```py\ndef examine_jaxpr(closed_jaxpr):\n  jaxpr = closed_jaxpr.jaxpr\n  print(\"invars:\", jaxpr.invars)\n  print(\"outvars:\", jaxpr.outvars)\n  print(\"constvars:\", jaxpr.constvars)\n  for eqn in jaxpr.eqns:\n    print(\"equation:\", eqn.invars, eqn.primitive, eqn.outvars, eqn.params)\n  print()\n  print(\"jaxpr:\", jaxpr)\n\ndef foo(x):\n  return x + 1\nprint(\"foo\")\nprint(\"=====\")\nexamine_jaxpr(jax.make_jaxpr(foo)(5))\n\nprint()\n\ndef bar(w, b, x):\n  return jnp.dot(w, x) + b + jnp.ones(5), x\nprint(\"bar\")\nprint(\"=====\")\nexamine_jaxpr(jax.make_jaxpr(bar)(jnp.ones((5, 10)), jnp.ones(5), jnp.ones(10))) \n```", "```py\nfoo\n=====\ninvars: [Var(id=140117887103104):int32[]]\noutvars: [Var(id=140117887103296):int32[]]\nconstvars: []\nequation: [Var(id=140117887103104):int32[], 1] add [Var(id=140117887103296):int32[]] {}\n\njaxpr: { lambda ; a:i32[]. let b:i32[] = add a 1 in (b,) }\n\nbar\n=====\ninvars: [Var(id=140117843771968):float32[5,10], Var(id=140117843772032):float32[5], Var(id=140117843772096):float32[10]]\noutvars: [Var(id=140117843772352):float32[5], Var(id=140117843772096):float32[10]]\nconstvars: []\nequation: [Var(id=140117843771968):float32[5,10], Var(id=140117843772096):float32[10]] dot_general [Var(id=140117843772160):float32[5]] {'dimension_numbers': (((1,), (0,)), ((), ())), 'precision': None, 'preferred_element_type': dtype('float32')}\nequation: [Var(id=140117843772160):float32[5], Var(id=140117843772032):float32[5]] add [Var(id=140117843772224):float32[5]] {}\nequation: [1.0] broadcast_in_dim [Var(id=140117843772288):float32[5]] {'shape': (5,), 'broadcast_dimensions': ()}\nequation: [Var(id=140117843772224):float32[5], Var(id=140117843772288):float32[5]] add [Var(id=140117843772352):float32[5]] {}\n\njaxpr: { lambda ; a:f32[5,10] b:f32[5] c:f32[10]. let\n    d:f32[5] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a c\n    e:f32[5] = add d b\n    f:f32[5] = broadcast_in_dim[broadcast_dimensions=() shape=(5,)] 1.0\n    g:f32[5] = add e f\n  in (g, c) } \n```", "```py\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\nf_inv = inverse(f)\nassert jnp.allclose(f_inv(f(1.0)), 1.0) \n```", "```py\n# Importing Jax functions useful for tracing/interpreting.\nimport numpy as np\nfrom functools import wraps\n\nfrom jax import core\nfrom jax import lax\nfrom jax._src.util import safe_map \n```", "```py\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nclosed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\nprint(closed_jaxpr.jaxpr)\nprint(closed_jaxpr.literals) \n```", "```py\n{ lambda ; a:f32[5]. let b:f32[5] = tanh a; c:f32[5] = exp b in (c,) }\n[] \n```", "```py\ndef eval_jaxpr(jaxpr, consts, *args):\n  # Mapping from variable -> value\n  env = {}\n\n  def read(var):\n    # Literals are values baked into the Jaxpr\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # Bind args and consts to environment\n  safe_map(write, jaxpr.invars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Loop through equations and evaluate primitives using `bind`\n  for eqn in jaxpr.eqns:\n    # Read inputs to equation from environment\n    invals = safe_map(read, eqn.invars)  \n    # `bind` is how a primitive is called\n    outvals = eqn.primitive.bind(*invals, **eqn.params)\n    # Primitives may return multiple outputs or not\n    if not eqn.primitive.multiple_results: \n      outvals = [outvals]\n    # Write the results of the primitive into the environment\n    safe_map(write, eqn.outvars, outvals) \n  # Read the final result of the Jaxpr from the environment\n  return safe_map(read, jaxpr.outvars) \n```", "```py\nclosed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\neval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, jnp.ones(5)) \n```", "```py\n[Array([2.1416876, 2.1416876, 2.1416876, 2.1416876, 2.1416876], dtype=float32)] \n```", "```py\ninverse_registry = {} \n```", "```py\ninverse_registry[lax.exp_p] = jnp.log\ninverse_registry[lax.tanh_p] = jnp.arctanh \n```", "```py\ndef inverse(fun):\n  @wraps(fun)\n  def wrapped(*args, **kwargs):\n    # Since we assume unary functions, we won't worry about flattening and\n    # unflattening arguments.\n    closed_jaxpr = jax.make_jaxpr(fun)(*args, **kwargs)\n    out = inverse_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, *args)\n    return out[0]\n  return wrapped \n```", "```py\ndef inverse_jaxpr(jaxpr, consts, *args):\n  env = {}\n\n  def read(var):\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n  # Args now correspond to Jaxpr outvars\n  safe_map(write, jaxpr.outvars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Looping backward\n  for eqn in jaxpr.eqns[::-1]:\n    #  outvars are now invars \n    invals = safe_map(read, eqn.outvars)\n    if eqn.primitive not in inverse_registry:\n      raise NotImplementedError(\n          f\"{eqn.primitive} does not have registered inverse.\")\n    # Assuming a unary function \n    outval = inverse_registry[eqn.primitive](*invals)\n    safe_map(write, eqn.invars, [outval])\n  return safe_map(read, jaxpr.invars) \n```", "```py\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nf_inv = inverse(f)\nassert jnp.allclose(f_inv(f(1.0)), 1.0) \n```", "```py\njax.make_jaxpr(inverse(f))(f(1.)) \n```", "```py\n{ lambda ; a:f32[]. let b:f32[] = log a; c:f32[] = atanh b in (c,) } \n```", "```py\njit(vmap(grad(inverse(f))))((jnp.arange(5) + 1.) / 5.) \n```", "```py\nArray([-3.1440797, 15.584931 ,  2.2551253,  1.3155028,  1\\.       ],      dtype=float32, weak_type=True) \n```"]