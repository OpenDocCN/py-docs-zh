["```py\ninit_fun(params)\n\nArgs:\n  params: pytree representing the initial parameters.\n\nReturns:\n  A pytree representing the initial optimizer state, which includes the\n  initial parameters and may also include auxiliary values like initial\n  momentum. The optimizer state pytree structure generally differs from that\n  of `params`. \n```", "```py\nupdate_fun(step, grads, opt_state)\n\nArgs:\n  step: integer representing the step index.\n  grads: a pytree with the same structure as `get_params(opt_state)`\n    representing the gradients to be used in updating the optimizer state.\n  opt_state: a pytree representing the optimizer state to be updated.\n\nReturns:\n  A pytree with the same structure as the `opt_state` argument representing\n  the updated optimizer state. \n```", "```py\nget_params(opt_state)\n\nArgs:\n  opt_state: pytree representing an optimizer state.\n\nReturns:\n  A pytree representing the parameters extracted from `opt_state`, such that\n  the invariant `params == get_params(init_fun(params))` holds true. \n```", "```py\nopt_init, opt_update, get_params = optimizers.sgd(learning_rate)\nopt_state = opt_init(params)\n\ndef step(step, opt_state):\n  value, grads = jax.value_and_grad(loss_fn)(get_params(opt_state))\n  opt_state = opt_update(step, grads, opt_state)\n  return value, opt_state\n\nfor i in range(num_steps):\n  value, opt_state = step(i, opt_state) \n```", "```py\nclass jax.example_libraries.optimizers.JoinPoint(subtree)\n```", "```py\nclass jax.example_libraries.optimizers.Optimizer(init_fn, update_fn, params_fn)\n```", "```py\ninit_fn: Callable[[Any], OptimizerState]\n```", "```py\nparams_fn: Callable[[OptimizerState], Any]\n```", "```py\nupdate_fn: Callable[[int, Any, OptimizerState], OptimizerState]\n```", "```py\nclass jax.example_libraries.optimizers.OptimizerState(packed_state, tree_def, subtree_defs)\n```", "```py\npacked_state\n```", "```py\nsubtree_defs\n```", "```py\ntree_def\n```", "```py\njax.example_libraries.optimizers.adagrad(step_size, momentum=0.9)\n```", "```py\njax.example_libraries.optimizers.adam(step_size, b1=0.9, b2=0.999, eps=1e-08)\n```", "```py\njax.example_libraries.optimizers.adamax(step_size, b1=0.9, b2=0.999, eps=1e-08)\n```", "```py\njax.example_libraries.optimizers.clip_grads(grad_tree, max_norm)\n```", "```py\njax.example_libraries.optimizers.constant(step_size)\n```", "```py\njax.example_libraries.optimizers.exponential_decay(step_size, decay_steps, decay_rate)\n```", "```py\njax.example_libraries.optimizers.inverse_time_decay(step_size, decay_steps, decay_rate, staircase=False)\n```", "```py\njax.example_libraries.optimizers.l2_norm(tree)\n```", "```py\njax.example_libraries.optimizers.make_schedule(scalar_or_schedule)\n```", "```py\njax.example_libraries.optimizers.momentum(step_size, mass)\n```", "```py\njax.example_libraries.optimizers.nesterov(step_size, mass)\n```", "```py\njax.example_libraries.optimizers.optimizer(opt_maker)\n```", "```py\ninit_fun  ::  ndarray  ->  OptStatePytree  ndarray\nupdate_fun  ::  OptStatePytree  ndarray  ->  OptStatePytree  ndarray\nget_params  ::  OptStatePytree  ndarray  ->  ndarray \n```", "```py\ninit_fun  ::  ParameterPytree  ndarray  ->  OptimizerState\nupdate_fun  ::  OptimizerState  ->  OptimizerState\nget_params  ::  OptimizerState  ->  ParameterPytree  ndarray \n```", "```py\njax.example_libraries.optimizers.pack_optimizer_state(marked_pytree)\n```", "```py\njax.example_libraries.optimizers.piecewise_constant(boundaries, values)\n```", "```py\njax.example_libraries.optimizers.polynomial_decay(step_size, decay_steps, final_step_size, power=1.0)\n```", "```py\njax.example_libraries.optimizers.rmsprop(step_size, gamma=0.9, eps=1e-08)\n```", "```py\njax.example_libraries.optimizers.rmsprop_momentum(step_size, gamma=0.9, eps=1e-08, momentum=0.9)\n```", "```py\njax.example_libraries.optimizers.sgd(step_size)\n```", "```py\njax.example_libraries.optimizers.sm3(step_size, momentum=0.9)\n```", "```py\njax.example_libraries.optimizers.unpack_optimizer_state(opt_state)\n```"]