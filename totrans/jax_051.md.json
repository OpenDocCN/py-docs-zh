["```py\nimport jax\nimport tensorflow as tf\nimport numpy as np\n\n################################################################################\n# Step 1: setup the Dataset for pure data parallelism (do once)\n################################################################################\n# Fake example data (replace with your Dataset)\nds = tf.data.Dataset.from_tensor_slices(\n    [np.ones((16, 3)) * i for i in range(100)])\n\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n\n################################################################################\n# Step 2: create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step). This can be used with batches\n# produced by different data loaders as well!\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\n\nper_process_batch_size = per_process_batch.shape[0]  # adjust if your batch dim\n                                                     # isn't 0\n\nper_replica_batch_size = per_process_batch_size // jax.local_device_count()\nassert per_process_batch_size % per_replica_batch_size == 0, \\\n  \"This example doesn't implement padding.\"\nper_replica_batches = np.split(per_process_batch, jax.local_device_count())\n\n# Thanks to the very important trick about data parallelism, no need to care what\n# order the devices appear in the sharding.\nsharding = jax.sharding.PositionalSharding(jax.devices())\n# PositionalSharding must have same rank as data being sharded.\nsharding = sharding.reshape((jax.device_count(),) +\n                            (1,) * (per_process_batch.ndim - 1))\n\nglobal_batch_size = per_replica_batch_size * jax.device_count()\nglobal_batch_shape = ((global_batch_size,) + per_process_batch.shape[1:])\n\nglobal_batch_array = jax.make_array_from_single_device_arrays(\n    global_batch_shape, sharding,\n    # Thanks again to the very important trick, no need to care which device gets\n    # which per-replica batch.\n    arrays=[jax.device_put(batch, device)\n            for batch, device \n            in zip(per_replica_batches, sharding.addressable_devices)])\n\nassert global_batch_array.shape == global_batch_shape\nassert (global_batch_array.addressable_shards[0].data.shape ==\n        per_replica_batches[0].shape) \n```", "```py\nimport jax\nimport tensorflow as tf\nimport numpy as np\n\n################################################################################\n# Step 1: Set up the Dataset with a different data shard per-process (do once)\n#         (same as for pure data parallelism)\n################################################################################\n# Fake example data (replace with your Dataset)\nper_process_batches = [np.ones((16, 3)) * i for i in range(100)]\nds = tf.data.Dataset.from_tensor_slices(per_process_batches)\n\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n\n################################################################################\n# Step 2: Create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step)\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\n\nnum_model_replicas_per_process = 2 # set according to your parallelism strategy\nnum_model_replicas_total = num_model_replicas_per_process * jax.process_count()\n\nper_process_batch_size = per_process_batch.shape[0]  # adjust if your batch dim\n                                                     # isn't 0\n\nper_replica_batch_size = (per_process_batch_size //\n                          num_model_replicas_per_process)\nassert per_process_batch_size % per_replica_batch_size == 0, \\\n  \"This example doesn't implement padding.\"\nper_replica_batches = np.split(per_process_batch,\n                               num_model_replicas_per_process)\n\n# Create an example `Mesh` for per-process data parallelism. Make sure all devices\n# are grouped by process, and then resize so each row is a model replica.\nmesh_devices = np.array([jax.local_devices(process_idx)\n                         for process_idx in range(jax.process_count())])\nmesh_devices = mesh_devices.reshape(num_model_replicas_total, -1)\n# Double check that each replica's devices are on a single process.\nfor replica_devices in mesh_devices:\n  num_processes = len(set(d.process_index for d in replica_devices))\n  assert num_processes == 1\nmesh = jax.sharding.Mesh(mesh_devices, [\"model_replicas\", \"data_parallelism\"])\n\n# Shard the data across model replicas. You don't shard across the\n# data_parallelism mesh axis, meaning each per-replica shard will be replicated\n# across that axis.\nsharding = jax.sharding.NamedSharding(\n    mesh, jax.sharding.PartitionSpec(\"model_replicas\"))\n\nglobal_batch_size = per_replica_batch_size * num_model_replicas_total\nglobal_batch_shape = ((global_batch_size,) + per_process_batch.shape[1:])\n\n# Create the final jax.Array using jax.make_array_from_callback. The callback\n# will be called for each local device, and passed the N-D numpy-style index\n# that describes what shard of the global data that device should receive.\n#\n# You don't need care exactly which index is passed in due to the very important data\n# parallelism, but you do use the index argument to make sure you replicate each\n# per-replica batch correctly -- the `index` argument will be the same for\n# devices in the same model replica, and different for devices in different\n# model replicas.\n\nindex_to_batch  = {}\ndef callback(index: tuple[slice, ...]) -> np.ndarray:\n  # Python `slice` objects aren't hashable, so manually create dict key.\n  index_key = tuple((slice_.start, slice_.stop) for slice_ in index)\n  if index_key not in index_to_batch:\n    # You don't care which per-replica batch goes to which replica, just take the\n    # next unused one.\n    index_to_batch[index_key] = per_replica_batches[len(index_to_batch)]\n  return index_to_batch[index_key]\n\nglobal_batch_array = jax.make_array_from_callback(\n    global_batch_shape, sharding, callback)\n\nassert global_batch_array.shape == global_batch_shape\nassert (global_batch_array.addressable_shards[0].data.shape ==\n        per_replica_batches[0].shape) \n```"]