["```py\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8' # Use 8 CPU devices \n```", "```py\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\n\nfrom jax.sharding import Mesh, PartitionSpec as P\nfrom jax.experimental import mesh_utils\nfrom jax.experimental.shard_map import shard_map \n```", "```py\ndevices = mesh_utils.create_device_mesh((4, 2))\nmesh = Mesh(devices, axis_names=('x', 'y'))\n\na = jnp.arange( 8 * 16.).reshape(8, 16)\nb = jnp.arange(16 *  4.).reshape(16, 4)\n\n@partial(shard_map, mesh=mesh, in_specs=(P('x', 'y'), P('y', None)),\n         out_specs=P('x', None))\ndef matmul_basic(a_block, b_block):\n  # a_block: f32[2, 8]\n  # b_block: f32[8, 4]\n  c_partialsum = jnp.dot(a_block, b_block)\n  c_block = jax.lax.psum(c_partialsum, 'y')\n  # c_block: f32[2, 4]\n  return c_block\n\nc = matmul_basic(a, b)   # c: f32[8, 4] \n```", "```py\nfrom jax.tree_util import tree_map, tree_all\n\ndef allclose(a, b):\n  return tree_all(tree_map(partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))\n\nallclose(c, jnp.dot(a, b)) \n```", "```py\nTrue \n```", "```py\njax.debug.visualize_array_sharding(c) \n```", "```py\n\n CPU 0,1 \n\n CPU 2,3 \n\n CPU 4,5 \n\n CPU 6,7 \n\n```", "```py\nfrom jax.sharding import NamedSharding\n\na = jax.device_put(a, NamedSharding(mesh, P('x', 'y')))\nb = jax.device_put(b, NamedSharding(mesh, P('y', None)))\n\n@jax.jit\ndef matmul_reference(a, b):\n  c = jnp.dot(a, b)\n  return jax.lax.with_sharding_constraint(c, NamedSharding(mesh, P('x', None)))\n\nc_ref = matmul_reference(a, b)\nallclose(c_ref, jnp.dot(a, b)) \n```", "```py\nTrue \n```", "```py\nprint('a blocks:'); jax.debug.visualize_array_sharding(a)\nprint('b blocks:'); jax.debug.visualize_array_sharding(b)\nprint('c blocks:'); jax.debug.visualize_array_sharding(c) \n```", "```py\na blocks:\nb blocks:\nc blocks: \n```", "```py\n\n CPU 0CPU 1 \n\n CPU 2CPU 3 \n\n CPU 4CPU 5 \n\n CPU 6CPU 7 \n\n```", "```py\n\nCPU 0,2,4,6\n\nCPU 1,3,5,7\n\n```", "```py\n\n CPU 0,1 \n\n CPU 2,3 \n\n CPU 4,5 \n\n CPU 6,7 \n\n```", "```py\ndef check_vmap(f, xs):\n  ans = jax.vmap(f, in_axes=(0,), out_axes=0)(xs)\n  expected = jnp.stack([f(x) for x in xs])  # vmap reference semantics\n  print(allclose(ans, expected))\n\ncheck_vmap(lambda x: x @ x, jnp.arange(12).reshape(4, 3)) \n```", "```py\nTrue \n```", "```py\nimport numpy as np\ndevices = np.array(jax.devices()[:4])\nmesh = Mesh(devices, ('i',))  # mesh.shape['i'] = 4\n\ndef check_shmap(f, y):\n  ans = shard_map(f, mesh, in_specs=P('i'), out_specs=P('i'))(y)\n  expected = jnp.concatenate([f(y_blk) for y_blk in jnp.split(y, mesh.shape['i'])])\n  print(allclose(ans, expected))\n\ncheck_shmap(lambda x: x.T @ x, jnp.arange(32).reshape(8, 4)) \n```", "```py\nTrue \n```", "```py\ndevices = mesh_utils.create_device_mesh((4, 2))\nmesh = Mesh(devices, ('i', 'j'))\n\n@partial(shard_map, mesh=mesh, in_specs=P('i', None), out_specs=P('i', 'j'))\ndef f1(x_block):\n  print(x_block.shape)  # prints (3, 12)\n  return x_block\n\nx1 = jnp.arange(12 * 12).reshape(12, 12)\ny = f1(x1) \n```", "```py\n(3, 12) \n```", "```py\n@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P('i', 'j'))\ndef f2(x_block):\n  print(x_block.shape)\n  return x_block\n\nx = jnp.arange(12 * 12).reshape(12, 12)\nx_ = jnp.tile(x, (1, mesh.shape['j']))  # x_ has shape (12, 24)\ny = f2(x_)  # prints (3,12), and f1(x) == f2(x_) \n```", "```py\n(3, 12) \n```", "```py\nx = jnp.array([[3.]])\n\nz = shard_map(lambda: x, mesh=mesh, in_specs=(), out_specs=P('i', 'j'))()\nprint(z)  # prints the same as jnp.tile(x, (4, 2))\n\nz = shard_map(lambda: x, mesh=mesh, in_specs=(), out_specs=P('i', None))()\nprint(z)  # prints the same as jnp.tile(x, (4, 1)), or just jnp.tile(x, (4,))\n\nz = shard_map(lambda: x, mesh=mesh, in_specs=(), out_specs=P(None, None))()\nprint(z)  # prints the same as jnp.tile(x, (1, 1)), or just x \n```", "```py\n[[3\\. 3.]\n [3\\. 3.]\n [3\\. 3.]\n [3\\. 3.]]\n[[3.]\n [3.]\n [3.]\n [3.]]\n[[3.]] \n```", "```py\n@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P('i', None))\ndef f3(x_block):\n  return jax.lax.psum(x_block, 'j')\n\nx = jnp.arange(12 * 12).reshape(12, 12)\ny3 = f3(x)\nprint(y3.shape) \n```", "```py\n(12, 6) \n```", "```py\n@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P(None, 'j'))\ndef f4(x_block):\n  return jax.lax.psum(x_block, 'i')\n\nx = jnp.arange(12 * 12).reshape(12, 12)\ny4 = f4(x)\nprint(y4.shape)  # (3,12)\n\n@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P(None, None))\ndef f5(x_block):\n  return jax.lax.psum(x_block, ('i', 'j'))\n\ny5 = f5(x)\nprint(y5.shape)  # (3,6) \n```", "```py\n(3, 12)\n(3, 6) \n```", "```py\nfrom jax.sharding import Mesh\nSpecs = PyTree[PartitionSpec]\n\ndef shard_map(\n    f: Callable, mesh: Mesh, in_specs: Specs, out_specs: Specs,\n    auto: collections.abc.Set[AxisName] = frozenset([]),\n    check_rep: bool = True,\n) -> Callable:\n  ... \n```", "```py\nmesh = Mesh(jax.devices(), ('i',))\nx = jnp.arange(16.)\nf_shmapped = shard_map(f, mesh, in_specs=P('i'), out_specs=P('i'))\ny = f_shmapped(x) \n```", "```py\ndef f_shmapped_ref(x):\n  x_blocks = jnp.array_split(x, mesh.shape[0])\n  y_blocks = [f(x_blk) for x_blk in x_blocks]\n  return jnp.concatenate(y_blocks) \n```", "```py\ndef f(x_blk):\n  z_blk = f_part1(x_blk)\n  u_blk = collective(z_blk, axis_name)\n  v_blk = f_part2(x_blk, z_blk, u_blk)\n  return v_blk \n```", "```py\ndef f_shmapped_ref(x):\n  x_blocks = jnp.array_split(x, mesh.shape[0])\n  z_blocks = [f_part1(x_blk) for x_blk in x_blocks]\n  u_blocks = [collective_ref(i, z_blocks) for i in range(len(z_blocks))]\n  v_blocks = [f_part2(x_blk, z_blk, u_blk) for x_blk, z_blk, u_blk\n              in zip(x_blocks, z_blocks, u_blocks)]\n  return jnp.concatenate(v_blocks) \n```", "```py\nimport jax\nimport jax.numpy as jnp\nfrom jax import lax\n\nfrom jax.sharding import Mesh, NamedSharding, PartitionSpec as P\nfrom jax.experimental.shard_map import shard_map \n```", "```py\nmesh1d = Mesh(jax.devices()[:4], ('i',))\n\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P(None))\ndef f1(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.psum(x_block, 'i')\n  print('AFTER:\\n', y_block)\n  return y_block \n```", "```py\nx = jnp.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 1, 2])\ny = f1(x)\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3 1 4 1]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[5 9 2 6]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[5 3 5 8]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[9 7 1 2]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[22 20 12 17]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[22 20 12 17]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[22 20 12 17]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[22 20 12 17]\n\nFINAL RESULT:\n [22 20 12 17] \n```", "```py\ndef psum_ref(_, x_blocks):\n  tot = sum(x_blocks)\n  return [tot] * len(x_blocks) \n```", "```py\nmesh2d = Mesh(np.array(jax.devices()[:4]).reshape(2, 2), ('i', 'j'))\n\n@partial(shard_map, mesh=mesh2d, in_specs=P('i', 'j'), out_specs=P(None, 'j'))\ndef f2(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.psum(x_block, 'i')\n  print('AFTER:\\n', y_block)\n  return y_block\n\ny = f2(jnp.arange(16).reshape(4, 4))\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i, j,) = (0, 0):\n[[0 1]\n [4 5]]\n\nOn TFRT_CPU_1 at mesh coordinates (i, j,) = (0, 1):\n[[2 3]\n [6 7]]\n\nOn TFRT_CPU_2 at mesh coordinates (i, j,) = (1, 0):\n[[ 8  9]\n [12 13]]\n\nOn TFRT_CPU_3 at mesh coordinates (i, j,) = (1, 1):\n[[10 11]\n [14 15]]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i, j,) = (0, 0):\n[[ 8 10]\n [16 18]]\n\nOn TFRT_CPU_1 at mesh coordinates (i, j,) = (0, 1):\n[[12 14]\n [20 22]]\n\nOn TFRT_CPU_2 at mesh coordinates (i, j,) = (1, 0):\n[[ 8 10]\n [16 18]]\n\nOn TFRT_CPU_3 at mesh coordinates (i, j,) = (1, 1):\n[[12 14]\n [20 22]]\n\nFINAL RESULT:\n [[ 8 10 12 14]\n [16 18 20 22]] \n```", "```py\n@partial(shard_map, mesh=mesh2d, in_specs=P('i', 'j'), out_specs=P(None, None))\ndef f3(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.psum(x_block, ('i', 'j'))\n  print('AFTER:\\n', y_block)\n  return y_block\n\ny = f3(jnp.arange(16).reshape(4, 4))\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i, j,) = (0, 0):\n[[0 1]\n [4 5]]\n\nOn TFRT_CPU_1 at mesh coordinates (i, j,) = (0, 1):\n[[2 3]\n [6 7]]\n\nOn TFRT_CPU_2 at mesh coordinates (i, j,) = (1, 0):\n[[ 8  9]\n [12 13]]\n\nOn TFRT_CPU_3 at mesh coordinates (i, j,) = (1, 1):\n[[10 11]\n [14 15]]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i, j,) = (0, 0):\n[[20 24]\n [36 40]]\n\nOn TFRT_CPU_1 at mesh coordinates (i, j,) = (0, 1):\n[[20 24]\n [36 40]]\n\nOn TFRT_CPU_2 at mesh coordinates (i, j,) = (1, 0):\n[[20 24]\n [36 40]]\n\nOn TFRT_CPU_3 at mesh coordinates (i, j,) = (1, 1):\n[[20 24]\n [36 40]]\n\nFINAL RESULT:\n [[20 24]\n [36 40]] \n```", "```py\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P('i'))\ndef f4(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.all_gather(x_block, 'i', tiled=True)\n  print('AFTER:\\n', y_block)\n  return y_block\n\nx = jnp.array([3, 9, 5, 2])\ny = f4(x)\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[9]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[5]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[2]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3 9 5 2]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[3 9 5 2]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[3 9 5 2]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[3 9 5 2]\n\nFINAL RESULT:\n [3 9 5 2 3 9 5 2 3 9 5 2 3 9 5 2] \n```", "```py\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P('i'))\ndef f5(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.all_gather(x_block, 'i', tiled=False)\n  print('AFTER:\\n', y_block)\n  return y_block\n\ny = f5(x)\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[9]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[5]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[2]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[[3]\n [9]\n [5]\n [2]]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[[3]\n [9]\n [5]\n [2]]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[[3]\n [9]\n [5]\n [2]]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[[3]\n [9]\n [5]\n [2]]\n\nFINAL RESULT:\n [[3]\n [9]\n [5]\n [2]\n [3]\n [9]\n [5]\n [2]\n [3]\n [9]\n [5]\n [2]\n [3]\n [9]\n [5]\n [2]] \n```", "```py\ndef all_gather_ref(_, x_blocks, *, tiled=False):\n  combine = jnp.concatenate if tiled else jnp.stack\n  return [combine(x_blocks)] * len(x_blocks) \n```", "```py\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P('i'))\ndef f6(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.psum_scatter(x_block, 'i', tiled=True)\n  print('AFTER:\\n', y_block)\n  return y_block\n\nx = jnp.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 1, 2])\ny = f6(x)\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3 1 4 1]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[5 9 2 6]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[5 3 5 8]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[9 7 1 2]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[22]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[20]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[12]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[17]\n\nFINAL RESULT:\n [22 20 12 17] \n```", "```py\ndef psum_scatter_ref(i, x_blocks, *, tiled=False):\n  axis_size = len(x_blocks)\n  tot = sum(x_blocks)\n  if tiled:\n    tot = tot.reshape(axis_size, -1, *tot.shape[1:])  # split leading axis\n  return [tot[i] for i in range(tot.shape[0])] \n```", "```py\ndef psum(x, axis_name):\n  summed_chunk = jax.lax.psum_scatter(x, axis_name)\n  return jax.lax.all_gather(summed_chunk, axis_name) \n```", "```py\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P('i'))\ndef f7(x_block):\n  sz = jax.lax.psum(1, 'i')\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.ppermute(x_block, 'i', [(i, (i + 1) % sz) for i in range(sz)])\n  print('AFTER:\\n', y_block)\n  return y_block\n\ny = f7(jnp.arange(8))\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[0 1]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[2 3]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[4 5]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[6 7]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[6 7]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[0 1]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[2 3]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[4 5]\n\nFINAL RESULT:\n [6 7 0 1 2 3 4 5] \n```", "```py\ndef ppermute_ref(i, x_blocks, perm):\n  results = [jnp.zeros_like(x_blocks[0])] * len(x_blocks)\n  for src, dst in perm:\n    results[dst] = x_blocks[src]\n  return results \n```", "```py\ndef psum_scatter(x, axis_name, *, tiled=False):\n  size = jax.lax.psum(1, axis_name)\n  idx = jax.lax.axis_index(axis_name)  # function instance index along axis_name\n  if tiled:\n    x = x.reshape(size, -1, *x.shape[1:])  # split leading axis\n  shift = partial(jax.lax.ppermute, axis_name=axis_name,\n                  perm=[(i, (i - 1) % size) for i in range(size)])\n  for i in range(1, size):\n    update = shift(x[(idx + i) % size])\n    x = x.at[(idx + i + 1) % size].add(update)\n  return x[idx] \n```", "```py\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P('i'))\ndef f8(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = psum_scatter(x_block, 'i', tiled=True)\n  print('AFTER:\\n', y_block)\n  return y_block\n\nx = jnp.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 1, 2])\ny = f8(x)\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3 1 4 1]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[5 9 2 6]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[5 3 5 8]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[9 7 1 2]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[22]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[20]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[12]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[17]\n\nFINAL RESULT:\n [22 20 12 17] \n```", "```py\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P('i'))\ndef f9(x_block):\n  print('BEFORE:\\n', x_block)\n  y_block = jax.lax.all_to_all(x_block, 'i', split_axis=0, concat_axis=0,\n                               tiled=True)\n  print('AFTER:\\n', y_block)\n  return y_block\n\nx = jnp.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 1, 2])\ny = f9(x)\nprint('FINAL RESULT:\\n', y) \n```", "```py\nBEFORE:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3 1 4 1]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[5 9 2 6]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[5 3 5 8]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[9 7 1 2]\n\nAFTER:\n On TFRT_CPU_0 at mesh coordinates (i,) = (0,):\n[3 5 5 9]\n\nOn TFRT_CPU_1 at mesh coordinates (i,) = (1,):\n[1 9 3 7]\n\nOn TFRT_CPU_2 at mesh coordinates (i,) = (2,):\n[4 2 5 1]\n\nOn TFRT_CPU_3 at mesh coordinates (i,) = (3,):\n[1 6 8 2]\n\nFINAL RESULT:\n [3 5 5 9 1 9 3 7 4 2 5 1 1 6 8 2] \n```", "```py\ndef all_to_all_ref(_, x_blocks, *, tiled=False):\n  axis_size = len(x_blocks)\n  if tiled:\n    splits = [jnp.array_split(x, axis_size) for x in x_blocks]\n    return [jnp.concatenate(s) for s in zip(*splits)]\n  else:\n    splits = [list(x) for x in x_blocks]\n    return [jnp.stack(s) for s in zip(*splits)] \n```", "```py\nimport jax\nimport jax.numpy as jnp\n\nfrom jax.sharding import Mesh, NamedSharding, PartitionSpec as P\nfrom jax.experimental.shard_map import shard_map \n```", "```py\nmesh = Mesh(jax.devices()[:4], ('i',))\n\ndef device_put(x, pspec):\n  return jax.device_put(x, NamedSharding(mesh, pspec)) \n```", "```py\nlhs_spec = P('i', None)\nlhs = device_put(jax.random.normal(jax.random.key(0), (8, 8)), lhs_spec) \n```", "```py\nrhs_spec = P('i', None)\nrhs = device_put(jax.random.normal(jax.random.key(1), (8, 4)), rhs_spec) \n```", "```py\n@jax.jit\n@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec),\n         out_specs=rhs_spec)\ndef matmul_allgather(lhs_block, rhs_block):\n  rhs = jax.lax.all_gather(rhs_block, 'i', tiled=True)\n  return lhs_block @ rhs \n```", "```py\nout = matmul_allgather(lhs, rhs)\nprint(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3)) \n```", "```py\nTrue \n```", "```py\n@jax.jit\n@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec),\n         out_specs=rhs_spec)\ndef matmul_allgather_overlapped(lhs_block, rhs_block):\n  size = jax.lax.psum(1, 'i')\n  idx = jax.lax.axis_index('i')\n  shift = partial(jax.lax.ppermute, axis_name='i',\n                  perm=[(i, (i + 1) % size) for i in range(size)])\n\n  B = lhs_block.shape[1] // size\n  lhs_blocks = lambda i: lax.dynamic_slice_in_dim(lhs_block, i * B, B, 1)\n\n  out_block = lhs_blocks(idx) @ rhs_block\n  for i in range(1, size):\n    rhs_block = shift(rhs_block)\n    out_block += lhs_blocks((idx - i) % size) @ rhs_block\n  return out_block \n```", "```py\nout = matmul_allgather_overlapped(lhs, rhs)\nprint(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3)) \n```", "```py\nTrue \n```", "```py\n@jax.jit\n@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec),\n         out_specs=rhs_spec)\ndef matmul_allgather_overlapped_bidi(lhs_block, rhs_block):\n  size = jax.lax.psum(1, 'i')\n  idx = jax.lax.axis_index('i')\n  shift_up = partial(jax.lax.ppermute, axis_name='i',\n                     perm=[(i, (i + 1) % size) for i in range(size)])\n  shift_dn = partial(jax.lax.ppermute, axis_name='i',\n                     perm=[(i, (i - 1) % size) for i in range(size)])\n\n  B = lhs_block.shape[1] // size // 2  # half-size blocks\n  lhs_blocks = lambda i, hi: lax.dynamic_slice_in_dim(lhs_block, (2*i+hi) * B, B, 1)\n\n  rhs_block_lo, rhs_block_hi = jnp.split(rhs_block, 2, axis=0)\n  out_block  = lhs_blocks(idx, 0) @ rhs_block_lo\n  out_block += lhs_blocks(idx, 1) @ rhs_block_hi\n  for i in range(1, size):\n    rhs_block_lo = shift_up(rhs_block_lo)\n    rhs_block_hi = shift_dn(rhs_block_hi)\n    out_block += lhs_blocks((idx - i) % size, 0) @ rhs_block_lo\n    out_block += lhs_blocks((idx + i) % size, 1) @ rhs_block_hi\n  return out_block \n```", "```py\nout = matmul_allgather_overlapped_bidi(lhs, rhs)\nprint(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3)) \n```", "```py\nTrue \n```", "```py\nlhs_spec = P(None, 'i')\nlhs = device_put(lhs, lhs_spec)\n\nrhs_spec = P('i', None)\nrhs = device_put(rhs, rhs_spec) \n```", "```py\n@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec),\n         out_specs=rhs_spec)\ndef matmul_psumscatter(lhs_block, rhs_block):\n  out_summand = lhs_block @ rhs_block\n  return jax.lax.psum_scatter(out_summand, 'i', tiled=True)\n\nout = matmul_psumscatter(lhs, rhs)\nprint(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3)) \n```", "```py\nTrue \n```", "```py\n@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec),\n         out_specs=rhs_spec)\ndef matmul_psumscatter_overlapped(lhs_block, rhs_block):\n  size = jax.lax.psum(1, 'i')\n  idx = jax.lax.axis_index('i')\n  shift = partial(jax.lax.ppermute, axis_name='i',\n                  perm=[(i, (i - 1) % size) for i in range(size)])\n  lhs_block = lhs_block.reshape(size, -1, lhs_block.shape[1])  # split 1st axis\n\n  out_summand = lhs_block[(idx + 1) % size] @ rhs_block\n  for i in range(1, size):\n    out_summand = shift(out_summand)\n    out_summand += lhs_block[(idx + i + 1) % size] @ rhs_block\n  return out_summand \n```", "```py\nout = matmul_psumscatter_overlapped(lhs, rhs)\nprint(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3)) \n```", "```py\nTrue \n```", "```py\n@partial(shard_map, mesh=mesh, in_specs=(lhs_spec, rhs_spec),\n         out_specs=rhs_spec)\ndef matmul_psumscatter_overlapped_bidi(lhs_block, rhs_block):\n  size = jax.lax.psum(1, 'i')\n  idx = jax.lax.axis_index('i')\n  shift_up = partial(jax.lax.ppermute, axis_name='i',\n                     perm=[(i, (i + 1) % size) for i in range(size)])\n  shift_dn = partial(jax.lax.ppermute, axis_name='i',\n                     perm=[(i, (i - 1) % size) for i in range(size)])\n\n  B = lhs_block.shape[0] // size // 2  # half-size blocks\n  lhs_blocks = lambda i, hi: lax.dynamic_slice_in_dim(lhs_block, (2*i+hi) * B, B, 0)\n\n  out_summand_lo = lhs_blocks((idx - 1) % size, 0) @ rhs_block\n  out_summand_hi = lhs_blocks((idx + 1) % size, 1) @ rhs_block\n  for i in range(1, size):\n    out_summand_lo = shift_up(out_summand_lo)\n    out_summand_hi = shift_dn(out_summand_hi)\n    out_summand_lo += lhs_blocks((idx - i - 1) % size, 0) @ rhs_block\n    out_summand_hi += lhs_blocks((idx + i + 1) % size, 1) @ rhs_block\n  return jnp.concatenate([out_summand_lo, out_summand_hi]) \n```", "```py\nout = matmul_psumscatter_overlapped_bidi(lhs, rhs)\nprint(jnp.allclose(out, lhs @ rhs, atol=1e-3, rtol=1e-3)) \n```", "```py\nTrue \n```", "```py\nimport jax\nimport jax.numpy as jnp\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jax.nn.relu(outputs)\n  return outputs\n\ndef loss(params, batch):\n  inputs, targets = batch\n  predictions = predict(params, inputs)\n  return jnp.mean(jnp.sum((predictions - targets)**2, axis=-1)) \n```", "```py\ndef init_layer(key, n_in, n_out):\n    k1, k2 = jax.random.split(key)\n    W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)\n    b = jax.random.normal(k2, (n_out,))\n    return W, b\n\ndef init(key, layer_sizes, batch_size):\n    key, *keys = jax.random.split(key, len(layer_sizes))\n    params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n\n    key, *keys = jax.random.split(key, 3)\n    inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n    targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))\n\n    return params, (inputs, targets) \n```", "```py\nlayer_sizes = [784, 128, 128, 128, 128, 128, 8]\nbatch_size = 32\n\nparams, batch = init(jax.random.PRNGKey(0), layer_sizes, batch_size) \n```", "```py\nfrom functools import partial\n\nfrom jax.sharding import NamedSharding, Mesh, PartitionSpec as P\nfrom jax.experimental.shard_map import shard_map\nfrom jax.experimental import mesh_utils\n\ndevices = mesh_utils.create_device_mesh((8,))\n\n# replicate initial params on all devices, shard data batch over devices\nmesh = Mesh(devices, ('batch',))\nbatch = jax.device_put(batch, NamedSharding(mesh, P('batch')))\nparams = jax.device_put(params, NamedSharding(mesh, P()))\n\n# adapt the loss function to sum the losses across devices\ndef loss_dp(params, batch):\n  @partial(shard_map, mesh=mesh, in_specs=P('batch', None), out_specs=P())\n  def loss_spmd(local_batch):\n    inputs, targets = local_batch\n    predictions = predict(params, inputs)  # use reference 'predict`\n    local_loss = jnp.mean(jnp.sum((predictions - targets)**2, axis=-1))\n    return jax.lax.pmean(local_loss, 'batch')\n  return loss_spmd(batch) \n```", "```py\nprint(jax.jit(loss)(params, batch))\nprint(jax.jit(loss_dp)(params, batch)) \n```", "```py\n22.779888\n22.779888 \n```", "```py\ndef allclose(a, b):\n  return tree_all(tree_map(partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))\n\nprint(allclose(jax.jit(jax.grad(loss))(params, batch),\n               jax.jit(jax.grad(loss_dp))(params, batch))) \n```", "```py\nTrue \n```", "```py\n# shard data batch *and params* over devices\nmesh = Mesh(devices, ('batch',))\nbatch = jax.device_put(batch, NamedSharding(mesh, P('batch')))\nparams = jax.device_put(params, NamedSharding(mesh, P('batch')))\n\n# adapt the prediction function to gather weights just before their use,\n# and to re-gather them on the backward pass (rather than saving them)\n@partial(jax.remat, policy=lambda op, *_, **__: str(op) != 'all_gather')\ndef predict_fsdp(params_frag, inputs):\n  for W_frag, b_frag in params_frag:\n    W = jax.lax.all_gather(W_frag, 'batch', tiled=True)\n    b = jax.lax.all_gather(b_frag, 'batch', tiled=True)\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jax.nn.relu(outputs)\n  return outputs\n\ndef loss_fsdp(params, batch):\n  @partial(shard_map, mesh=mesh, in_specs=P('batch'), out_specs=P())\n  def loss_spmd(local_params, local_batch):\n    inputs, targets = local_batch\n    predictions = predict_fsdp(local_params, inputs)\n    local_loss = jnp.mean(jnp.sum((predictions - targets)**2, axis=-1))\n    return jax.lax.pmean(local_loss, 'batch')\n  return loss_spmd(params, batch) \n```", "```py\nprint(jax.jit(loss)(params, batch))\nprint(jax.jit(loss_fsdp)(params, batch))\n\nprint(allclose(jax.jit(jax.grad(loss))(params, batch),\n               jax.jit(jax.grad(loss_fsdp))(params, batch))) \n```", "```py\n22.779888\n22.779888\nTrue \n```", "```py\ndevices = mesh_utils.create_device_mesh((8,))\nmesh = Mesh(devices, ('feats',))\n\nbatch = jax.device_put(batch, NamedSharding(mesh, P(None, 'feats')))\nparams = jax.device_put(params, NamedSharding(mesh, P('feats')))\n\ndef predict_tp(params, inputs):\n  for W, b in params:\n    outputs = gemm_tp(inputs, W, b)\n    inputs = jax.nn.relu(outputs)\n  return outputs\n\n@partial(shard_map, mesh=mesh,\n         in_specs=(P(None, 'feats'), P('feats', None), P('feats')),\n         out_specs=P(None, 'feats'))\ndef gemm_tp(inputs, W, b):\n  block_result = jnp.dot(inputs, W)\n  return jax.lax.psum_scatter(block_result, 'feats',\n                              scatter_dimension=1, tiled=True) + b\n\ndef loss_tp(params, batch):\n  inputs, targets = batch\n  predictions = predict_tp(params, inputs)\n  return jnp.mean(jnp.sum((predictions - targets) ** 2, axis=-1))  # NOTE psum! \n```", "```py\ndevices = mesh_utils.create_device_mesh((4, 2))\nmesh = Mesh(devices, ('batch', 'feats'))\n\nbatch_ = jax.device_put(batch, NamedSharding(mesh, P('batch', 'feats')))\nparams_ = jax.device_put(params, NamedSharding(mesh, P(('batch', 'feats'))))\n\n# mostly same as previous predict_fsdp definition, except we call gemm_tp\n@partial(jax.remat, policy=lambda op, *_, **__: str(op) != 'all_gather')\ndef predict_fsdp_tp(params_frag, inputs):\n  for W_frag, b_frag in params_frag:\n    W = jax.lax.all_gather(W_frag, 'batch', tiled=True)\n    b = jax.lax.all_gather(b_frag, 'batch', tiled=True)\n    block_result = jnp.dot(inputs, W)\n    outputs = jax.lax.psum_scatter(block_result, 'feats',\n                                   scatter_dimension=1, tiled=True) + b\n    inputs = jax.nn.relu(outputs)\n  return outputs\n\n@partial(shard_map, mesh=mesh,\n         in_specs=(P(('feats', 'batch')), P('batch', 'feats')),\n         out_specs=P())\ndef loss_fsdp_tp(local_params, local_batch):\n  inputs, targets = local_batch\n  predictions = predict_fsdp_tp(local_params, inputs)\n  sq_err = jax.lax.psum(jnp.sum((predictions - targets)**2, axis=-1), 'feats')\n  return jax.lax.pmean(jnp.mean(sq_err), 'batch') \n```", "```py\nprint(jax.jit(loss)(params, batch))\nprint(jax.jit(loss_fsdp_tp)(params_, batch_))\n\nprint(allclose(jax.jit(jax.grad(loss))(params, batch),\n               jax.jit(jax.grad(loss_fsdp_tp))(params, batch))) \n```", "```py\n22.779886\n22.779886\nTrue \n```", "```py\nL = len(params) - 2        # num layers, excluding first and last\nN = batch_size             # batch size\nF = params[0][0].shape[1]  # num features\n\n# choose some pipeline parameters\nS = 2      # number of stages\nB = 8      # size of each microbatch\nassert L % S == 0, \"S (number of stages) must divide L (number of inner layers)\"\n\n# compute some useful quantities\nM, ragged = divmod(N, B)  # M is number of microbatches\nassert not ragged, \"B (size of each microbatch) must divide total batch size\"\nK, ragged = divmod(M, S)  # K is microbatches per stage\nassert not ragged, \"S (number of stages) must divide number of microbatches\"\nprint(f'{S} stages, {L  //  S} layer(s) per stage, {L} pipelined layers total')\nprint(f'{B} examples per microbatch, {M} microbatches total') \n```", "```py\n2 stages, 2 layer(s) per stage, 4 pipelined layers total\n8 examples per microbatch, 4 microbatches total \n```", "```py\nmesh = Mesh(jax.devices()[:S], ('stages',))\n\ndef predict_pp(params, inputs):\n  (W_first, b_first), inner_params, (W_last, b_last) = params\n  inputs = jax.nn.relu(jnp.dot(inputs, W_first) + b_first)\n  inputs = spmd_pipeline(lambda Wb, x: jax.nn.relu(x @ Wb[0] + Wb[1]),\n                        inner_params, inputs)\n  outputs = jnp.dot(inputs, W_last) + b_last\n  return outputs\n\n@partial(shard_map, mesh=mesh, in_specs=((P(), P('stages'), P()), P('stages')),\n         out_specs=P())\ndef loss_pp(params, batch):\n  inputs, targets = batch\n  predictions = predict_pp(params, inputs.reshape(K, B, -1)).reshape(K * B, -1)\n  local_loss = jnp.mean(jnp.sum((predictions - targets)**2, axis=-1))\n  return jax.lax.pmean(local_loss, 'stages') \n```", "```py\ndef spmd_pipeline(fn, stage_params, inputs):\n  stage = jax.lax.axis_index('stages')\n  outputs = jnp.zeros_like(inputs) * jnp.nan\n  state = jnp.zeros((L // S, B, F)) * jnp.nan\n  for i in range(M+L-1):\n    state = state.at[0].set(jnp.where(stage == 0, inputs[i % K], state[0]))\n    state = jax.vmap(fn)(stage_params, state)\n    outputs = outputs.at[(i-L+1) % K].set(jnp.where(stage == S-1, state[-1], outputs[(i-L+1) % K]))\n    state, inputs, outputs = shift(i, state, inputs, outputs)\n  outputs = jax.lax.ppermute(outputs, 'stages', [(i, (i+1) % S) for i in range(S)])\n  return outputs\n\ndef shift(i, state, inputs, outputs):\n  sh = lambda x, d: jax.lax.ppermute(x, 'stages', [(i, (i+d) % S) for i in range(S)])\n  state = jnp.roll(state, +1, axis=0).at[0].set(sh(state[-1], +1))\n  if (i % K) == (-1 % K):\n    inputs = sh(inputs, +1)\n  if ((i-L+1) % K) == (-1 % K):\n    outputs = sh(outputs, +1)\n  return state, inputs, outputs \n```", "```py\nfirst_params, *inner_params, last_params = params\nWs, bs = zip(*inner_params)\nparams_stacked = jnp.stack(Ws), jnp.stack(bs)\nfirst_params = jax.device_put(first_params, NamedSharding(mesh, P()))\nparams_stacked = jax.device_put(params_stacked, NamedSharding(mesh, P('stages')))\nlast_params = jax.device_put(last_params, NamedSharding(mesh, P()))\nparams_ = first_params, params_stacked, last_params\n\nbatch_ = jax.device_put(batch, NamedSharding(mesh, P('stages'))) \n```", "```py\nprint(jax.jit(loss)(params, batch))\nprint(jax.jit(loss_pp)(params_, batch_)) \n```", "```py\n22.779886\n22.779884 \n```", "```py\n_ = jax.jit(jax.grad(loss_pp))(params_, batch_)   # don't crash \n```"]