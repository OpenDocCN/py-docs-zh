- en: Writing TPU kernels with Pallas
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pallas编写TPU内核
- en: 原文：[`jax.readthedocs.io/en/latest/pallas/tpu/details.html`](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`jax.readthedocs.io/en/latest/pallas/tpu/details.html`](https://jax.readthedocs.io/en/latest/pallas/tpu/details.html)
- en: This page focuses on the details that are important when attempting to run Pallas
    kernels on Google TPUs. For one, the TPU backend is still in an experimental phase,
    and only a subset of JAX NumPy will be accepted. Furthermore, writing performant
    code for TPUs might require thinking carefully about the native capabilities of
    the hardware. While many patterns that are unnatural to the hardware will be accepted,
    they might end up requiring software emulation, and can slow down the computation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本页关注试图在Google TPU上运行Pallas内核时的重要细节。首先，TPU后端仍处于实验阶段，并且只接受JAX NumPy的子集。此外，为TPU编写高性能代码可能需要仔细考虑硬件的本机能力。虽然许多对硬件不自然的模式将被接受，但它们最终可能需要软件模拟，并可能减慢计算速度。
- en: Warning
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: This feature should still be considered experimental as work is still in progress
    (in particular on improving the error messages).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能仍应视为实验性功能，因为工作仍在进行中（特别是在改进错误消息方面）。
- en: Note
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While all the features described here are experimental, we remain very serious
    about maintaining their correctness. As such, it might not be uncommon to see
    a “not implemented” error while attempting to write TPU kernels. But, if a kernel
    is accepted by the compiler, it *must* return the expected results.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然此处描述的所有功能都是实验性的，但我们仍然非常认真地维护其正确性。因此，在尝试编写TPU内核时可能看到“未实现”错误并不罕见。但是，如果编译器接受了内核，它*必须*返回预期的结果。
- en: If you see unexpected outputs, please compare them against a kernel run with
    `interpret=True` passed in to `pallas_call`. If the results diverge, please file
    a [bug report](https://github.com/google/jax/issues/new/choose).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到意外的输出，请将其与传递`interpret=True`到`pallas_call`的内核运行进行比较。如果结果不一致，请提交[错误报告](https://github.com/google/jax/issues/new/choose)。
- en: What is a TPU?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是TPU？
- en: '![A TPUv4 board](https://lh3.googleusercontent.com/PBWR5LFWaz8Nx4F7vRstDjt_nvUYdfxe9H3O9i3KMam_RmmwIOQMr1GAq3RUfowET2cK5kAcb_zGpw=e14-rw-lo-sc0xffffff-w2540)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![一个TPUv4板](https://lh3.googleusercontent.com/PBWR5LFWaz8Nx4F7vRstDjt_nvUYdfxe9H3O9i3KMam_RmmwIOQMr1GAq3RUfowET2cK5kAcb_zGpw=e14-rw-lo-sc0xffffff-w2540)'
- en: TPU is a hardware accelerator developed at Google. You can think of TPUs as
    GPUs, but specialized for machine learning workloads specifically. As such, their
    architecture differs quite significantly. However, we believe that Pallas can
    make it easy to start writing TPU kernels, even without having a full understanding
    of the underlying hardware. Having said that, understanding the hardware well
    will certainly make it easier to write performant kernels.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TPU是Google开发的硬件加速器。您可以将TPU视为专门用于机器学习工作负载的GPU。因此，它们的架构有相当大的差异。然而，我们相信Pallas可以使您轻松开始编写TPU内核，即使您没有完全理解底层硬件也是如此。话虽如此，深入了解硬件将确实使编写高性能内核变得更加容易。
- en: In a nutshell, the main difference between TPUs and GPUs is that TPUs are sequential
    machines with a very wide vector register (kind of like a CPU!). At the same time,
    they allow the software to schedule certain operations in the background, making
    them execute asynchronously with respect to the main instruction stream. This
    includes things like HBM memory accesses (which cannot be issued directly, but
    instead have to be prefetched to lower levels of the memory hierarchy by the DMA
    subunits), matrix multiplies (supported by the MXU unit) or matrix transpositions
    and permutes (supported by the XLU unit).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，TPU与GPU的主要区别在于TPU是顺序机器，具有非常宽的向量寄存器（类似于CPU！）。与此同时，它们允许软件安排某些操作在后台执行，使其与主指令流异步执行。这包括HBM内存访问（无法直接发出，而是必须通过DMA子单元预取到较低层次的内存层次结构）、矩阵乘法（由MXU单元支持）或矩阵转置和置换（由XLU单元支持）。
- en: If you’re interested in learning more about the TPU architecture in detail,
    we recommend reading a collection of papers published over the years. While many
    of them talk about specific TPU generations, many of the ideas described transfer
    to later generations as well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对详细了解TPU架构感兴趣，我们建议阅读多年来发表的一系列论文集。虽然许多论文谈论特定的TPU代，但其中许多描述的思想也适用于后续代。
- en: '[A Domain-Specific Supercomputer for Training Deep Neural Networks](https://dl.acm.org/doi/10.1145/3360307)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于训练深度神经网络的领域特定超级计算机](https://dl.acm.org/doi/10.1145/3360307)'
- en: '[The Design Process for Google’s Training Chips: TPUv2 and TPUv3](https://ieeexplore.ieee.org/document/9351692)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google培训芯片的设计过程：TPUv2和TPUv3](https://ieeexplore.ieee.org/document/9351692)'
- en: '[Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product](https://ieeexplore.ieee.org/document/9499913)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[三代形塑Google TPUv4i的十大经验教训：工业产品](https://ieeexplore.ieee.org/document/9499913)'
- en: '[TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with
    Hardware Support for Embeddings](https://dl.acm.org/doi/abs/10.1145/3579371.3589350)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TPU v4：支持嵌入式硬件的机器学习光学可重构超级计算机](https://dl.acm.org/doi/abs/10.1145/3579371.3589350)'
- en: Noteworthy properties and restrictions
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值得注意的属性和限制
- en: '`BlockSpec`s and grid iteration'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`BlockSpec`s 和网格迭代'
- en: '`BlockSpec`s generally behave as expected in Pallas — every invocation of the
    kernel body gets access to slices of the inputs and is meant to initialize a slice
    of the output.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pallas中，`BlockSpec`s通常按预期行为——每次核心体调用都会访问输入的片段，并且旨在初始化输出的一个片段。
- en: Warning
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Not all window shapes are supported. If the last two dimensions of your input
    are larger than 8 and 128 respectively, the window shape in those dimensions must
    be a multiple of the respective factor. If the input dimension is smaller, the
    window should span the full dimension.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的窗口形状都受支持。如果你的输入的最后两个维度分别大于8和128，那么这些维度中的窗口形状必须是对应因子的倍数。如果输入维度较小，则窗口应跨越整个维度。
- en: 'One interesting aspect of Pallas TPU kernels is the way they handle memory
    spaces: While the inputs to `pallas_call` will often reside in HBM (the main TPU
    memory), the references passed in to the kernel body will point to buffers in
    lower levels of memory hierarchy (VMEM or SMEM). This enables the kernel body
    to write and read them at very high speeds, while all the communication with HBM
    (which has very high latency) is handled by the compiler and overlapped with compute.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas TPU核心的一个有趣方面是它们处理内存空间的方式：虽然`pallas_call`的输入通常驻留在HBM（主TPU内存）中，但传递到核心体的引用将指向内存层次结构较低的缓冲区（VMEM或SMEM）。这使得核心体能够以非常高的速度读写它们，而所有与HBM的通信（具有非常高的延迟）由编译器处理并与计算重叠。
- en: 'What’s more, compared to GPUs, TPUs are actually highly sequential machines.
    Ergo, the grid is generally not processed in parallel, but sequentially, in lexicographic
    order (though see the Multicore TPU configurations section for exceptions). This
    unlocks some interesting capabilities:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与GPU相比，TPU实际上是高度序列化的机器。因此，网格通常不是并行处理的，而是按字典顺序顺序处理（尽管请参阅多核TPU配置部分的例外情况）。这解锁了一些有趣的功能：
- en: When two (lexicographically) consecutive grid indices use the same slice of
    an input, the HBM transfer for the second iteration is skipped, as the data is
    already available.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当两个（按字典顺序）连续的网格索引使用相同输入的片段时，第二次迭代的HBM传输将被跳过，因为数据已经可用。
- en: Multiple invocations of the kernel body can write to the same slice of the output,
    without any risk of race conditions. However, we do require that all invocations
    that write to a particular slice are consecutive.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个核心体调用可以向输出的同一片段写入，而不会有任何竞态条件的风险。但我们确实要求写入特定片段的所有调用是连续的。
- en: The “consecutive” restriction on the output usually means that the some prefix
    of the grid dimensions always vary the slice of the output an invocation needs
    to access, while the output window remains constant for the remaining suffix.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 关于输出的“连续”限制通常意味着网格维度的某些前缀总是变化，而调用需要访问的输出窗口对于其余后缀保持不变。
- en: 'For example, when implementing a Pallas TPU kernel for matrix multiplication,
    one would generally use a 3 dimensional grid: the first two dimensions would correspond
    to slicing along the first axis of the left operand and the second axis of the
    second operand. The third and *last* grid axis would tile the reduction dimension.
    The grid axis corresponding to the reduction dimension has to be the last one,
    since the output window does not vary along this axis. The output reference can
    be then used as an accumulator for partial results.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在实现矩阵乘法的Pallas TPU核心时，通常会使用三维网格：前两个维度对应于沿左操作数的第一轴和第二操作数的第二轴切片。第三和*最后*网格轴将瓦片化减少维度。与减少维度对应的网格轴必须是最后一个，因为输出窗口沿此轴不变。输出引用随后可用作部分结果的累加器。
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: VMEM is fairly large for such a low-level memory hierarchy (16MB+), making it
    possible to use large window sizes. And, oftentimes, the larger the window size,
    the better the eventual hardware utilization will be. However, it is possible
    to specify a window size that (together with space necessary to hold spilled vector
    registers) exceeds the size of VMEM. In this case, you will likely see a low-level
    compiler error message complaining about an out-of-memory error.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一个低级内存层次结构（16MB+），VMEM相当大，这使得可以使用较大的窗口大小。通常情况下，窗口大小越大，最终硬件利用率就越好。然而，可能会指定一个窗口大小，该大小（加上保存溢出矢量寄存器所需的空间）超过了VMEM的大小。在这种情况下，您可能会看到一个低级编译器错误消息，抱怨内存不足错误。
- en: Dimension ordering is meaningful
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度排序是有意义的
- en: In JAX programs, the ordering of intermediate arrays inside `jax.jit` usually
    has no impact on performance, as the compiler is free to rearrange them. However,
    as Pallas is meant to expose lower-level capabilities, the dimension order can
    have great impact on the quality of generated code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在JAX程序中，`jax.jit`内部数组的排序通常不会影响性能，因为编译器可以自由地重新排列它们。但是，由于Pallas旨在暴露更低级的功能，维度顺序对生成的代码质量有很大影响。
- en: Recall that the TPUs perform bulk of the computation on 2D vector registers.
    Pallas TPU will only ever consider mapping the last two dimensions of intermediate
    arrays to those vector register dimensions (sublanes and lanes respectively).
    An array of shape `(n, 1, 1)` is guaranteed to require at least `n` vector registers
    to represent. If `n` becomes too large, this can lead to spills, and potential
    VMEM OOM errors due to an overly large memory footprint. But it also might not
    — the low-level compiler is free to rearrange the instructions to lower the register
    pressure, and is in fact very good at it. Still, it is a good rule of thumb to
    keep the last two dimensions large (especially the last dimension), while keeping
    the leading dimensions small.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，TPU主要在2D矢量寄存器上执行大部分计算。Pallas TPU只会考虑将中间数组的最后两个维度映射到这些矢量寄存器维度（子通道和通道）。形状为`(n,
    1, 1)`的数组保证需要至少`n`个矢量寄存器来表示。如果`n`变得太大，则可能会导致溢出，并由于过大的内存占用而导致VMEM内存不足错误。但这也可能不会发生
    — 低级编译器可以重新排列指令以降低寄存器压力，并且实际上在这方面做得非常好。尽管如此，保持最后两个维度大（特别是最后一个维度），同时使前导维度保持小是一个很好的经验法则。
- en: Multicore TPU configurations
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多核TPU配置
- en: 'In newer TPU generations, the two cores on a chip are often abstracted as a
    single device. To take advantage of multiple cores, Pallas has to break the sequential
    grid execution guarantees, and will need to parallelize one of the grid axes over
    cores. This is an opt-in procedure. To allow that, `pallas_call` requires an extra
    parameter named `dimension_semantics`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新的TPU生成中，芯片上的两个核心通常被抽象为单个设备。为了利用多个核心，Pallas必须打破顺序网格执行的保证，并且需要在核心上并行化一个网格轴。这是一个选择加入的过程。为了允许这样做，`pallas_call`需要一个额外的名为`dimension_semantics`的参数：
- en: That parameter is a list, with as many entries as many axes there are in the
    grid. Only `parallel` dimensions can be partitioned over cores. As a rule of thumb,
    the dimensions are parallel, unless the output window does not vary. As such,
    `dimension_semantics` is always a number of `parallel` axes followed by a number
    of `arbitrary` axes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该参数是一个列表，其条目数量与网格中的轴数量相同。只有`parallel`维度可以在核心上分区。作为一个经验法则，维度是并行的，除非输出窗口不变。因此，`dimension_semantics`始终是一些`parallel`轴的数字，后跟一些`arbitrary`轴的数字。
- en: While partitioning a kernel over a 2-core TPU device often leads to a 2x speedup,
    it can be in fact significantly smaller. This is especially true if different
    instances of the body have highly varying cost. If all of the expensive steps
    get mapped to one core, but all cheap steps are assigned to the other, the second
    core will be sitting idle until the first one completes its tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在2核TPU设备上分区内核通常会导致2倍速度提升，但实际上可能会显著小于此值。特别是如果体的不同实例具有非常不同的成本，这一点尤为真实。如果所有昂贵的步骤都映射到一个核心，而所有廉价的步骤都分配给另一个核心，则第二个核心将在第一个完成其任务之前处于空闲状态。
- en: Pallas TPU generally favors partitioning axes of a size that is a multiple of
    the number of TPU cores, and prefers to partition leading grid axes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas TPU通常偏好将大小为TPU核心数量倍数的轴进行分区，并且更喜欢分区主导的网格轴。
- en: Placing operands in SMEM
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将操作数放入SMEM
- en: Most of the compute on the TPU will happen on the vector unit. Still, there
    are many cases where it is useful to perform a number of scalar operations, e.g.,
    to carry out control-flow. For that reason, TPUs come with a separate scalar unit,
    and a separate scalar memory (SMEM) attached to it. As a rule of thumb, any data
    used to perform control-flow decisions should be placed in SMEM.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数TPU计算将在向量单元上进行。然而，有许多情况下进行一些标量操作是有用的，例如执行控制流。因此，TPU配备了一个单独的标量单元，并附有一个单独的标量存储器（SMEM）。按照一个经验法则，用于执行控制流决策的任何数据应放置在SMEM中。
- en: SMEM is a low-latency memory that supports random access, but lets you only
    read and write 32-bit values with a single instruction (very small compared to
    the 4KBi granularity of VMEM transactions, but much more flexible due to lack
    of alignment requirements!).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SMEM是一种低延迟内存，支持随机访问，但只能用单个指令读写32位值（与VMEM事务的4KBi粒度相比非常小，但由于没有对齐要求而更加灵活！）。
- en: The scalar memory is also very useful when implementing kernels that do not
    access the tiles of inputs in a regular pattern, such as when writing block-sparse
    kernels. In Pallas, this can be achieved by replacing the `grid` argument to `pallas_call`
    with a `grid_spec` of `PrefetchScalarGridSpec` with a non-zero `num_scalar_prefetch`
    argument. If `num_scalar_prefetch` is `n`, then the first `n` arguments to `pallas_call`
    will be placed in SMEM. No `BlockSpec`s should be specified for those arguments.
    But, the `BlockSpec`s for all subsequent arguments will receive not only the grid
    indices, but also the SMEM references to the leading operands.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现不按规则模式访问输入块的内核时，标量内存也非常有用，例如编写块稀疏内核时。在Pallas中，可以通过将`pallas_call`的`grid`参数替换为具有非零`num_scalar_prefetch`参数的`PrefetchScalarGridSpec`的`grid_spec`来实现这一点。如果`num_scalar_prefetch`为`n`，那么`pallas_call`的前`n`个参数将放置在SMEM中。对于这些参数，不应指定任何`BlockSpec`。但是，对于所有后续参数的`BlockSpec`，不仅会收到网格索引，还会收到领先操作数的SMEM引用。
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are working on implementing examples for this feature. Stay tuned!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在努力实现此功能的示例。敬请关注！
- en: Supported data types
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持的数据类型
- en: 'At the moment Pallas TPU only supports the following data types:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Pallas TPU仅支持以下数据类型：
- en: '`jnp.float32`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.float32`'
- en: '`jnp.bfloat16`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.bfloat16`'
- en: '`jnp.int*` (all precisions, except for `jnp.int4`)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.int*`（所有精度，除了`jnp.int4`）'
- en: '`jnp.uint*` (all precisions)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jnp.uint*`（所有精度）'
- en: Computation placement
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算放置
- en: All scalar (i.e. 0D) arrays will be stored in scalar registers, and operations
    on then will be executed on the scalar core. All other operations (even on single-element,
    but 1D+ arrays) will be executed on the vector core.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所有标量（即0D）数组将存储在标量寄存器中，并在标量核心上执行操作。所有其他操作（甚至是对单个元素但是1D+数组的操作）将在向量核心上执行。
- en: Supported operations
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的操作
- en: Matrix multiplication
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: Matrix multiplication always produces results in the float32 format. If your
    inputs are not float32, we recommend using `lax.dot` with `preferred_element_type`
    set to `jnp.float32`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法始终以`float32`格式生成结果。如果您的输入不是float32，建议使用`lax.dot`并将`preferred_element_type`设置为`jnp.float32`。
- en: When using `lax.dot_general`, it is possible to fuse transpositions of the last
    two dimensions of matrix multiplication operands into the operation, which can
    improve overall kernel performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`lax.dot_general`时，可以将矩阵乘法操作数的最后两个维度的转置融合到操作中，这可以提高整体内核性能。
- en: Precision control
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精度控制
- en: Pallas TPU lowering is aware of `jax.default_matmul_precision`. For best performance
    (and lowest precision), use `bfloat16`. If you care about numerical accuracy,
    you might want to set the precision to `float32`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Pallas TPU的降低考虑到了`jax.default_matmul_precision`。为了获得最佳性能（和最低精度），请使用`bfloat16`。如果您关心数值精度，可能需要将精度设置为`float32`。
- en: Warning
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Even if you pass in 32-bit operands to a matrix multiplication, they will be
    rounded to `bfloat16` unless `float32` precision is requested.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 即使将32位操作数传递给矩阵乘法，除非请求`float32`精度，否则它们将会被四舍五入为`bfloat16`。
- en: Transposition
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转置
- en: If the value has at least 4 dimensions, arbitrary transpositions of all but
    the last two axes are free. Otherwise, only the transposition of the last two
    axes is implemented. Note that some transpositions of the last two dimensions
    can be fused into matrix multiplication.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果值至少有4个维度，则除了最后两个轴以外的任意转置都是免费的。否则，仅实现了最后两个轴的转置。请注意，一些最后两个维度的转置可以融合到矩阵乘法中。
- en: Accessing memory
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问内存
- en: Arbitrary slices of references can be read or updated, subject to implementation
    constraints. Currently, no restrictions are placed on inputs that are 32-bit wide,
    but only some slicing patterns are supported for narrower types. Reads and writes
    that are aligned to multiples of, and have a length that is a multiple of 8 and
    128 respectively in the last two dimensions are always supported.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以读取或更新引用的任意片段，受实现约束的限制。目前，对于宽度为 32 位的输入没有限制，但只支持某些更窄类型的切片模式。总是支持最后两个维度中分别是
    8 和 128 的倍数的对齐读写。
- en: Reads and writes to vector memory generally happen on tiles of shape `(8, 128)`.
    As such, when reading or writing to references that have at least two dimensions,
    the best performance is achieved when the base offset of the memory access has
    indices divisible by the tiling, and the size of the read region is a multiple
    of the tile size.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在向量内存的读写发生在形状为 `(8, 128)` 的瓦片上。因此，当读取或写入至少有两个维度的引用时，最佳性能是在内存访问的基础偏移具有瓦片可整除的索引，并且读取区域的大小是瓦片大小的倍数。
- en: Elementwise operations
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逐元素操作
- en: Many elementwise operations are supported. It is worth noting that the hardware
    generally only supports elementwise computation using 32-bit types. When loading
    operands that use lower-precision types, they should generally be upcast to a
    32-bit type before applying elementwise ops.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 支持许多逐元素操作。值得注意的是，硬件通常仅支持使用 32 位类型进行逐元素计算。在加载使用较低精度类型的操作数时，通常应先将其升级为 32 位类型再应用逐元素操作。
- en: 'It is worth noting that they can vary *significantly* in their cost. As such,
    we outline three categories of supported operations: cheap (🟢), medium (🌕) and
    expensive (🔴).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，它们的成本可能*显著*不同。因此，我们列出了三类支持的操作：廉价（🟢）、中等（🌕）和昂贵（🔴）。
- en: '| Operation | Cost |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 成本 |'
- en: '| --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `jnp.add`, `+` | 🟢 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.add`，`+` | 🟢 |'
- en: '| `jnp.sub`, `-` | 🟢 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.sub`，`-` | 🟢 |'
- en: '| `jnp.mul`, `*` | 🟢 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.mul`，`*` | 🟢 |'
- en: '| `/`, `//`, `%` | 🌕 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `/`，`//`，`%` | 🌕 |'
- en: '| `jnp.max`, `jnp.min` | 🟢 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.max`，`jnp.min` | 🟢 |'
- en: '| `jnp.where` (select) | 🟢 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.where`（选择） | 🟢 |'
- en: '| `jnp.abs` | 🟢 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.abs` | 🟢 |'
- en: '| `&#124;`, `^`, `&`, `~` | 🟢 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `|`，`^`，`&`，`~` | 🟢 |'
- en: '| `<<`, `>>` | 🟢 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `<<`，`>>` | 🟢 |'
- en: '| Comparisons (`==`, …) | 🟢 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 比较运算（`==`，...） | 🟢 |'
- en: '| Type casts (`.astype`) | 🟢 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 类型转换（`.astype`） | 🟢 |'
- en: '| `jnp.exp` | 🌕 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.exp` | 🌕 |'
- en: '| `jnp.tanh` | 🌕 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.tanh` | 🌕 |'
- en: '| `jnp.pow` | 🌕 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.pow` | 🌕 |'
- en: '| `jnp.sin` | 🔴 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.sin` | 🔴 |'
- en: '| `jnp.cos` | 🔴 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `jnp.cos` | 🔴 |'
- en: Many JAX functions are implemented in terms of other JAX primitives, so this
    list might not be comprehensive. For example, `jax.nn.relu` is implemented in
    terms of comparisons and `jnp.where` will work in Pallas kernels too.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 JAX 函数是基于其他 JAX 原语实现的，因此此列表可能不完整。例如，`jax.nn.relu` 是基于比较实现的，而 `jnp.where`
    在 Pallas 内核中也能工作。
- en: Array constructors
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数组构造函数
- en: All constant array constructors are supported (`jnp.ones`, `jnp.zeros`, `jnp.full`).
    Notably, the `jax.random` module is **not** compatible with Pallas as of today.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所有常数数组构造函数都受支持（`jnp.ones`，`jnp.zeros`，`jnp.full`）。特别是，截至今天，`jax.random` 模块与
    Pallas **不** 兼容。
- en: Reductions
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 归约
- en: Sum, maximum and minimum reductions are supported, but only on a single array
    axis at a time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 支持求和、最大值和最小值的归约，但一次只能在一个数组轴上进行。
- en: Reductions over the last array dimension are generally the slowest. Reductions
    over the second last dimension are faster, but still slower than over the leading
    dimensions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对最后一个数组维度的归约通常是最慢的。对倒数第二个维度的归约更快，但仍比前面的维度慢。
- en: Broadcasting
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广播
- en: The performance characteristics of broadcasting are very similar to those of
    reductions. Broadcasting along all but the two trailing dimensions is always supported
    and free. Broadcasting along the second to last dimension is slower, while broadcasting
    along the last dimension is the slowest.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 广播的性能特性与归约非常相似。总是支持除了最后两个维度之外的所有广播，且是免费的。沿着倒数第二个维度进行广播较慢，而沿着最后一个维度进行广播最慢。
- en: Reshapes
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重塑
- en: As usual, reshapes in all dimensions but the last two dimensions are supported
    and free.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如常地，所有维度除了最后两个维度的重塑都是支持的且是免费的。
- en: The only two supported cases when a reshape can modify the last two dimensions
    of an array is when (1) some leading dimensions are flattened onto the second
    to last dimension, or (2) it adds a dimension that was just removed by a reduction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一支持的情况是当重塑可以修改数组的最后两个维度时，即（1）某些前导维度展平到倒数第二个维度，或者（2）它添加了刚刚由归约移除的维度。
- en: Control flow
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制流程
- en: The TPU backend features limited support for control flow at the moment. The
    currently supported functions are `cond`, `fori_loop` and `for_loop`. However,
    loop primitives get fully unrolled during the compilation at the moment, so try
    to keep the loop trip count reasonably small.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，TPU后端对控制流的支持有限。目前支持的函数有`cond`、`fori_loop`和`for_loop`。然而，在编译时，循环原语会完全展开，因此请尽量保持循环执行次数合理小。
- en: Overusing control flow can lead to significant regressions in low-level code
    generation, and it is recommended to try to squeeze as many computationally expensive
    operations into a single basic block as possible.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 过度使用控制流可能导致低级代码生成中的显著回归，建议尽量将多个计算密集型操作挤入一个基本块中。
