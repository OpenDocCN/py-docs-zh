["```py\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random \n```", "```py\n# A helper function to randomly initialize weights and biases\n# for a dense neural network layer\ndef random_layer_params(m, n, key, scale=1e-2):\n  w_key, b_key = random.split(key)\n  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n\n# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\ndef init_network_params(sizes, key):\n  keys = random.split(key, len(sizes))\n  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n\nlayer_sizes = [784, 512, 512, 10]\nstep_size = 0.01\nnum_epochs = 10\nbatch_size = 128\nn_targets = 10\nparams = init_network_params(layer_sizes, random.key(0)) \n```", "```py\nfrom jax.scipy.special import logsumexp\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  # per-example predictions\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits - logsumexp(logits) \n```", "```py\n# This works on single examples\nrandom_flattened_image = random.normal(random.key(1), (28 * 28,))\npreds = predict(params, random_flattened_image)\nprint(preds.shape) \n```", "```py\n(10,) \n```", "```py\n# Doesn't work with a batch\nrandom_flattened_images = random.normal(random.key(1), (10, 28 * 28))\ntry:\n  preds = predict(params, random_flattened_images)\nexcept TypeError:\n  print('Invalid shapes!') \n```", "```py\nInvalid shapes! \n```", "```py\n# Let's upgrade it to handle batches using `vmap`\n\n# Make a batched version of the `predict` function\nbatched_predict = vmap(predict, in_axes=(None, 0))\n\n# `batched_predict` has the same call signature as `predict`\nbatched_preds = batched_predict(params, random_flattened_images)\nprint(batched_preds.shape) \n```", "```py\n(10, 10) \n```", "```py\ndef one_hot(x, k, dtype=jnp.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n\ndef accuracy(params, images, targets):\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\n\n@jit\ndef update(params, x, y):\n  grads = grad(loss)(params, x, y)\n  return [(w - step_size * dw, b - step_size * db)\n          for (w, b), (dw, db) in zip(params, grads)] \n```", "```py\nimport tensorflow as tf\n# Ensure TF does not see GPU and grab all GPU memory.\ntf.config.set_visible_devices([], device_type='GPU')\n\nimport tensorflow_datasets as tfds\n\ndata_dir = '/tmp/tfds'\n\n# Fetch full datasets for evaluation\n# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\nmnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\nmnist_data = tfds.as_numpy(mnist_data)\ntrain_data, test_data = mnist_data['train'], mnist_data['test']\nnum_labels = info.features['label'].num_classes\nh, w, c = info.features['image'].shape\nnum_pixels = h * w * c\n\n# Full train set\ntrain_images, train_labels = train_data['image'], train_data['label']\ntrain_images = jnp.reshape(train_images, (len(train_images), num_pixels))\ntrain_labels = one_hot(train_labels, num_labels)\n\n# Full test set\ntest_images, test_labels = test_data['image'], test_data['label']\ntest_images = jnp.reshape(test_images, (len(test_images), num_pixels))\ntest_labels = one_hot(test_labels, num_labels) \n```", "```py\nprint('Train:', train_images.shape, train_labels.shape)\nprint('Test:', test_images.shape, test_labels.shape) \n```", "```py\nTrain: (60000, 784) (60000, 10)\nTest: (10000, 784) (10000, 10) \n```", "```py\nimport time\n\ndef get_train_batches():\n  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n  # You can build up an arbitrary tf.data input pipeline\n  ds = ds.batch(batch_size).prefetch(1)\n  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n  return tfds.as_numpy(ds)\n\nfor epoch in range(num_epochs):\n  start_time = time.time()\n  for x, y in get_train_batches():\n    x = jnp.reshape(x, (len(x), num_pixels))\n    y = one_hot(y, num_labels)\n    params = update(params, x, y)\n  epoch_time = time.time() - start_time\n\n  train_acc = accuracy(params, train_images, train_labels)\n  test_acc = accuracy(params, test_images, test_labels)\n  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n  print(\"Training set accuracy {}\".format(train_acc))\n  print(\"Test set accuracy {}\".format(test_acc)) \n```", "```py\nEpoch 0 in 28.30 sec\nTraining set accuracy 0.8400499820709229\nTest set accuracy 0.8469000458717346\nEpoch 1 in 14.74 sec\nTraining set accuracy 0.8743667006492615\nTest set accuracy 0.8803000450134277\nEpoch 2 in 14.57 sec\nTraining set accuracy 0.8901500105857849\nTest set accuracy 0.8957000374794006\nEpoch 3 in 14.36 sec\nTraining set accuracy 0.8991333246231079\nTest set accuracy 0.903700053691864\nEpoch 4 in 14.20 sec\nTraining set accuracy 0.9061833620071411\nTest set accuracy 0.9087000489234924\nEpoch 5 in 14.89 sec\nTraining set accuracy 0.9113333225250244\nTest set accuracy 0.912600040435791\nEpoch 6 in 13.95 sec\nTraining set accuracy 0.9156833291053772\nTest set accuracy 0.9176000356674194\nEpoch 7 in 13.32 sec\nTraining set accuracy 0.9192000031471252\nTest set accuracy 0.9214000701904297\nEpoch 8 in 13.55 sec\nTraining set accuracy 0.9222500324249268\nTest set accuracy 0.9241000413894653\nEpoch 9 in 13.40 sec\nTraining set accuracy 0.9253666996955872\nTest set accuracy 0.9269000291824341 \n```"]