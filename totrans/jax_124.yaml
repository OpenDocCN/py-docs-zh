- en: jax.nn module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`jax.readthedocs.io/en/latest/jax.nn.html`](https://jax.readthedocs.io/en/latest/jax.nn.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`jax.nn.initializers` module'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common functions for neural network libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| `relu` | Rectified linear unit activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `relu6` | Rectified Linear Unit 6 activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `sigmoid`(x) | Sigmoid activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `softplus`(x) | Softplus activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `sparse_plus`(x) | Sparse plus function. |'
  prefs: []
  type: TYPE_TB
- en: '| `sparse_sigmoid`(x) | Sparse sigmoid activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `soft_sign`(x) | Soft-sign activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `silu`(x) | SiLU (aka swish) activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `swish`(x) | SiLU (aka swish) activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `log_sigmoid`(x) | Log-sigmoid activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `leaky_relu`(x[, negative_slope]) | Leaky rectified linear unit activation
    function. |'
  prefs: []
  type: TYPE_TB
- en: '| `hard_sigmoid`(x) | Hard Sigmoid activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `hard_silu`(x) | Hard SiLU (swish) activation function |'
  prefs: []
  type: TYPE_TB
- en: '| `hard_swish`(x) | Hard SiLU (swish) activation function |'
  prefs: []
  type: TYPE_TB
- en: '| `hard_tanh`(x) | Hard \(\mathrm{tanh}\) activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `elu`(x[, alpha]) | Exponential linear unit activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `celu`(x[, alpha]) | Continuously-differentiable exponential linear unit
    activation. |'
  prefs: []
  type: TYPE_TB
- en: '| `selu`(x) | Scaled exponential linear unit activation. |'
  prefs: []
  type: TYPE_TB
- en: '| `gelu`(x[, approximate]) | Gaussian error linear unit activation function.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `glu`(x[, axis]) | Gated linear unit activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `squareplus`(x[, b]) | Squareplus activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `mish`(x) | Mish activation function. |'
  prefs: []
  type: TYPE_TB
- en: Other functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| `softmax`(x[, axis, where, initial]) | Softmax function. |'
  prefs: []
  type: TYPE_TB
- en: '| `log_softmax`(x[, axis, where, initial]) | Log-Softmax function. |'
  prefs: []
  type: TYPE_TB
- en: '| `logsumexp`() | Log-sum-exp reduction. |'
  prefs: []
  type: TYPE_TB
- en: '| `standardize`(x[, axis, mean, variance, ...]) | Normalizes an array by subtracting
    `mean` and dividing by \(\sqrt{\mathrm{variance}}\). |'
  prefs: []
  type: TYPE_TB
- en: '| `one_hot`(x, num_classes, *[, dtype, axis]) | One-hot encodes the given indices.
    |'
  prefs: []
  type: TYPE_TB
