["```py\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8' # Use 8 CPU devices \n```", "```py\nimport jax.numpy as jnp\nfrom jax import lax\nfrom jax.nn import one_hot, relu\nfrom jax.scipy.special import logsumexp\n\ndef predict(w1, w2, images):\n  hiddens = relu(jnp.dot(images, w1))\n  logits = jnp.dot(hiddens, w2)\n  return logits - logsumexp(logits, axis=1, keepdims=True)\n\ndef loss(w1, w2, images, labels):\n  predictions = predict(w1, w2, images)\n  targets = one_hot(labels, predictions.shape[-1])\n  losses = jnp.sum(targets * predictions, axis=1)\n  return -jnp.mean(losses, axis=0) \n```", "```py\nw1 = jnp.zeros((784, 512))\nw2 = jnp.zeros((512, 10))\nimages = jnp.zeros((128, 784))\nlabels = jnp.zeros(128, dtype=jnp.int32)\n\nprint(loss(w1, w2, images, labels)) \n```", "```py\ndef named_predict(w1, w2, image):\n  hidden = relu(lax.pdot(image, w1, 'inputs'))\n  logits = lax.pdot(hidden, w2, 'hidden')\n  return logits - logsumexp(logits, 'classes')\n\ndef named_loss(w1, w2, images, labels):\n  predictions = named_predict(w1, w2, images)\n  num_classes = lax.psum(1, 'classes')\n  targets = one_hot(labels, num_classes, axis='classes')\n  losses = lax.psum(targets * predictions, 'classes')\n  return -lax.pmean(losses, 'batch') \n```", "```py\nfrom jax.experimental.maps import xmap\n\nin_axes = [['inputs', 'hidden', ...],\n           ['hidden', 'classes', ...],\n           ['batch', 'inputs', ...],\n           ['batch', ...]]\n\nloss = xmap(named_loss, in_axes=in_axes, out_axes=[...])\nprint(loss(w1, w2, images, labels)) \n```", "```py\nimport jax\nimport numpy as np\nfrom jax.sharding import Mesh\n\nloss = xmap(named_loss, in_axes=in_axes, out_axes=[...],\n            axis_resources={'batch': 'x'})\n\ndevices = np.array(jax.local_devices())\nwith Mesh(devices, ('x',)):\n  print(loss(w1, w2, images, labels)) \n```", "```py\nloss = xmap(named_loss, in_axes=in_axes, out_axes=[...],\n            axis_resources={'hidden': 'x'})\n\ndevices = np.array(jax.local_devices())\nwith Mesh(devices, ('x',)):\n  print(loss(w1, w2, images, labels)) \n```", "```py\nloss = xmap(named_loss, in_axes=in_axes, out_axes=[...],\n            axis_resources={'batch': 'x', 'hidden': 'y'})\n\ndevices = np.array(jax.local_devices()).reshape((4, 2))\nwith Mesh(devices, ('x', 'y')):\n  print(loss(w1, w2, images, labels)) \n```", "```py\nimport jax.numpy as jnp\nfrom jax import lax\nfrom functools import partial\nimport jax\nimport numpy as np \n```", "```py\nfrom typing import Any, Callable\n\nclass ArrayType:\n  def __getitem__(self, idx):\n    return Any\nf32 = ArrayType()\ni32 = ArrayType() \n```", "```py\nx: f32[(2, 3)] = np.ones((2, 3), dtype=np.float32)\ny: f32[(3, 5)] = np.ones((3, 5), dtype=np.float32)\nz: f32[(2, 5)] = x.dot(y)  # matrix multiplication\nw: f32[(7, 1, 5)] = np.ones((7, 1, 5), dtype=np.float32)\nq: f32[(7, 2, 5)] = z + w  # broadcasting \n```", "```py\ne: f32[(5, 7), {'batch': 20, 'sequence': 30}]\n# e.shape == (5, 7)\n# e.named_shape == {'batch': 20, 'sequence': 30} == {'sequence': 30, 'batch': 20} \n```", "```py\nfrom jax.experimental.maps import xmap\n\ndef my_func(x: f32[(5,), {'batch': 20}]) -> f32[(5,), {'batch': 20}]:\n  assert x.shape == (5,)\n  # assert x.named_shape == {'batch': 20}  # TODO: Implement named_shape\n  return x\n\nx: f32[(20, 5)] = jnp.zeros((20, 5), dtype=np.float32)\nf = xmap(my_func,\n         in_axes={0: 'batch'},   # Name the first axis of the only argument 'batch'\n         out_axes={1: 'batch'})  # Place the 'batch' named axis of the output as the second positional axis\ny: f32[(5, 20)] = f(x)\nassert (y == x.T).all()  # The first dimension was removed from x and then re-inserted as the last dim \n```", "```py\nx = jnp.arange(5)\ny = jnp.arange(7)\ntry:\n  jnp.einsum('i,i->i', x, y)\nexcept Exception as e:\n  print('einsum:', e)\ntry:\n  xmap(lambda x, y: x * y,\n       in_axes=(['i', ...], ['i', ...]),\n       out_axes=['i', ...])(x, y)\nexcept Exception as e:\n  print('xmap:', e) \n```", "```py\nfrom jax.scipy.linalg import expm_frechet\n\n# Any other function that does not assume existence of any named axes would do too,\n# at least as long as it matches this type signature:\nexpm_frechet: Callable[[f32[(3, 3)], f32[(3, 3)]], f32[(3, 3)]]\nf = partial(expm_frechet, compute_expm=False)\n\n# Each A with each E\nbatch_A = jnp.ones((5, 3, 3), dtype=np.float32)\nbatch_E = jnp.ones((5, 3, 3), dtype=np.float32)\nbatch_AE = xmap(f,\n                in_axes=(['b', ...], ['b', ...]),      # Map first axes of both inputs to 'b'\n                out_axes=['b', ...])(batch_A, batch_E) # Place 'b' as the first positional axis in the result\nfor i in range(5):\n  np.testing.assert_allclose(batch_AE[i], f(batch_A[i], batch_E[i]))\n\n# All-pairs of As and Es\nbatch_A = jnp.ones((7, 3, 3), dtype=np.float32)\nbatch_E = jnp.ones((5, 3, 3), dtype=np.float32)\nbatch_AE = xmap(f,\n                in_axes=(['ba', ...], ['be', ...]),           # Map first axes of inputs to 'ba' and 'be' respectively\n                out_axes=['ba', 'be', ...])(batch_A, batch_E) # Prefix all positional dimensions of output with 'ba' and 'be'\nfor i in range(7):\n  for j in range(5):\n    np.testing.assert_allclose(batch_AE[i,j], f(batch_A[i], batch_E[j])) \n```", "```py\ndef named_broadcasting(\n    x: f32[(2, 1, 1), {'a': 2}],\n    y: f32[(1, 3, 1), {'b': 3}],\n    z: f32[(1, 1, 5), {'c': 5}]) \\\n      -> f32[(2, 3, 5), {'a': 2, 'b': 3, 'c': 5}]:\n  i: f32[(2, 3, 1), {'a': 2, 'b': 3}] = x + y\n  j: f32[(1, 3, 5), {'b': 3, 'c': 5}] = y + z\n  k: f32[(2, 3, 5), {'a': 2, 'b': 3, 'c': 5}] = i + j\n  return k\n\nx = jnp.ones((2, 2, 1, 1), dtype=np.float32)\ny = jnp.ones((3, 1, 3, 1), dtype=np.float32)\nz = jnp.ones((5, 1, 1, 5), dtype=np.float32)\nk = xmap(named_broadcasting,\n         in_axes=(['a', ...], ['b', ...], ['c', ...]),\n         out_axes=['a', 'b', 'c', ...])(x, y, z)\nassert k.shape == (2, 3, 5, 2, 3, 5) \n```", "```py\ndef named_broadcast_and_reduce(\n    x: f32[(), {'x': 2}],\n    y: f32[(5,), {'y': 4}]) \\\n      -> f32[()]:\n  z: f32[(5,), {'x': 2, 'y': 4}] = x + y\n  w: f32[()] = jnp.sum(z, axis=(0, 'x', 'y'))\n  # We could also reduce in steps:\n  # w0 : f32[(), {'x': 2, 'y': 4}] = jnp.sum(z, 0)      # eliminate the positional axis\n  # w0x: f32[(), {'y': 4}]         = jnp.sum(w0, 'x')   # eliminate the `x` axis\n  # w  : f32[()]                   = jnp.sum(w0x, 'y')  # eliminate the `y` axis\n  return w\n\npositional_broadcast_and_reduce: Callable[[f32[(2,)], f32[(5, 4)]], f32[()]]\npositional_broadcast_and_reduce = \\\n  xmap(named_broadcast_and_reduce,\n       in_axes=({0: 'x'}, {1: 'y'}),\n       out_axes={})\npositional_broadcast_and_reduce(jnp.arange(2, dtype=np.float32),\n                                jnp.arange(20, dtype=np.float32).reshape((5, 4))) \n```", "```py\ndef named_batch_matrix_single_matrix(\n    x: f32[(5,), {'b': 20, 'k': 7}],\n    y: f32[(), {'k': 7, 'm': 11}]) \\\n      -> f32[(5,), {'b': 20, 'm': 11}]:\n  return jnp.einsum('n{b,k},{k,m}->n{b,m}', x, y)\n\nx = jnp.ones((20, 5, 7))\ny = jnp.ones((7, 11))\nz = jnp.einsum('bnk,km->bnm', x, y)\nzx = xmap(named_batch_matrix_single_matrix,\n          in_axes=[{0: 'b', 2: 'k'}, ['k', 'm', ...]],\n          out_axes={0: 'b', 2: 'm'})(x, y)\nnp.testing.assert_allclose(z, zx) \n```", "```py\nx = jnp.arange(8)\nxmap(lambda x: lax.pshuffle(x, 'i', list(reversed(range(8)))),\n     in_axes=['i', ...], out_axes=['i', ...])(x) \n```", "```py\nx = jnp.ones((2048, 2048))\n\nlocal_matmul = xmap(jnp.vdot,\n                    in_axes=({0: 'left'}, {1: 'right'}),\n                    out_axes=['left', 'right', ...])\ndistr_matmul = xmap(jnp.vdot,\n                    in_axes=({0: 'left'}, {1: 'right'}),\n                    out_axes=['left', 'right', ...],\n                    axis_resources={'left': 'x', 'right': 'y'}) \n```", "```py\naxis_names = ('x', 'y')\nmesh_devices = np.array(jax.devices()).reshape((2, 4))\nassert len(axis_names) == mesh_devices.ndim\nmesh_def = (mesh_devices, axis_names)\nmesh_def \n```", "```py\ntry:\n  distr_matmul(x, x)\nexcept Exception as e:\n  print(e) \n```", "```py\nfrom jax.sharding import Mesh\n\nlocal = local_matmul(x, x)  # The local function doesn't require the mesh definition\nwith Mesh(*mesh_def):  # Makes the mesh axis names available as resources\n  distr = distr_matmul(x, x)\nnp.testing.assert_allclose(local, distr) \n```", "```py\ndef sum_two_args(x: f32[(), {'a': 4}], y: f32[(), {'b': 12}]) -> f32[()]:\n  return jnp.sum(x, axis='a') + jnp.sum(y, axis='b')\n\nq = jnp.ones((4,), dtype=np.float32)\nu = jnp.ones((12,), dtype=np.float32)\nwith Mesh(np.array(jax.devices()[:4]), ('x',)):\n  v = xmap(sum_two_args,\n           in_axes=(['a', ...], ['b', ...]),\n           out_axes=[...],\n           axis_resources={'a': 'x', 'b': 'x'})(q, u)\n  print(v) \n```"]