["```py\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random \n```", "```py\n# A helper function to randomly initialize weights and biases\n# for a dense neural network layer\ndef random_layer_params(m, n, key, scale=1e-2):\n  w_key, b_key = random.split(key)\n  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n\n# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\ndef init_network_params(sizes, key):\n  keys = random.split(key, len(sizes))\n  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n\nlayer_sizes = [784, 512, 512, 10]\nstep_size = 0.01\nnum_epochs = 8\nbatch_size = 128\nn_targets = 10\nparams = init_network_params(layer_sizes, random.key(0)) \n```", "```py\nfrom jax.scipy.special import logsumexp\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  # per-example predictions\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits - logsumexp(logits) \n```", "```py\n# This works on single examples\nrandom_flattened_image = random.normal(random.key(1), (28 * 28,))\npreds = predict(params, random_flattened_image)\nprint(preds.shape) \n```", "```py\n(10,) \n```", "```py\n# Doesn't work with a batch\nrandom_flattened_images = random.normal(random.key(1), (10, 28 * 28))\ntry:\n  preds = predict(params, random_flattened_images)\nexcept TypeError:\n  print('Invalid shapes!') \n```", "```py\nInvalid shapes! \n```", "```py\n# Let's upgrade it to handle batches using `vmap`\n\n# Make a batched version of the `predict` function\nbatched_predict = vmap(predict, in_axes=(None, 0))\n\n# `batched_predict` has the same call signature as `predict`\nbatched_preds = batched_predict(params, random_flattened_images)\nprint(batched_preds.shape) \n```", "```py\n(10, 10) \n```", "```py\ndef one_hot(x, k, dtype=jnp.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n\ndef accuracy(params, images, targets):\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\n\n@jit\ndef update(params, x, y):\n  grads = grad(loss)(params, x, y)\n  return [(w - step_size * dw, b - step_size * db)\n          for (w, b), (dw, db) in zip(params, grads)] \n```", "```py\n!pip  install  torch  torchvision \n```", "```py\nRequirement already satisfied: torch in /opt/anaconda3/lib/python3.7/site-packages (1.4.0)\nRequirement already satisfied: torchvision in /opt/anaconda3/lib/python3.7/site-packages (0.5.0)\nRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from torchvision) (1.17.2)\nRequirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from torchvision) (1.12.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/anaconda3/lib/python3.7/site-packages (from torchvision) (6.2.0) \n```", "```py\nimport numpy as np\nfrom jax.tree_util import tree_map\nfrom torch.utils import data\nfrom torchvision.datasets import MNIST\n\ndef numpy_collate(batch):\n  return tree_map(np.asarray, data.default_collate(batch))\n\nclass NumpyLoader(data.DataLoader):\n  def __init__(self, dataset, batch_size=1,\n                shuffle=False, sampler=None,\n                batch_sampler=None, num_workers=0,\n                pin_memory=False, drop_last=False,\n                timeout=0, worker_init_fn=None):\n    super(self.__class__, self).__init__(dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        sampler=sampler,\n        batch_sampler=batch_sampler,\n        num_workers=num_workers,\n        collate_fn=numpy_collate,\n        pin_memory=pin_memory,\n        drop_last=drop_last,\n        timeout=timeout,\n        worker_init_fn=worker_init_fn)\n\nclass FlattenAndCast(object):\n  def __call__(self, pic):\n    return np.ravel(np.array(pic, dtype=jnp.float32)) \n```", "```py\n# Define our dataset, using torch datasets\nmnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\ntraining_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0) \n```", "```py\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\nProcessing...\nDone! \n```", "```py\n# Get the full train dataset (for checking accuracy while training)\ntrain_images = np.array(mnist_dataset.train_data).reshape(len(mnist_dataset.train_data), -1)\ntrain_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)\n\n# Get full test dataset\nmnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)\ntest_images = jnp.array(mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1), dtype=jnp.float32)\ntest_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets) \n```", "```py\n/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n  warnings.warn(\"train_labels has been renamed targets\")\n/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n  warnings.warn(\"test_data has been renamed data\")\n/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n  warnings.warn(\"test_labels has been renamed targets\") \n```", "```py\nimport time\n\nfor epoch in range(num_epochs):\n  start_time = time.time()\n  for x, y in training_generator:\n    y = one_hot(y, n_targets)\n    params = update(params, x, y)\n  epoch_time = time.time() - start_time\n\n  train_acc = accuracy(params, train_images, train_labels)\n  test_acc = accuracy(params, test_images, test_labels)\n  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n  print(\"Training set accuracy {}\".format(train_acc))\n  print(\"Test set accuracy {}\".format(test_acc)) \n```", "```py\nEpoch 0 in 55.15 sec\nTraining set accuracy 0.9157500267028809\nTest set accuracy 0.9195000529289246\nEpoch 1 in 42.26 sec\nTraining set accuracy 0.9372166991233826\nTest set accuracy 0.9384000301361084\nEpoch 2 in 44.37 sec\nTraining set accuracy 0.9491666555404663\nTest set accuracy 0.9469000697135925\nEpoch 3 in 41.75 sec\nTraining set accuracy 0.9568166732788086\nTest set accuracy 0.9534000158309937\nEpoch 4 in 41.16 sec\nTraining set accuracy 0.9631333351135254\nTest set accuracy 0.9577000737190247\nEpoch 5 in 38.89 sec\nTraining set accuracy 0.9675000309944153\nTest set accuracy 0.9616000652313232\nEpoch 6 in 40.68 sec\nTraining set accuracy 0.9708333611488342\nTest set accuracy 0.9650000333786011\nEpoch 7 in 41.50 sec\nTraining set accuracy 0.973716676235199\nTest set accuracy 0.9672000408172607 \n```"]