- en: Autobatching for Bayesian Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`jax.readthedocs.io/en/latest/notebooks/vmapped_log_probs.html`](https://jax.readthedocs.io/en/latest/notebooks/vmapped_log_probs.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Open in Colab](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/vmapped_log_probs.ipynb)
    ![Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/vmapped_log_probs.ipynb)'
  prefs: []
  type: TYPE_IMG
- en: This notebook demonstrates a simple Bayesian inference example where autobatching
    makes user code easier to write, easier to read, and less likely to include bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by a notebook by @davmre.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Generate a fake binary classification dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Write the log-joint function for the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll write a non-batched version, a manually batched version, and an autobatched
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Non-batched
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Manually batched
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Autobatched with vmap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It just works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Self-contained variational inference example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A little code is copied from above.
  prefs: []
  type: TYPE_NORMAL
- en: Set up the (batched) log-joint function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Define the ELBO and its gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Optimize the ELBO using SGD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Display the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coverage isn’t quite as good as we might like, but it’s not bad, and nobody
    said variational inference was exact.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![../_images/f3f380106b7365b483cc90c02f9030fe13977e2a0e954dfada1276bb3d3e0444.png](img/6a11d4a300017440a27b6c2c06c1e0ee.png)'
  prefs: []
  type: TYPE_IMG
