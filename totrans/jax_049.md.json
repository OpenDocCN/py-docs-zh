["```py\nimport os\n\nimport functools\nfrom typing import Optional\n\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp \n```", "```py\nif len(jax.local_devices()) < 8:\n  raise Exception(\"Notebook requires 8 devices to run\") \n```", "```py\nfrom jax.experimental import mesh_utils\nfrom jax.sharding import PositionalSharding \n```", "```py\n# Create a Sharding object to distribute a value across devices:\nsharding = PositionalSharding(mesh_utils.create_device_mesh((8,))) \n```", "```py\n# Create an array of random values:\nx = jax.random.normal(jax.random.key(0), (8192, 8192))\n# and use jax.device_put to distribute it across devices:\ny = jax.device_put(x, sharding.reshape(4, 2))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\nz = jnp.sin(y)\njax.debug.visualize_array_sharding(z) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\n# `x` is present on a single device\n%timeit -n 5 -r 5 jnp.sin(x).block_until_ready() \n```", "```py\nThe slowest run took 13.32 times longer than the fastest. This could mean that an intermediate result is being cached \n5 loops, best of 5: 9.69 ms per loop \n```", "```py\n# `y` is sharded across 8 devices.\n%timeit -n 5 -r 5 jnp.sin(y).block_until_ready() \n```", "```py\n5 loops, best of 5: 1.86 ms per loop \n```", "```py\nimport jax\nx = jax.random.normal(jax.random.key(0), (8192, 8192)) \n```", "```py\njax.debug.visualize_array_sharding(x) \n```", "```py\n┌───────────────────────┐\n│                       │\n│                       │\n│                       │\n│                       │\n│         TPU 0         │\n│                       │\n│                       │\n│                       │\n│                       │\n└───────────────────────┘ \n```", "```py\nfrom jax.experimental import mesh_utils\ndevices = mesh_utils.create_device_mesh((8,)) \n```", "```py\nfrom jax.sharding import PositionalSharding\n\nsharding = PositionalSharding(devices)\n\nx = jax.device_put(x, sharding.reshape(8, 1))\njax.debug.visualize_array_sharding(x) \n```", "```py\n┌───────────────────────┐\n│         TPU 0         │\n├───────────────────────┤\n│         TPU 1         │\n├───────────────────────┤\n│         TPU 2         │\n├───────────────────────┤\n│         TPU 3         │\n├───────────────────────┤\n│         TPU 6         │\n├───────────────────────┤\n│         TPU 7         │\n├───────────────────────┤\n│         TPU 4         │\n├───────────────────────┤\n│         TPU 5         │\n└───────────────────────┘ \n```", "```py\nsharding \n```", "```py\nPositionalSharding([{TPU 0} {TPU 1} {TPU 2} {TPU 3} {TPU 6} {TPU 7} {TPU 4} {TPU 5}]) \n```", "```py\nsharding.reshape(8, 1) \n```", "```py\nPositionalSharding([[{TPU 0}]\n                    [{TPU 1}]\n                    [{TPU 2}]\n                    [{TPU 3}]\n                    [{TPU 6}]\n                    [{TPU 7}]\n                    [{TPU 4}]\n                    [{TPU 5}]]) \n```", "```py\nsharding.reshape(4, 2) \n```", "```py\nPositionalSharding([[{TPU 0} {TPU 1}]\n                    [{TPU 2} {TPU 3}]\n                    [{TPU 6} {TPU 7}]\n                    [{TPU 4} {TPU 5}]]) \n```", "```py\ndef is_congruent(x_shape: Sequence[int], sharding_shape: Sequence[int]) -> bool:\n  return (len(x_shape) == len(sharding_shape) and\n          all(d1 % d2 == 0 for d1, d2 in zip(x_shape, sharding_shape))) \n```", "```py\nsharding = sharding.reshape(4, 2)\nprint(sharding) \n```", "```py\nPositionalSharding([[{TPU 0} {TPU 1}]\n                    [{TPU 2} {TPU 3}]\n                    [{TPU 6} {TPU 7}]\n                    [{TPU 4} {TPU 5}]]) \n```", "```py\ny = jax.device_put(x, sharding)\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\nsharding = sharding.reshape(1, 8)\nprint(sharding) \n```", "```py\nPositionalSharding([[{TPU 0} {TPU 1} {TPU 2} {TPU 3} {TPU 6} {TPU 7} {TPU 4} {TPU 5}]]) \n```", "```py\ny = jax.device_put(x, sharding)\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n│       │       │       │       │       │       │       │       │\n│       │       │       │       │       │       │       │       │\n│       │       │       │       │       │       │       │       │\n│       │       │       │       │       │       │       │       │\n│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │\n│       │       │       │       │       │       │       │       │\n│       │       │       │       │       │       │       │       │\n│       │       │       │       │       │       │       │       │\n│       │       │       │       │       │       │       │       │\n└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ \n```", "```py\nsharding = sharding.reshape(4, 2)\nprint(sharding.replicate(axis=0, keepdims=True)) \n```", "```py\nPositionalSharding([[{TPU 0, 2, 4, 6} {TPU 1, 3, 5, 7}]]) \n```", "```py\ny = jax.device_put(x, sharding.replicate(axis=0, keepdims=True))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────────┬───────────┐\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n│TPU 0,2,4,6│TPU 1,3,5,7│\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n└───────────┴───────────┘ \n```", "```py\nprint(sharding.replicate(0).shape)\nprint(sharding.replicate(1).shape) \n```", "```py\n(1, 2)\n(4, 1) \n```", "```py\ny = jax.device_put(x, sharding.replicate(1))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────────────────────┐\n│        TPU 0,1        │\n├───────────────────────┤\n│        TPU 2,3        │\n├───────────────────────┤\n│        TPU 6,7        │\n├───────────────────────┤\n│        TPU 4,5        │\n└───────────────────────┘ \n```", "```py\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec\nfrom jax.sharding import NamedSharding\nfrom jax.experimental import mesh_utils\n\nP = PartitionSpec\n\ndevices = mesh_utils.create_device_mesh((4, 2))\nmesh = Mesh(devices, axis_names=('a', 'b'))\ny = jax.device_put(x, NamedSharding(mesh, P('a', 'b')))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\ndevices = mesh_utils.create_device_mesh((4, 2))\ndefault_mesh = Mesh(devices, axis_names=('a', 'b'))\n\ndef mesh_sharding(\n    pspec: PartitionSpec, mesh: Optional[Mesh] = None,\n  ) -> NamedSharding:\n  if mesh is None:\n    mesh = default_mesh\n  return NamedSharding(mesh, pspec) \n```", "```py\ny = jax.device_put(x, mesh_sharding(P('a', 'b')))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\ny = jax.device_put(x, mesh_sharding(P('b', 'a')))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────┬───────┬───────┬───────┐\n│       │       │       │       │\n│ TPU 0 │ TPU 2 │ TPU 6 │ TPU 4 │\n│       │       │       │       │\n│       │       │       │       │\n├───────┼───────┼───────┼───────┤\n│       │       │       │       │\n│ TPU 1 │ TPU 3 │ TPU 7 │ TPU 5 │\n│       │       │       │       │\n│       │       │       │       │\n└───────┴───────┴───────┴───────┘ \n```", "```py\n# This `None` means that `x` is not sharded on its second dimension,\n# and since the Mesh axis name 'b' is not mentioned, shards are\n# replicated across it.\ny = jax.device_put(x, mesh_sharding(P('a', None)))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────────────────────┐\n│        TPU 0,1        │\n├───────────────────────┤\n│        TPU 2,3        │\n├───────────────────────┤\n│        TPU 6,7        │\n├───────────────────────┤\n│        TPU 4,5        │\n└───────────────────────┘ \n```", "```py\ny = jax.device_put(x, mesh_sharding(P(None, 'b')))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────────┬───────────┐\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n│TPU 0,2,4,6│TPU 1,3,5,7│\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n└───────────┴───────────┘ \n```", "```py\ny = jax.device_put(x, mesh_sharding(P(None, 'a')))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────┬───────┬───────┬───────┐\n│       │       │       │       │\n│       │       │       │       │\n│       │       │       │       │\n│       │       │       │       │\n│TPU 0,1│TPU 2,3│TPU 6,7│TPU 4,5│\n│       │       │       │       │\n│       │       │       │       │\n│       │       │       │       │\n│       │       │       │       │\n└───────┴───────┴───────┴───────┘ \n```", "```py\ny = jax.device_put(x, mesh_sharding(P(('a', 'b'), None)))\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌───────────────────────┐\n│         TPU 0         │\n├───────────────────────┤\n│         TPU 1         │\n├───────────────────────┤\n│         TPU 2         │\n├───────────────────────┤\n│         TPU 3         │\n├───────────────────────┤\n│         TPU 6         │\n├───────────────────────┤\n│         TPU 7         │\n├───────────────────────┤\n│         TPU 4         │\n├───────────────────────┤\n│         TPU 5         │\n└───────────────────────┘ \n```", "```py\nfrom jax.experimental import mesh_utils\nfrom jax.sharding import PositionalSharding\nsharding = PositionalSharding(mesh_utils.create_device_mesh((8,))) \n```", "```py\nx = jax.device_put(x, sharding.reshape(4, 2))\nprint('input sharding:')\njax.debug.visualize_array_sharding(x)\n\ny = jnp.sin(x)\nprint('output sharding:')\njax.debug.visualize_array_sharding(y) \n```", "```py\ninput sharding:\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘\noutput sharding:\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\ny = jax.device_put(x, sharding.reshape(4, 2).replicate(1))\nz = jax.device_put(x, sharding.reshape(4, 2).replicate(0))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)\nprint('out sharding:')\njax.debug.visualize_array_sharding(w) \n```", "```py\nlhs sharding:\n┌───────────────────────┐\n│        TPU 0,1        │\n├───────────────────────┤\n│        TPU 2,3        │\n├───────────────────────┤\n│        TPU 6,7        │\n├───────────────────────┤\n│        TPU 4,5        │\n└───────────────────────┘\nrhs sharding:\n┌───────────┬───────────┐\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n│TPU 0,2,4,6│TPU 1,3,5,7│\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n└───────────┴───────────┘\nout sharding:\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\nx_single = jax.device_put(x, jax.devices()[0])\njax.debug.visualize_array_sharding(x_single) \n```", "```py\n┌───────────────────────┐\n│                       │\n│                       │\n│                       │\n│                       │\n│         TPU 0         │\n│                       │\n│                       │\n│                       │\n│                       │\n└───────────────────────┘ \n```", "```py\nnp.allclose(jnp.dot(x_single, x_single),\n            jnp.dot(y, z)) \n```", "```py\nTrue \n```", "```py\n%timeit -n 5 -r 5 jnp.dot(x_single, x_single).block_until_ready() \n```", "```py\n5 loops, best of 5: 19.3 ms per loop \n```", "```py\n%timeit -n 5 -r 5 jnp.dot(y, z).block_until_ready() \n```", "```py\n5 loops, best of 5: 3.25 ms per loop \n```", "```py\nw_copy = jnp.copy(w)\njax.debug.visualize_array_sharding(w_copy) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘ \n```", "```py\nimport textwrap\nfrom termcolor import colored\n\ndef print_exception(e):\n  name = colored(f'{type(e).__name__}', 'red')\n  print(textwrap.fill(f'{name}: {str(e)}')) \n```", "```py\nsharding1 = PositionalSharding(jax.devices()[:4])\nsharding2 = PositionalSharding(jax.devices()[4:])\n\ny = jax.device_put(x, sharding1.reshape(2, 2))\nz = jax.device_put(x, sharding2.reshape(2, 2))\ntry: y + z\nexcept ValueError as e: print_exception(e) \n```", "```py\nValueError: Devices of all `Array` inputs and outputs should\nbe the same. Got array device ids [0, 1, 2, 3] on platform TPU and\nanother array's device ids [4, 5, 6, 7] on platform TPU \n```", "```py\ndevices = jax.devices()\npermuted_devices = [devices[i] for i in [0, 1, 2, 3, 6, 7, 4, 5]]\n\nsharding1 = PositionalSharding(devices)\nsharding2 = PositionalSharding(permuted_devices)\n\ny = jax.device_put(x, sharding1.reshape(4, 2))\nz = jax.device_put(x, sharding2.reshape(4, 2))\ntry: y + z\nexcept ValueError as e: print_exception(e) \n```", "```py\nValueError: Devices of all `Array` inputs and outputs should\nbe the same. Got array device ids [0, 1, 2, 3, 4, 5, 6, 7] on platform\nTPU and another array's device ids [0, 1, 2, 3, 6, 7, 4, 5] on\nplatform TPU \n```", "```py\ny = jax.device_put(x, sharding1.reshape(4, 2))\ny + jnp.ones_like(y)\ny + jnp.arange(y.size).reshape(y.shape)\nprint('no error!') \n```", "```py\nno error! \n```", "```py\nsharding = PositionalSharding(mesh_utils.create_device_mesh((8,))) \n```", "```py\nx = jax.random.normal(jax.random.key(0), (8192, 8192))\nx = jax.device_put(x, sharding.reshape(4, 2)) \n```", "```py\n@jax.jit\ndef f(x):\n  x = x + 1\n  y = jax.lax.with_sharding_constraint(x, sharding.reshape(2, 4))\n  return y \n```", "```py\njax.debug.visualize_array_sharding(x)\ny = f(x)\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘\n┌───────┬───────┬───────┬───────┐\n│       │       │       │       │\n│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │\n│       │       │       │       │\n│       │       │       │       │\n├───────┼───────┼───────┼───────┤\n│       │       │       │       │\n│ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │\n│       │       │       │       │\n│       │       │       │       │\n└───────┴───────┴───────┴───────┘ \n```", "```py\n@jax.jit\ndef f(x):\n  x = x + 1\n  y = jax.lax.with_sharding_constraint(x, sharding.replicate())\n  return y \n```", "```py\njax.debug.visualize_array_sharding(x)\ny = f(x)\njax.debug.visualize_array_sharding(y) \n```", "```py\n┌──────────┬──────────┐\n│  TPU 0   │  TPU 1   │\n├──────────┼──────────┤\n│  TPU 2   │  TPU 3   │\n├──────────┼──────────┤\n│  TPU 6   │  TPU 7   │\n├──────────┼──────────┤\n│  TPU 4   │  TPU 5   │\n└──────────┴──────────┘\n┌───────────────────────┐\n│                       │\n│                       │\n│                       │\n│                       │\n│  TPU 0,1,2,3,4,5,6,7  │\n│                       │\n│                       │\n│                       │\n│                       │\n└───────────────────────┘ \n```", "```py\nimport jax\nimport jax.numpy as jnp \n```", "```py\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.maximum(outputs, 0)\n  return outputs\n\ndef loss(params, batch):\n  inputs, targets = batch\n  predictions = predict(params, inputs)\n  return jnp.mean(jnp.sum((predictions - targets)**2, axis=-1)) \n```", "```py\nloss_jit = jax.jit(loss)\ngradfun = jax.jit(jax.grad(loss)) \n```", "```py\ndef init_layer(key, n_in, n_out):\n    k1, k2 = jax.random.split(key)\n    W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)\n    b = jax.random.normal(k2, (n_out,))\n    return W, b\n\ndef init_model(key, layer_sizes, batch_size):\n    key, *keys = jax.random.split(key, len(layer_sizes))\n    params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n\n    key, *keys = jax.random.split(key, 3)\n    inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n    targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))\n\n    return params, (inputs, targets)\n\nlayer_sizes = [784, 8192, 8192, 8192, 10]\nbatch_size = 8192\n\nparams, batch = init_model(jax.random.key(0), layer_sizes, batch_size) \n```", "```py\nsharding = PositionalSharding(jax.devices()).reshape(8, 1) \n```", "```py\nbatch = jax.device_put(batch, sharding)\nparams = jax.device_put(params, sharding.replicate()) \n```", "```py\nloss_jit(params, batch) \n```", "```py\nArray(23.469475, dtype=float32) \n```", "```py\nstep_size = 1e-5\n\nfor _ in range(30):\n  grads = gradfun(params, batch)\n  params = [(W - step_size * dW, b - step_size * db)\n            for (W, b), (dW, db) in zip(params, grads)]\n\nprint(loss_jit(params, batch)) \n```", "```py\n10.760101 \n```", "```py\n%timeit -n 5 -r 5 gradfun(params, batch)[0][0].block_until_ready() \n```", "```py\n5 loops, best of 5: 26.3 ms per loop \n```", "```py\nbatch_single = jax.device_put(batch, jax.devices()[0])\nparams_single = jax.device_put(params, jax.devices()[0]) \n```", "```py\n%timeit -n 5 -r 5 gradfun(params_single, batch_single)[0][0].block_until_ready() \n```", "```py\n5 loops, best of 5: 122 ms per loop \n```", "```py\nsharding = sharding.reshape(4, 2) \n```", "```py\nbatch = jax.device_put(batch, sharding.replicate(1))\njax.debug.visualize_array_sharding(batch[0])\njax.debug.visualize_array_sharding(batch[1]) \n```", "```py\n┌───────┐\n│TPU 0,1│\n├───────┤\n│TPU 2,3│\n├───────┤\n│TPU 4,5│\n├───────┤\n│TPU 6,7│\n└───────┘\n┌───────┐\n│TPU 0,1│\n├───────┤\n│TPU 2,3│\n├───────┤\n│TPU 4,5│\n├───────┤\n│TPU 6,7│\n└───────┘ \n```", "```py\n(W1, b1), (W2, b2), (W3, b3), (W4, b4) = params\n\nW1 = jax.device_put(W1, sharding.replicate())\nb1 = jax.device_put(b1, sharding.replicate())\n\nW2 = jax.device_put(W2, sharding.replicate(0))\nb2 = jax.device_put(b2, sharding.replicate(0))\n\nW3 = jax.device_put(W3, sharding.replicate(0).T)\nb3 = jax.device_put(b3, sharding.replicate())\n\nW4 = jax.device_put(W4, sharding.replicate())\nb4 = jax.device_put(b4, sharding.replicate())\n\nparams = (W1, b1), (W2, b2), (W3, b3), (W4, b4) \n```", "```py\njax.debug.visualize_array_sharding(W2) \n```", "```py\n┌───────────┬───────────┐\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n│TPU 0,2,4,6│TPU 1,3,5,7│\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n└───────────┴───────────┘ \n```", "```py\njax.debug.visualize_array_sharding(W3) \n```", "```py\n┌───────────────────────┐\n│                       │\n│      TPU 0,2,4,6      │\n│                       │\n│                       │\n├───────────────────────┤\n│                       │\n│      TPU 1,3,5,7      │\n│                       │\n│                       │\n└───────────────────────┘ \n```", "```py\nprint(loss_jit(params, batch)) \n```", "```py\n10.760103 \n```", "```py\nstep_size = 1e-5\n\nfor _ in range(30):\n    grads = gradfun(params, batch)\n    params = [(W - step_size * dW, b - step_size * db)\n              for (W, b), (dW, db) in zip(params, grads)] \n```", "```py\nprint(loss_jit(params, batch)) \n```", "```py\n10.752466 \n```", "```py\n(W1, b1), (W2, b2), (W3, b3), (W4, b4) = params\njax.debug.visualize_array_sharding(W2)\njax.debug.visualize_array_sharding(W3) \n```", "```py\n┌───────────┬───────────┐\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n│TPU 0,2,4,6│TPU 1,3,5,7│\n│           │           │\n│           │           │\n│           │           │\n│           │           │\n└───────────┴───────────┘\n┌───────────────────────┐\n│                       │\n│      TPU 0,2,4,6      │\n│                       │\n│                       │\n├───────────────────────┤\n│                       │\n│      TPU 1,3,5,7      │\n│                       │\n│                       │\n└───────────────────────┘ \n```", "```py\n%timeit -n 10 -r 10 gradfun(params, batch)[0][0].block_until_ready() \n```", "```py\n10 loops, best of 10: 30.5 ms per loop \n```", "```py\n@jax.jit\ndef f(key, x):\n  numbers = jax.random.uniform(key, x.shape)\n  return x + numbers\n\nkey = jax.random.key(42)\nx_sharding = jax.sharding.PositionalSharding(jax.devices())\nx = jax.device_put(jnp.arange(24), x_sharding) \n```", "```py\njax.debug.visualize_array_sharding(f(key, x)) \n```", "```py\n┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │\n└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ \n```", "```py\nf_exe = f.lower(key, x).compile()\nprint('Communicating?', 'collective-permute' in f_exe.as_text()) \n```", "```py\nCommunicating? True \n```", "```py\njax.config.update('jax_threefry_partitionable', True)\nf_exe = f.lower(key, x).compile()\nprint('Communicating?', 'collective-permute' in f_exe.as_text()) \n```", "```py\nCommunicating? False \n```", "```py\njax.debug.visualize_array_sharding(f(key, x)) \n```", "```py\n┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │\n└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ \n```", "```py\njax.config.update('jax_threefry_partitionable', False)\nprint('Stable:')\nprint(f(key, x))\nprint()\n\njax.config.update('jax_threefry_partitionable', True)\nprint('Partitionable:')\nprint(f(key, x)) \n```", "```py\nStable:\n[ 0.72503686  1.8532515   2.983416    3.083253    4.0332246   5.4782867\n  6.1720605   7.6900277   8.602836    9.810046   10.861367   11.907651\n 12.330483   13.456195   14.808557   15.960099   16.067581   17.739723\n 18.335474   19.46401    20.390276   21.116539   22.858128   23.223194  ]\n\nPartitionable:\n[ 0.48870957  1.6797972   2.6162715   3.561016    4.4506445   5.585866\n  6.0748096   7.775133    8.698959    9.818634   10.350306   11.87282\n 12.925881   13.86013    14.477554   15.818481   16.711355   17.586697\n 18.073738   19.777622   20.404566   21.119123   22.026257   23.63918   ] \n```"]