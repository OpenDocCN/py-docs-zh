- en: GPU performance tips
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`jax.readthedocs.io/en/latest/gpu_performance_tips.html`](https://jax.readthedocs.io/en/latest/gpu_performance_tips.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This document focuses on performance tips for neural network workloads
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Matmul precision
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On recent GPU generations, such as the Nvidia A100 generation or later, it
    can be a good idea to perform most computations in `bfloat16` precision. For example,
    if using [Flax](https://github.com/google/flax), instantiate `Dense` layers using
    `flax.linen.Dense(..., dtype=jax.numpy.bfloat16)`. Here are some code examples:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In the [Flax LM1B example](https://github.com/google/flax/tree/main/examples/lm1b),
    `Dense` modules are [instantiated with a configurable dtype](https://github.com/google/flax/blob/fd8fd76a4af5307a61f85bac98feab9b26d60db8/examples/lm1b/models.py#L188)
    which [defaults](https://github.com/google/flax/blob/fd8fd76a4af5307a61f85bac98feab9b26d60db8/examples/lm1b/configs/default.py#L112)
    to [bfloat16](https://github.com/google/flax/blob/c0087535d7f2e5bfcbf2a7be6825b9f5055a54c6/examples/lm1b/train.py#L431).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [MaxText](https://github.com/google/maxtext), `DenseGeneral` modules are
    also [instantiated with a configurable dtype](https://github.com/google/maxtext/blob/07dc6ce27ced1246407d0de311d4a0d6a9fd46d8/MaxText/layers.py#L592)
    that [defaults to bfloat16](https://github.com/google/maxtext/blob/07dc6ce27ced1246407d0de311d4a0d6a9fd46d8/MaxText/configs/base.yml#L41).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XLA performance flags
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: JAX-Toolbox also has a page on [NVIDIA XLA performance FLAGS](https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/docs/GPU_performance.md).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The existence and exact behavior of XLA flags may be `jaxlib`-version dependent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: As of `jaxlib==0.4.18` (released [Oct 6 2023](https://pypi.org/project/jaxlib/#history)),
    setting these XLA flags can improve performance. Some are related to communication
    between GPUs, and so are only relevant when running computations on multiple devices,
    while others are related to code generation on each device.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Some of these may be set by default in future releases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'These flags can be set via the `XLA_FLAGS` shell environment variable. For
    example, we can add this to the top of a Python file:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For more examples, see also [XLA Flags recommended for Pax training on Nvidia
    GPUs](https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/rosetta/projects/pax/README.md#xla-flags).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Code generation flags
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**–xla_gpu_enable_triton_softmax_fusion** This flag enables an automatic softmax
    fusion, based on pattern-matching backed by Triton code generation. The default
    value is False.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**–xla_gpu_triton_gemm_any** Use the Triton-based GEMM (matmul) emitter for
    any GEMM that it supports. The default value is False.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication flags
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**–xla_gpu_enable_async_collectives** This flag enables the collective ops
    such as `AllReduce`, `AllGather`, `ReduceScatter` and `CollectivePermute` to be
    asynchronous. Asynchronous communication can overlap cross-core communication
    with computation. The default value is False.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**–xla_gpu_enable_latency_hiding_scheduler** This flag enables latency hiding
    schedulers to overlap asynchronous communication with computation efficiently.
    The default value is False.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**–xla_gpu_enable_latency_hiding_scheduler** 这个标志启用了延迟隐藏调度器，可以高效地将异步通信与计算重叠。默认值为
    False。'
- en: '**–xla_gpu_enable_pipelined_collectives** When using pipeline parallelism,
    this flag enables overlapping the (i+1)-th layer weight `AllGather` with the i-th
    layer computation. It also enables overlapping (i+1)-th layer weight `Reduce`/`ReduceScatter`
    with i-th layer’s computation. The default value is False. **There are some bugs
    when this flag is turned on.**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**–xla_gpu_enable_pipelined_collectives** 在使用管道并行时，此标志允许将(i+1)层权重的`AllGather`与第
    i 层的计算重叠。它还允许将(i+1)层权重的`Reduce`/`ReduceScatter`与第 i 层的计算重叠。默认值为 False。**在启用此标志时存在一些错误。**'
- en: '**–xla_gpu_collective_permute_decomposer_threshold** This flag is useful when
    performing [GSPMD pipelining](https://arxiv.org/abs/2105.04663). Setting a nonzero
    threshold decomposes `CollectivePermute`s into `CollectivePermuteReceiveDone`
    and `CollectivePermuteSendDone` pairs, so that computation can be performed between
    each corresponding `ReceiveDone`/`SendDone` pair and hence achieve more overlap.
    By default the threshold is 0 and there is no decomposition. Setting it to threshold
    > 0 such as `--xla_gpu_collective_permute_decomposer_threshold=1024` can enable
    this feature.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**–xla_gpu_collective_permute_decomposer_threshold** 当执行[GSPMD pipelining](https://arxiv.org/abs/2105.04663)时，这个标志非常有用。设置一个非零的阈值会将`CollectivePermute`分解为`CollectivePermuteReceiveDone`和`CollectivePermuteSendDone`对，从而可以在每个对应的`ReceiveDone`/`SendDone`对之间执行计算，从而实现更多的重叠。默认阈值为
    0，不进行分解。将其设置为大于 0 的阈值，例如`--xla_gpu_collective_permute_decomposer_threshold=1024`，可以启用此功能。'
- en: '**–xla_gpu_all_gather_combine_threshold_bytes** **–xla_gpu_reduce_scatter_combine_threshold_bytes**
    **–xla_gpu_all_reduce_combine_threshold_bytes** These flags tune when to combine
    multiple small `AllGather`/`ReduceScatter`/`AllReduce` into one big `AllGather`/`ReduceScatter`/`AllReduce`
    to reduce time spent on cross-device communication. For example, for the `AllGather`/`ReduceScatter`
    thresholds on a Transformer-based workload, consider tuning them high enough so
    as to combine at least a Transformer Layer’s weight `AllGather`/`ReduceScatter`.
    By default, the `combine_threshold_bytes` is set to 256.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**–xla_gpu_all_gather_combine_threshold_bytes** **–xla_gpu_reduce_scatter_combine_threshold_bytes**
    **–xla_gpu_all_reduce_combine_threshold_bytes** 这些标志用于调整何时将多个小的`AllGather`/`ReduceScatter`/`AllReduce`组合成一个大的`AllGather`/`ReduceScatter`/`AllReduce`，以减少跨设备通信所花费的时间。例如，在基于
    Transformer 的工作负载上，可以考虑将`AllGather`/`ReduceScatter`阈值调高，以至少组合一个 Transformer 层的权重`AllGather`/`ReduceScatter`。默认情况下，`combine_threshold_bytes`设置为
    256。'
- en: NCCL flags
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NCCL 标志
- en: 'These Nvidia NCCL flag values may be useful for single-host multi-device computations
    on Nvidia GPUs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 Nvidia NCCL 标志值可能对在 Nvidia GPU 上进行单主机多设备计算有用：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These NCCL flags could improve single-host communication speed. These flags
    don’t seem useful for multi-host communication yet.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 NCCL 标志可以提高单主机通信速度。然而，这些标志对多主机通信似乎不太有用。
- en: Multi-Process
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多进程
- en: We recommand using one process per GPU and not one per node. In some cases,
    this can speed up jitted computation. The `jax.distributed.initialize()` API will
    automatically understand that configuration when run under SLURM. However, this
    only a rule of thumb and it may be useful to test both one process per GPU and
    one process per node on your use case.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议每个 GPU 使用一个进程，而不是每个节点使用一个进程。在某些情况下，这可以加速 jitted 计算。当在 SLURM 下运行时，`jax.distributed.initialize()`
    API 将自动理解此配置。然而，这只是一个经验法则，可能有必要在您的用例中测试每个 GPU 一个进程和每个节点一个进程的情况。
