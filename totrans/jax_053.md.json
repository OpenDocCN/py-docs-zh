["```py\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\n\nkey = random.key(0) \n```", "```py\ngrad_tanh = grad(jnp.tanh)\nprint(grad_tanh(2.0)) \n```", "```py\n0.070650816 \n```", "```py\nprint(grad(grad(jnp.tanh))(2.0))\nprint(grad(grad(grad(jnp.tanh)))(2.0)) \n```", "```py\n-0.13621868\n0.25265405 \n```", "```py\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12,  0.77],\n                   [0.88, -1.08, 0.15],\n                   [0.52, 0.06, -1.30],\n                   [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = random.split(key, 3)\nW = random.normal(W_key, (3,))\nb = random.normal(b_key, ()) \n```", "```py\n# Differentiate `loss` with respect to the first positional argument:\nW_grad = grad(loss, argnums=0)(W, b)\nprint('W_grad', W_grad)\n\n# Since argnums=0 is the default, this does the same thing:\nW_grad = grad(loss)(W, b)\nprint('W_grad', W_grad)\n\n# But we can choose different values too, and drop the keyword:\nb_grad = grad(loss, 1)(W, b)\nprint('b_grad', b_grad)\n\n# Including tuple values\nW_grad, b_grad = grad(loss, (0, 1))(W, b)\nprint('W_grad', W_grad)\nprint('b_grad', b_grad) \n```", "```py\nW_grad [-0.16965583 -0.8774644  -1.4901346 ]\nW_grad [-0.16965583 -0.8774644  -1.4901346 ]\nb_grad -0.29227245\nW_grad [-0.16965583 -0.8774644  -1.4901346 ]\nb_grad -0.29227245 \n```", "```py\ndef loss2(params_dict):\n    preds = predict(params_dict['W'], params_dict['b'], inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nprint(grad(loss2)({'W': W, 'b': b})) \n```", "```py\n{'W': Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32), 'b': Array(-0.29227245, dtype=float32)} \n```", "```py\nfrom jax import value_and_grad\nloss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)\nprint('loss value', loss_value)\nprint('loss value', loss(W, b)) \n```", "```py\nloss value 3.0519385\nloss value 3.0519385 \n```", "```py\n# Set a step size for finite differences calculations\neps = 1e-4\n\n# Check b_grad with scalar finite differences\nb_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\nprint('b_grad_numerical', b_grad_numerical)\nprint('b_grad_autodiff', grad(loss, 1)(W, b))\n\n# Check W_grad with finite differences in a random direction\nkey, subkey = random.split(key)\nvec = random.normal(subkey, W.shape)\nunitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\nW_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\nprint('W_dirderiv_numerical', W_grad_numerical)\nprint('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec)) \n```", "```py\nb_grad_numerical -0.29325485\nb_grad_autodiff -0.29227245\nW_dirderiv_numerical -0.2002716\nW_dirderiv_autodiff -0.19909117 \n```", "```py\nfrom jax.test_util import check_grads\ncheck_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives \n```", "```py\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x) \n```", "```py\nfrom jax import jacfwd, jacrev\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nJ = jacfwd(f)(W)\nprint(\"jacfwd result, with shape\", J.shape)\nprint(J)\n\nJ = jacrev(f)(W)\nprint(\"jacrev result, with shape\", J.shape)\nprint(J) \n```", "```py\njacfwd result, with shape (4, 3)\n[[ 0.05981758  0.12883787  0.08857603]\n [ 0.04015916 -0.04928625  0.00684531]\n [ 0.12188288  0.01406341 -0.3047072 ]\n [ 0.00140431 -0.00472531  0.00263782]]\njacrev result, with shape (4, 3)\n[[ 0.05981757  0.12883787  0.08857603]\n [ 0.04015916 -0.04928625  0.00684531]\n [ 0.12188289  0.01406341 -0.3047072 ]\n [ 0.00140431 -0.00472531  0.00263782]] \n```", "```py\ndef predict_dict(params, inputs):\n    return predict(params['W'], params['b'], inputs)\n\nJ_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\nfor k, v in J_dict.items():\n    print(\"Jacobian from {} to logits is\".format(k))\n    print(v) \n```", "```py\nJacobian from W to logits is\n[[ 0.05981757  0.12883787  0.08857603]\n [ 0.04015916 -0.04928625  0.00684531]\n [ 0.12188289  0.01406341 -0.3047072 ]\n [ 0.00140431 -0.00472531  0.00263782]]\nJacobian from b to logits is\n[0.11503381 0.04563541 0.23439017 0.00189771] \n```", "```py\ndef hessian(f):\n    return jacfwd(jacrev(f))\n\nH = hessian(f)(W)\nprint(\"hessian, with shape\", H.shape)\nprint(H) \n```", "```py\nhessian, with shape (4, 3, 3)\n[[[ 0.02285465  0.04922541  0.03384247]\n  [ 0.04922541  0.10602397  0.07289147]\n  [ 0.03384247  0.07289147  0.05011288]]\n\n [[-0.03195215  0.03921401 -0.00544639]\n  [ 0.03921401 -0.04812629  0.00668421]\n  [-0.00544639  0.00668421 -0.00092836]]\n\n [[-0.01583708 -0.00182736  0.03959271]\n  [-0.00182736 -0.00021085  0.00456839]\n  [ 0.03959271  0.00456839 -0.09898177]]\n\n [[-0.00103524  0.00348343 -0.00194457]\n  [ 0.00348343 -0.01172127  0.0065432 ]\n  [-0.00194457  0.0065432  -0.00365263]]] \n```", "```py\nfrom jax import jvp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nkey, subkey = random.split(key)\nv = random.normal(subkey, W.shape)\n\n# Push forward the vector `v` along `f` evaluated at `W`\ny, u = jvp(f, (W,), (v,)) \n```", "```py\njvp  ::  (a  ->  b)  ->  a  ->  T  a  ->  (b,  T  b) \n```", "```py\nfrom jax import vjp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\ny, vjp_fun = vjp(f, W)\n\nkey, subkey = random.split(key)\nu = random.normal(subkey, y.shape)\n\n# Pull back the covector `u` along `f` evaluated at `W`\nv = vjp_fun(u) \n```", "```py\nvjp  ::  (a  ->  b)  ->  a  ->  (b,  CT  b  ->  CT  a) \n```", "```py\nfrom jax import vjp\n\ndef vgrad(f, x):\n  y, vjp_fn = vjp(f, x)\n  return vjp_fn(jnp.ones(y.shape))[0]\n\nprint(vgrad(lambda x: 3*x**2, jnp.ones((2, 2)))) \n```", "```py\n[[6\\. 6.]\n [6\\. 6.]] \n```", "```py\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x) \n```", "```py\nfrom jax import jvp, grad\n\n# forward-over-reverse\ndef hvp(f, primals, tangents):\n  return jvp(grad(f), primals, tangents)[1] \n```", "```py\ndef f(X):\n  return jnp.sum(jnp.tanh(X)**2)\n\nkey, subkey1, subkey2 = random.split(key, 3)\nX = random.normal(subkey1, (30, 40))\nV = random.normal(subkey2, (30, 40))\n\nans1 = hvp(f, (X,), (V,))\nans2 = jnp.tensordot(hessian(f)(X), V, 2)\n\nprint(jnp.allclose(ans1, ans2, 1e-4, 1e-4)) \n```", "```py\nTrue \n```", "```py\n# reverse-over-forward\ndef hvp_revfwd(f, primals, tangents):\n  g = lambda primals: jvp(f, primals, tangents)[1]\n  return grad(g)(primals) \n```", "```py\n# reverse-over-reverse, only works for single arguments\ndef hvp_revrev(f, primals, tangents):\n  x, = primals\n  v, = tangents\n  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n\nprint(\"Forward over reverse\")\n%timeit -n10 -r3 hvp(f, (X,), (V,))\nprint(\"Reverse over forward\")\n%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\nprint(\"Reverse over reverse\")\n%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n\nprint(\"Naive full Hessian materialization\")\n%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2) \n```", "```py\nForward over reverse\n4.74 ms ± 157 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\nReverse over forward\n9.46 ms ± 5.05 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\nReverse over reverse\n14.3 ms ± 7.71 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\nNaive full Hessian materialization\n57.7 ms ± 1.32 ms per loop (mean ± std. dev. of 3 runs, 10 loops each) \n```", "```py\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\n# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.\n# First, use a list comprehension to loop over rows in the matrix M.\ndef loop_mjp(f, x, M):\n    y, vjp_fun = vjp(f, x)\n    return jnp.vstack([vjp_fun(mi) for mi in M])\n\n# Now, use vmap to build a computation that does a single fast matrix-matrix\n# multiply, rather than an outer loop over vector-matrix multiplies.\ndef vmap_mjp(f, x, M):\n    y, vjp_fun = vjp(f, x)\n    outs, = vmap(vjp_fun)(M)\n    return outs\n\nkey = random.key(0)\nnum_covecs = 128\nU = random.normal(key, (num_covecs,) + y.shape)\n\nloop_vs = loop_mjp(f, W, M=U)\nprint('Non-vmapped Matrix-Jacobian product')\n%timeit -n10 -r3 loop_mjp(f, W, M=U)\n\nprint('\\nVmapped Matrix-Jacobian product')\nvmap_vs = vmap_mjp(f, W, M=U)\n%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n\nassert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical' \n```", "```py\nNon-vmapped Matrix-Jacobian product\n168 ms ± 260 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n\nVmapped Matrix-Jacobian product\n6.39 ms ± 49.3 μs per loop (mean ± std. dev. of 3 runs, 10 loops each) \n```", "```py\n/tmp/ipykernel_1379/3769736790.py:8: DeprecationWarning: vstack requires ndarray or scalar arguments, got <class 'tuple'> at position 0\\. In a future JAX release this will be an error.\n  return jnp.vstack([vjp_fun(mi) for mi in M]) \n```", "```py\ndef loop_jmp(f, W, M):\n    # jvp immediately returns the primal and tangent values as a tuple,\n    # so we'll compute and select the tangents in a list comprehension\n    return jnp.vstack([jvp(f, (W,), (mi,))[1] for mi in M])\n\ndef vmap_jmp(f, W, M):\n    _jvp = lambda s: jvp(f, (W,), (s,))[1]\n    return vmap(_jvp)(M)\n\nnum_vecs = 128\nS = random.normal(key, (num_vecs,) + W.shape)\n\nloop_vs = loop_jmp(f, W, M=S)\nprint('Non-vmapped Jacobian-Matrix product')\n%timeit -n10 -r3 loop_jmp(f, W, M=S)\nvmap_vs = vmap_jmp(f, W, M=S)\nprint('\\nVmapped Jacobian-Matrix product')\n%timeit -n10 -r3 vmap_jmp(f, W, M=S)\n\nassert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Jacobian-Matrix products should be identical' \n```", "```py\nNon-vmapped Jacobian-Matrix product\n290 ms ± 437 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n\nVmapped Jacobian-Matrix product\n3.29 ms ± 22.5 μs per loop (mean ± std. dev. of 3 runs, 10 loops each) \n```", "```py\nfrom jax import jacrev as builtin_jacrev\n\ndef our_jacrev(f):\n    def jacfun(x):\n        y, vjp_fun = vjp(f, x)\n        # Use vmap to do a matrix-Jacobian product.\n        # Here, the matrix is the Euclidean basis, so we get all\n        # entries in the Jacobian at once. \n        J, = vmap(vjp_fun, in_axes=0)(jnp.eye(len(y)))\n        return J\n    return jacfun\n\nassert jnp.allclose(builtin_jacrev(f)(W), our_jacrev(f)(W)), 'Incorrect reverse-mode Jacobian results!' \n```", "```py\nfrom jax import jacfwd as builtin_jacfwd\n\ndef our_jacfwd(f):\n    def jacfun(x):\n        _jvp = lambda s: jvp(f, (x,), (s,))[1]\n        Jt =vmap(_jvp, in_axes=1)(jnp.eye(len(x)))\n        return jnp.transpose(Jt)\n    return jacfun\n\nassert jnp.allclose(builtin_jacfwd(f)(W), our_jacfwd(f)(W)), 'Incorrect forward-mode Jacobian results!' \n```", "```py\ndef f(x):\n    try:\n        if x < 3:\n            return 2 * x ** 3\n        else:\n            raise ValueError\n    except ValueError:\n        return jnp.pi * x\n\ny, f_vjp = vjp(f, 4.)\nprint(jit(f_vjp)(1.)) \n```", "```py\n(Array(3.1415927, dtype=float32, weak_type=True),) \n```", "```py\ndef f(z):\n  x, y = jnp.real(z), jnp.imag(z)\n  return u(x, y) + v(x, y) * 1j\n\ndef g(x, y):\n  return (u(x, y), v(x, y)) \n```", "```py\ndef check(seed):\n  key = random.key(seed)\n\n  # random coeffs for u and v\n  key, subkey = random.split(key)\n  a, b, c, d = random.uniform(subkey, (4,))\n\n  def fun(z):\n    x, y = jnp.real(z), jnp.imag(z)\n    return u(x, y) + v(x, y) * 1j\n\n  def u(x, y):\n    return a * x + b * y\n\n  def v(x, y):\n    return c * x + d * y\n\n  # primal point\n  key, subkey = random.split(key)\n  x, y = random.uniform(subkey, (2,))\n  z = x + y * 1j\n\n  # tangent vector\n  key, subkey = random.split(key)\n  c, d = random.uniform(subkey, (2,))\n  z_dot = c + d * 1j\n\n  # check jvp\n  _, ans = jvp(fun, (z,), (z_dot,))\n  expected = (grad(u, 0)(x, y) * c +\n              grad(u, 1)(x, y) * d +\n              grad(v, 0)(x, y) * c * 1j+\n              grad(v, 1)(x, y) * d * 1j)\n  print(jnp.allclose(ans, expected)) \n```", "```py\ncheck(0)\ncheck(1)\ncheck(2) \n```", "```py\nTrue\nTrue\nTrue \n```", "```py\ndef check(seed):\n  key = random.key(seed)\n\n  # random coeffs for u and v\n  key, subkey = random.split(key)\n  a, b, c, d = random.uniform(subkey, (4,))\n\n  def fun(z):\n    x, y = jnp.real(z), jnp.imag(z)\n    return u(x, y) + v(x, y) * 1j\n\n  def u(x, y):\n    return a * x + b * y\n\n  def v(x, y):\n    return c * x + d * y\n\n  # primal point\n  key, subkey = random.split(key)\n  x, y = random.uniform(subkey, (2,))\n  z = x + y * 1j\n\n  # cotangent vector\n  key, subkey = random.split(key)\n  c, d = random.uniform(subkey, (2,))\n  z_bar = jnp.array(c + d * 1j)  # for dtype control\n\n  # check vjp\n  _, fun_vjp = vjp(fun, z)\n  ans, = fun_vjp(z_bar)\n  expected = (grad(u, 0)(x, y) * c +\n              grad(v, 0)(x, y) * (-d) +\n              grad(u, 1)(x, y) * c * (-1j) +\n              grad(v, 1)(x, y) * (-d) * (-1j))\n  assert jnp.allclose(ans, expected, atol=1e-5, rtol=1e-5) \n```", "```py\ncheck(0)\ncheck(1)\ncheck(2) \n```", "```py\ndef f(z):\n  x, y = jnp.real(z), jnp.imag(z)\n  return x**2 + y**2\n\nz = 3. + 4j\ngrad(f)(z) \n```", "```py\nArray(6.-8.j, dtype=complex64) \n```", "```py\ndef f(z):\n  return jnp.sin(z)\n\nz = 3. + 4j\ngrad(f, holomorphic=True)(z) \n```", "```py\nArray(-27.034945-3.8511531j, dtype=complex64, weak_type=True) \n```", "```py\ndef f(z):\n  return jnp.conjugate(z)\n\nz = 3. + 4j\ngrad(f, holomorphic=True)(z)  # f is not actually holomorphic! \n```", "```py\nArray(1.-0.j, dtype=complex64, weak_type=True) \n```", "```py\nA = jnp.array([[5.,    2.+3j,    5j],\n              [2.-3j,   7.,  1.+7j],\n              [-5j,  1.-7j,    12.]])\n\ndef f(X):\n    L = jnp.linalg.cholesky(X)\n    return jnp.sum((L - jnp.sin(L))**2)\n\ngrad(f, holomorphic=True)(A) \n```", "```py\nArray([[-0.7534186  +0.j       , -3.0509028 -10.940544j ,\n         5.9896846  +3.5423026j],\n       [-3.0509028 +10.940544j , -8.904491   +0.j       ,\n        -5.1351523  -6.559373j ],\n       [ 5.9896846  -3.5423026j, -5.1351523  +6.559373j ,\n         0.01320427 +0.j       ]], dtype=complex64) \n```"]