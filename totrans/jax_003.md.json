["```py\npip install jax \n```", "```py\npip install -U \"jax[cuda12]\" \n```", "```py\nimport jax.numpy as jnp \n```", "```py\ndef selu(x, alpha=1.67, lmbda=1.05):\n  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(5.0)\nprint(selu(x)) \n```", "```py\n[0\\.        1.05      2.1       3.1499999 4.2      ] \n```", "```py\nfrom jax import random\n\nkey = random.key(1701)\nx = random.normal(key, (1_000_000,))\n%timeit selu(x).block_until_ready() \n```", "```py\n2.84 ms ± 9.23 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) \n```", "```py\nfrom jax import jit\n\nselu_jit = jit(selu)\n_ = selu_jit(x)  # compiles on first call\n%timeit selu_jit(x).block_until_ready() \n```", "```py\n844 μs ± 2.73 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) \n```", "```py\nfrom jax import grad\n\ndef sum_logistic(x):\n  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\nx_small = jnp.arange(3.)\nderivative_fn = grad(sum_logistic)\nprint(derivative_fn(x_small)) \n```", "```py\n[0.25       0.19661197 0.10499357] \n```", "```py\ndef first_finite_differences(f, x, eps=1E-3):\n  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n                   for v in jnp.eye(len(x))])\n\nprint(first_finite_differences(sum_logistic, x_small)) \n```", "```py\n[0.24998187 0.1965761  0.10502338] \n```", "```py\nprint(grad(jit(grad(jit(grad(sum_logistic)))))(1.0)) \n```", "```py\n-0.0353256 \n```", "```py\nfrom jax import jacobian\nprint(jacobian(jnp.exp)(x_small)) \n```", "```py\n[[1\\.        0\\.        0\\.       ]\n [0\\.        2.7182817 0\\.       ]\n [0\\.        0\\.        7.389056 ]] \n```", "```py\nfrom jax import jacfwd, jacrev\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\nprint(hessian(sum_logistic)(x_small)) \n```", "```py\n[[-0\\.         -0\\.         -0\\.        ]\n [-0\\.         -0.09085776 -0\\.        ]\n [-0\\.         -0\\.         -0.07996249]] \n```", "```py\nkey1, key2 = random.split(key)\nmat = random.normal(key1, (150, 100))\nbatched_x = random.normal(key2, (10, 100))\n\ndef apply_matrix(x):\n  return jnp.dot(mat, x) \n```", "```py\ndef naively_batched_apply_matrix(v_batched):\n  return jnp.stack([apply_matrix(v) for v in v_batched])\n\nprint('Naively batched')\n%timeit naively_batched_apply_matrix(batched_x).block_until_ready() \n```", "```py\nNaively batched\n962 μs ± 1.54 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) \n```", "```py\nimport numpy as np\n\n@jit\ndef batched_apply_matrix(batched_x):\n  return jnp.dot(batched_x, mat.T)\n\nnp.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n                           batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\nprint('Manually batched')\n%timeit batched_apply_matrix(batched_x).block_until_ready() \n```", "```py\nManually batched\n14.3 μs ± 28.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) \n```", "```py\nfrom jax import vmap\n\n@jit\ndef vmap_batched_apply_matrix(batched_x):\n  return vmap(apply_matrix)(batched_x)\n\nnp.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n                           vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\nprint('Auto-vectorized with vmap')\n%timeit vmap_batched_apply_matrix(batched_x).block_until_ready() \n```", "```py\nAuto-vectorized with vmap\n21.7 μs ± 98.7 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) \n```"]