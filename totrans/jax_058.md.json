["```py\nnamespace  gpu_ops  {\n\nvoid  rms_forward_affine_mixed_dtypes(cudaStream_t  stream,  void  **buffers,\n  const  char  *opaque,\n  std::size_t  opaque_len);\n\nvoid  rms_backward_affine(cudaStream_t  stream,  void  **buffers,\n  const  char  *opaque,  std::size_t  opaque_len);\n\n}  // namespace gpu_ops \n```", "```py\nnamespace  gpu_ops  {\n\nenum  ElementType  {  BF16,  F16,  F32,  F64  };\n\nstruct  RMSNormDescriptor  {\n  int  n1;\n  int  n2;\n  double  eps;\n  ElementType  x_type;\n  ElementType  w_type;\n  int  part_grad_size;\n};\n\n}  // namespace gpu_ops \n```", "```py\npybind11::dict  RMSNormRegistrations()  {\n  pybind11::dict  dict;\n  dict[\"rms_forward_affine_mixed_dtype\"]  =\n  gpu_ops::EncapsulateFunction(gpu_ops::rms_forward_affine_mixed_dtypes);\n  dict[\"rms_backward_affine\"]  =\n  gpu_ops::EncapsulateFunction(gpu_ops::rms_backward_affine);\n  return  dict;\n}\n\nPYBIND11_MODULE(gpu_ops,  m)  {\n  m.def(\"get_rms_norm_registrations\",  &RMSNormRegistrations);\n  m.def(\"create_rms_norm_descriptor\",\n  [](int  n1,  int  n2,  double  eps,  gpu_ops::ElementType  x_type,\n  gpu_ops::ElementType  w_type,  int  part_grad_size)  {\n  return  gpu_ops::PackDescriptor(gpu_ops::RMSNormDescriptor{\n  n1,  n2,  eps,  x_type,  w_type,  part_grad_size});\n  });\n\n  pybind11::enum_<gpu_ops::ElementType>(m,  \"ElementType\")\n  .value(\"BF16\",  gpu_ops::ElementType::BF16)\n  .value(\"F16\",  gpu_ops::ElementType::F16)\n  .value(\"F32\",  gpu_ops::ElementType::F32)\n  .value(\"F64\",  gpu_ops::ElementType::F64);\n\n} \n```", "```py\npython  -m  pip  install  pybind11==2.10.1\nmkdir  -p  build\npybind_include_path=$(python  -c  \"import pybind11; print(pybind11.get_include())\")\npython_executable=$(python  -c  'import sys; print(sys.executable)')\n\nnvcc  --threads  4  -Xcompiler  -Wall  -ldl  --expt-relaxed-constexpr  -O3  -DNDEBUG  -Xcompiler  -O3  --generate-code=arch=compute_70,code=[compute_70,sm_70]  --generate-code=arch=compute_75,code=[compute_75,sm_75]  --generate-code=arch=compute_80,code=[compute_80,sm_80]  --generate-code=arch=compute_86,code=[compute_86,sm_86]  -Xcompiler=-fPIC  -Xcompiler=-fvisibility=hidden  -x  cu  -c  gpu_ops/rms_norm_kernels.cu  -o  build/rms_norm_kernels.cu.o\nc++  -I/usr/local/cuda/include  -I$pybind_include_path  $(${python_executable}-config  --cflags)  -O3  -DNDEBUG  -O3  -fPIC  -fvisibility=hidden  -flto  -fno-fat-lto-objects  -o  build/gpu_ops.cpp.o  -c  gpu_ops/gpu_ops.cpp\nc++  -fPIC  -O3  -DNDEBUG  -O3  -flto  -shared  -o  build/gpu_ops$(${python_executable}-config  --extension-suffix)  build/gpu_ops.cpp.o  build/rms_norm_kernels.cu.o  -L/usr/local/cuda/lib64  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl\nstrip  build/gpu_ops$(${python_executable}-config  --extension-suffix) \n```", "```py\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nimport jax._src.test_util as jtu\nfrom build import gpu_ops\nfrom jax import core, dtypes\nfrom jax.interpreters import xla\nfrom jax.lib import xla_client\n\n# Create _rms_norm_fwd_p for forward operation.\n_rms_norm_fwd_p = core.Primitive(\"rms_norm_fwd\")\n_rms_norm_fwd_p.multiple_results = True\n_rms_norm_fwd_p.def_impl(partial(xla.apply_primitive, _rms_norm_fwd_p))\n\ndef rms_norm_fwd(x, weight, eps=1e-05):\n    output, invvar = _rms_norm_fwd_p.bind(x, weight, eps=eps)\n    return output\n\n# Create _rms_norm_bwd_p for backward operation.\n_rms_norm_bwd_p = core.Primitive(\"rms_norm_bwd\")\n_rms_norm_bwd_p.multiple_results = True\n_rms_norm_bwd_p.def_impl(partial(xla.apply_primitive, _rms_norm_bwd_p))\n\ndef rms_norm_bwd(g, invvar, x, weight, eps):\n    grad_input, grad_weight, part_grad = _rms_norm_bwd_p.bind(\n        g, invvar, x, weight, eps=eps\n    )\n    return grad_input, grad_weight \n```", "```py\nfrom functools import reduce\n\nfrom jax.interpreters import mlir\nfrom jax.interpreters.mlir import ir\nfrom jaxlib.hlo_helpers import custom_call\n\n# Register functions defined in gpu_ops as custom call target for GPUs\nfor _name, _value in gpu_ops.get_rms_norm_registrations().items():\n    xla_client.register_custom_call_target(_name, _value, platform=\"gpu\")\n\ndef element_type_to_descriptor_type_mapping(element_type):\n    _element_type_to_descriptor_type_mapping = {\n        ir.BF16Type.get(): gpu_ops.ElementType.BF16,\n        ir.F16Type.get(): gpu_ops.ElementType.F16,\n        ir.F32Type.get(): gpu_ops.ElementType.F32,\n        ir.F64Type.get(): gpu_ops.ElementType.F64,\n    }\n    return _element_type_to_descriptor_type_mapping.get(element_type)\n\ndef default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\ndef _rms_norm_fwd_cuda_lowering(ctx, x, weight, eps):\n    x_type = ir.RankedTensorType(x.type)\n    x_shape = x_type.shape\n    w_type = ir.RankedTensorType(weight.type)\n    w_shape = w_type.shape\n    iv_element_type = (\n        ir.F32Type.get()\n        if x_type.element_type in [ir.F16Type.get(), ir.BF16Type.get()]\n        else x_type.element_type\n    )\n\n    n2 = reduce(lambda x, y: x * y, w_shape)\n    n1 = reduce(lambda x, y: x * y, x_shape) // n2\n\n    opaque = gpu_ops.create_rms_norm_descriptor(\n        n1,\n        n2,\n        eps,\n        element_type_to_descriptor_type_mapping(x_type.element_type),\n        element_type_to_descriptor_type_mapping(w_type.element_type),\n        0,  # unused\n    )\n    out = custom_call(\n        b\"rms_forward_affine_mixed_dtype\",\n        result_types=[\n            ir.RankedTensorType.get(x_shape, w_type.element_type),\n            ir.RankedTensorType.get((n1,), iv_element_type),\n        ],\n        operands=[x, weight],\n        backend_config=opaque,\n        operand_layouts=default_layouts(x_shape, w_shape),\n        result_layouts=default_layouts(x_shape, (n1,)),\n    ).results\n    return out\n\nmlir.register_lowering(\n    _rms_norm_fwd_p,\n    _rms_norm_fwd_cuda_lowering,\n    platform=\"gpu\",\n)\n\ndef _rms_norm_bwd_cuda_lowering(ctx, grad_output, invvar, x, weight, eps):\n    x_type = ir.RankedTensorType(x.type)\n    x_shape = x_type.shape\n    w_type = ir.RankedTensorType(weight.type)\n    w_shape = w_type.shape\n    iv_type = ir.RankedTensorType(invvar.type)\n\n    n2 = reduce(lambda x, y: x * y, w_shape)\n    n1 = reduce(lambda x, y: x * y, x_shape) // n2\n\n    part_grad_shape = ctx.avals_out[-1].shape\n\n    opaque = gpu_ops.create_rms_norm_descriptor(\n        n1,\n        n2,\n        eps,\n        element_type_to_descriptor_type_mapping(x_type.element_type),\n        element_type_to_descriptor_type_mapping(w_type.element_type),\n        part_grad_shape[0],\n    )\n    out = custom_call(\n        b\"rms_backward_affine\",\n        result_types=[\n            ir.RankedTensorType.get(x_shape, x_type.element_type),\n            ir.RankedTensorType.get(w_shape, w_type.element_type),\n            ir.RankedTensorType.get(part_grad_shape, iv_type.element_type),\n        ],\n        operands=[grad_output, invvar, x, weight],\n        backend_config=opaque,\n        operand_layouts=default_layouts(x_shape, (n1,), x_shape, w_shape),\n        result_layouts=default_layouts(x_shape, w_shape, part_grad_shape),\n    ).results\n    return out\n\nmlir.register_lowering(\n    _rms_norm_bwd_p,\n    _rms_norm_bwd_cuda_lowering,\n    platform=\"gpu\",\n) \n```", "```py\nper_core_batch_size=4\nseq_len=512\nemb_dim=512\nx = jax.random.normal(\n    jax.random.PRNGKey(0),\n    shape=(jax.local_device_count() * per_core_batch_size, seq_len, emb_dim),\n    dtype=jnp.bfloat16,\n)\nnorm_shape = x.shape[-2:]\nweight = jnp.ones(norm_shape, dtype=jnp.bfloat16) \n```", "```py\nout = rms_norm_fwd(x, weight) \n```", "```py\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\nCell In [5], line 1\n----> 1 out = rms_norm_fwd(x, weight)\n\n...\n\nNotImplementedError: Abstract evaluation for 'rms_norm_fwd' not implemented \n```", "```py\nfrom functools import reduce\nfrom operator import mul\n\nfrom jax.core import ShapedArray\n\ndef _rms_norm_fwd_abstract(x, weight, eps):\n    w_dtype = dtypes.canonicalize_dtype(weight.dtype)\n    iv_dtype = dtypes.canonicalize_dtype(x.dtype)\n    if iv_dtype in [jnp.float16, jnp.bfloat16]:\n        iv_dtype = jnp.float32\n    n2 = reduce(mul, weight.shape)\n    n1 = reduce(mul, x.shape) // n2\n    return (\n        ShapedArray(x.shape, w_dtype, named_shape=x.named_shape),  # output\n        ShapedArray((n1,), iv_dtype, named_shape=x.named_shape),  # invvar\n    )\n\n_rms_norm_fwd_p.def_abstract_eval(_rms_norm_fwd_abstract)\n\ndef _rms_norm_bwd_abstract(grad_output, invvar, x, weight, eps):\n    iv_dtype = dtypes.canonicalize_dtype(invvar.dtype)\n    w_dtype = dtypes.canonicalize_dtype(weight.dtype)\n    x_dtype = dtypes.canonicalize_dtype(x.dtype)\n    n2 = reduce(lambda x, y: x * y, weight.shape)\n    n1 = reduce(lambda x, y: x * y, x.shape) // n2\n    part_grad_shape = (16, n2)\n    assert dtypes.canonicalize_dtype(grad_output.dtype) == w_dtype\n    assert grad_output.shape == x.shape\n    assert invvar.shape == (n1,)\n    assert (\n        iv_dtype == jnp.float32 if x_dtype in [jnp.float16, jnp.bfloat16] else x_dtype\n    )\n    assert grad_output.named_shape == x.named_shape\n    weight_named_shape = (\n        weight_named_shape if weight.named_shape else x.named_shape\n    )\n    return (\n        ShapedArray(\n            x.shape, x_dtype, named_shape=x.named_shape\n        ),  # grad input\n        ShapedArray(\n            weight.shape, w_dtype, named_shape=weight_named_shape\n        ),  # grad weight\n        ShapedArray(\n            part_grad_shape, iv_dtype, named_shape=weight_named_shape\n        ),  # part grad\n    )\n\n_rms_norm_bwd_p.def_abstract_eval(_rms_norm_bwd_abstract) \n```", "```py\nout = rms_norm_fwd(x, weight) \n```", "```py\ndef loss(x, weight):\n    predictions = rms_norm_fwd(x, weight)\n    return -jnp.mean(predictions**2)\n\nloss_grad = jax.grad(loss)\nout = loss_grad(x, weight)\njtu.check_grads(loss, (x, weight), modes=[\"rev\"], order=1) \n```", "```py\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\nCell In [8], line 7\n      3     return -jnp.mean(predictions**2)\n      6 loss_grad = jax.grad(loss)\n----> 7 out = loss_grad(x, weight)\n\n...\n\nNotImplementedError: Differentiation rule for 'rms_norm_fwd' not implemented \n```", "```py\n# rms_norm_fwd was previously defined as\n#\n# def rms_norm_fwd(x, weight, eps=1e-05):\n#     output, invvar = _rms_norm_fwd_p.bind(x, weight, eps=eps)\n#     return output\n#\ndef rms_norm_fwd(x, weight, eps=1e-05):\n    output, invvar = _rms_norm_fwd_p.bind(x, weight, eps=eps)\n    return output, (invvar, x, weight)\n\n# rms_norm_bwd was previously defined as\n#\n# def rms_norm_bwd(g, invvar, x, weight, eps):\n#     grad_input, grad_weight, part_grad = _rms_norm_bwd_p.bind(\n#         g, invvar, x, weight, eps=eps\n#     )\n#     return grad_input, grad_weight\n#\ndef rms_norm_bwd(eps, res, g):\n    invvar, x, weight = res\n    grad_input, grad_weight, part_grad = _rms_norm_bwd_p.bind(\n        g, invvar, x, weight, eps=eps\n    )\n    return grad_input, grad_weight \n```", "```py\n@partial(jax.custom_vjp, nondiff_argnums=(2,))\ndef rms_norm(x, weight, eps=1e-05):\n    output, _ = rms_norm_fwd(x, weight, eps=eps)\n    return output\n\nrms_norm.defvjp(rms_norm_fwd, rms_norm_bwd) \n```", "```py\ndef loss(x, weight):\n    predictions = rms_norm(x, weight)\n    return -jnp.mean(predictions**2)\n\nloss_grad = jax.grad(loss)\nout = loss_grad(x, weight)\njtu.check_grads(loss, (x, weight), modes=[\"rev\"], order=1) \n```", "```py\nfrom jax.sharding import Mesh, PartitionSpec\nfrom jax.experimental.pjit import pjit\n\nmesh = Mesh(jax.local_devices(), (\"x\",))\nref = rms_norm(x, weight)\npjitted = pjit(\n    rms_norm,\n    # Shard x by batch dimension and replicate weight on all devices.\n    in_shardings=(PartitionSpec(\"x\", None, None), PartitionSpec(None, None)),\n    # Shard the output by batch dimension.\n    out_shardings=PartitionSpec(\"x\", None, None),\n)\n\nwith mesh:\n    print(pjitted.lower(x, weight).compile().runtime_executable().hlo_modules()[0].to_string())\n    out = pjitted(x, weight)\n\njnp.allclose(ref, out, atol=1e-5, rtol=1e-5) \n```", "```py\nHloModule pjit_rms_norm, entry_computation_layout={(bf16[4,512,512]{2,1,0},bf16[512,512]{1,0})->bf16[4,512,512]{2,1,0}}\n\n%fused_computation (param_1: bf16[32,512,512], param_1.3: u32[]) -> bf16[4,512,512] {\n  %param_1 = bf16[32,512,512]{2,1,0} parameter(0)\n  %param_1.3 = u32[] parameter(1)\n  %convert.2 = s32[] convert(u32[] %param_1.3), metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n  %constant_9 = s32[] constant(4), metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n  %multiply.3 = s32[] multiply(s32[] %convert.2, s32[] %constant_9), metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n  %constant_8 = s32[] constant(0), metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n  ROOT %dynamic-slice.2 = bf16[4,512,512]{2,1,0} dynamic-slice(bf16[32,512,512]{2,1,0} %param_1, s32[] %multiply.3, s32[] %constant_8, s32[] %constant_8), dynamic_slice_sizes={4,512,512}, metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n}\n\nENTRY %main.7_spmd (param: bf16[4,512,512], param.1: bf16[512,512]) -> bf16[4,512,512] {\n  %param = bf16[4,512,512]{2,1,0} parameter(0), sharding={devices=[8,1,1]0,1,2,3,4,5,6,7}\n  %all-gather = bf16[32,512,512]{2,1,0} all-gather(bf16[4,512,512]{2,1,0} %param), channel_id=1, replica_groups={{0,1,2,3,4,5,6,7}}, dimensions={0}, use_global_device_ids=true, metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n  %param.1 = bf16[512,512]{1,0} parameter(1), sharding={replicated}\n  %custom-call.0 = (bf16[32,512,512]{2,1,0}, f32[32]{0}) custom-call(bf16[32,512,512]{2,1,0} %all-gather, bf16[512,512]{1,0} %param.1), custom_call_target=\"rms_forward_affine_mixed_dtype\", operand_layout_constraints={bf16[32,512,512]{2,1,0}, bf16[512,512]{1,0}}, api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}, backend_config=\" \\000\\000\\000\\000\\000\\004\\000\\361h\\343\\210\\265\\370\\344>\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\255\\177\\000\\000\"\n  %get-tuple-element = bf16[32,512,512]{2,1,0} get-tuple-element((bf16[32,512,512]{2,1,0}, f32[32]{0}) %custom-call.0), index=0, metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n  %partition-id = u32[] partition-id(), metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n  ROOT %fusion = bf16[4,512,512]{2,1,0} fusion(bf16[32,512,512]{2,1,0} %get-tuple-element, u32[] %partition-id), kind=kLoop, calls=%fused_computation, metadata={op_name=\"pjit(rms_norm)/jit(main)/rms_norm_fwd[eps=1e-05]\" source_file=\"/tmp/ipykernel_25235/3343076723.py\" source_line=8}\n} \n```", "```py\nTrue \n```", "```py\ndef register_primitive(cls):\n  \"\"\"\n register jax primitive\n\n The order of calls. Each operation is composed of two primitives: Inner and Outer.\n\n Inner, only the basic to wrap the custom_call itself.\n - impl to XLA custom_call in C.\n - abstract to know the static shapes\n - lower to StableHLO XLA custom_call.\n Outer, mostly all the rest:\n - impl: Bind to the inner primitive. Not used for real computation, but only for tracing. So we only need to bind.\n - abstract: same\n - lower to StableHLO custom_p. (XLA will call the python callback from it)\n - custom_p\n - vmap: could be added here.\n VJP is based on Outer, but not handled in this function.\n \"\"\"\n\n    def name_of_wrapper_p():\n        return cls.name + \"_wrapper\"\n\n    inner_p = core.Primitive(cls.name)\n    dispatch.prim_requires_devices_during_lowering.add(inner_p)\n    inner_p.multiple_results = cls.multiple_results\n    inner_p.def_impl(partial(xla.apply_primitive, inner_p))\n    inner_p.def_abstract_eval(cls.abstract)\n    mlir.register_lowering(inner_p, cls.lowering, platform='cuda')\n    cls.inner_primitive = inner_p\n\n    outer_p = core.Primitive(name_of_wrapper_p())\n    dispatch.prim_requires_devices_during_lowering.add(outer_p)\n    outer_p.multiple_results = cls.multiple_results\n    outer_p.def_impl(cls.impl)\n    outer_p.def_abstract_eval(cls.abstract)\n    batching.primitive_batchers[outer_p] = cls.batcher\n    outer_p_lower = custom_partitioning(cls.impl, static_argnums=cls.impl_static_args)\n    outer_p_lower.def_partition(infer_sharding_from_operands=cls.infer_sharding_from_operands,\n                                partition=cls.partition)\n    mlir.register_lowering(outer_p,\n                           mlir.lower_fun(outer_p_lower, multiple_results=cls.multiple_results))\n    cls.outer_primitive = outer_p\n... \n```", "```py\nclass RmsNormFwdClass:\n    name = \"rms_forward_affine_mixed_dtype\"\n    multiple_results = True\n    impl_static_args = (2,)    # eps\n    inner_primitive = None\n    outer_primitive = None\n\n    @staticmethod\n    def infer_sharding_from_operands(eps : float, mesh : jax.sharding.Mesh,\n                                     arg_infos : Tuple[jax._src.api.ShapeDtypeStruct],\n                                     result_infos : Tuple[jax._src.core.ShapedArray]):\n        del eps, result_infos  # Not needed for this example.\n        x_info, weight_info = arg_infos\n        assert len(x_info.shape) == 3\n        assert len(weight_info.shape) == 2\n        # partition() will force all dims of all inputs to be replicated except the\n        # first dim of x that will be kept as is.\n        # This is because the implementaion can only be sharded on the batch dimensions.\n\n        x_spec = arg_infos[0].sharding.spec\n        # None mean that we replicate on that dimension.\n        output_sharding = NamedSharding(mesh, PartitionSpec(x_spec[0], None, None))\n        invvar_sharding = NamedSharding(mesh, PartitionSpec(x_spec[0]))\n        return (output_sharding, invvar_sharding)\n\n    @staticmethod\n    def partition(eps : float, mesh : jax.sharding.Mesh,\n                  arg_infos : Tuple[jax._src.api.ShapeDtypeStruct],\n                  result_infos : Tuple[jax._src.api.ShapeDtypeStruct]):\n        del result_infos  # Not needed for this example.\n        x_info, weight_info = arg_infos\n        assert len(x_info.shape) == 3\n        assert len(weight_info.shape) == 2\n        x_spec = arg_infos[0].sharding.spec\n        # We only support sharding on the batch dimensions.\n        # Force sharding on all others dimensions with None.\n        arg_shardings = (NamedSharding(mesh, PartitionSpec(x_spec[0], None, None)),\n                         NamedSharding(mesh, PartitionSpec(None, None)))\n        invvar_sharding = NamedSharding(mesh, PartitionSpec(x_spec[0]))\n        output_shardings = (arg_shardings[0], invvar_sharding)\n        # Sharded_impl only accepts positional arugments\n        # And they should be Jax traceable variables\n        impl = partial(RmsNormFwdClass.impl, eps=eps)\n\n        return mesh, impl, output_shardings, arg_shardings\nregister_primitive(RmsNormFwdClass) \n```", "```py\nclass RmsNormBwdClass:\n    name = \"rms_norm_bwd\"\n    multiple_results = True\n    impl_static_args = (4,)    # eps\n    inner_primitive = None\n    outer_primitive = None\n\n    @staticmethod\n    def infer_sharding_from_operands(eps : float, mesh : jax.sharding.Mesh,\n                                     arg_infos : Tuple[jax._src.api.ShapeDtypeStruct],\n                                     result_infos : Tuple[jax._src.core.ShapedArray]):\n        del eps, result_infos  # Not needed for this example.\n        g_info, invvar_info, x_info, weight_info = arg_infos\n        assert len(g_info.shape) == 3\n        assert len(invvar_info.shape) == 1\n        assert len(x_info.shape) == 3\n        assert len(weight_info.shape) == 2\n        # partition() will force all dims to be replicated except the batch dimension.\n        x_spec = x_info.sharding.spec\n        output_sharding = NamedSharding(mesh, PartitionSpec(x_spec[0], None, None))\n        invvar_sharding = NamedSharding(mesh, PartitionSpec(None, None))\n        return (output_sharding, invvar_sharding, output_sharding, )\n\n    @staticmethod\n    def partition(eps : float, mesh : jax.sharding.Mesh,\n                  arg_infos : Tuple[jax._src.api.ShapeDtypeStruct],\n                  result_infos : Tuple[jax._src.api.ShapeDtypeStruct]):\n        del result_infos  # Not needed for this example.\n        g_info, invvar_info, x_info, weight_info = arg_infos\n        assert len(g_info.shape) == 3\n        assert len(invvar_info.shape) == 1\n        assert len(x_info.shape) == 3\n        assert len(weight_info.shape) == 2\n\n        # We only support sharding on the batch dimensions.\n        # Force sharding on all others dimensions with None.\n        # Also force gx, x and invvar to have the same batch sharding/replication.\n        x_spec = x_info.sharding.spec\n        arg_shardings = (NamedSharding(mesh, PartitionSpec(x_spec[0], None, None)),\n                         NamedSharding(mesh, PartitionSpec(x_spec[0],)),\n                         NamedSharding(mesh, PartitionSpec(x_spec[0], None, None)),\n                         NamedSharding(mesh, PartitionSpec(None, None)))\n\n        output_sharding = NamedSharding(mesh, PartitionSpec(x_spec[0], None, None))\n        invvar_sharding = NamedSharding(mesh, PartitionSpec(None, None))\n        output_shardings = (output_sharding, invvar_sharding, invvar_sharding)\n\n        # Sharded_impl only accepts positional arugments\n        # And they should be Jax traceable variables\n        def impl(g, invvar, x, weight):\n            grad_input, grad_weight, part_grad = _rms_norm_bwd_p.bind(\n                g, invvar, x, weight, eps=eps\n            )\n            # We need to sum the weight gradient from all partition.\n            global_weight = grad_weight\n            if x_spec[0]:\n                global_weight = jax.lax.psum(grad_weight, x_spec[0])\n            return grad_input, global_weight, part_grad\n        return mesh, impl, output_shardings, arg_shardings\nregister_primitive(RmsNormBwdClass) \n```", "```py\n@partial(jax.custom_vjp, nondiff_argnums=(2,))\ndef custom_p_rms_norm(x, weight, eps=1e-05):\n    output, _ = custom_p_rms_norm_fwd(x, weight, eps=eps)\n    return output\n\ndef custom_p_rms_norm_fwd(x, weight, eps=1e-05):\n    output, invvar = RmsNormFwdClass.outer_primitive.bind(x, weight, eps=eps)\n    return output, (invvar, x, weight)\n\ndef custom_p_rms_norm_bwd(eps, res, g):\n    invvar, x, weight = res\n    grad_input, grad_weight, part_grad = RmsNormBwdClass.outer_primitive.bind(\n        g, invvar, x, weight, eps=eps)\n    return grad_input, grad_weight\n\ncustom_p_rms_norm.defvjp(custom_p_rms_norm_fwd, custom_p_rms_norm_bwd) \n```", "```py\ndef ref_loss(x, weight):\n    predictions = rms_norm(x, weight)\n    return -jnp.mean(predictions**2)\n\nref = jax.grad(ref_loss, argnums=(0, 1))(x, weight)\n\ndef custom_p_loss(x, weight):\n    predictions = custom_p_rms_norm(x, weight)\n    return -jnp.mean(predictions**2) \n```", "```py\nwith Mesh(jax.local_devices(), (\"x\",)):\n    def run_and_verify(loss):\n        pjitted = pjit(\n            jax.grad(loss, argnums=(0, 1)),\n            # Shard x by batch dimension and replicate weight on all devices.\n            in_shardings=(\n                PartitionSpec(\"x\", None, None),\n                PartitionSpec(None, None),\n            ),\n            # Shard the output by batch dimension and replicate weight grad on all devices.\n            out_shardings=(\n                PartitionSpec(\"x\", None, None),\n                PartitionSpec(None, None),\n            ),\n        )\n        hlo = pjitted.lower(x, weight).compile().as_text()\n        out = pjitted(x, weight)\n        print(hlo)\n        assert \"all-reduce-done\" in hlo, \"The gradient will produce wrong value!\"\n        if \"all-gather-start\" in hlo:\n            print(\"NOT OPTIMIZED, ALL_GATHER in the graph!\")\n        return out\n\n    custom_p_out = run_and_verify(custom_p_loss)\n\nfor r, o in zip(ref_out, custom_p_out):\n    print(jnp.allclose(r, o, atol=1e-6, rtol=1e-6)) \n```", "```py\nHloModule pjit_custom_p_loss, is_scheduled=true, entry_computation_layout={(f16[4,512,512]{2,1,0}, f16[512,512]{1,0})->(f16[4,512,512]{2,1,0}, f16[512,512]{1,0})}, allow_spmd_sharding_propagation_to_parameters={false,false}, allow_spmd_sharding_propagation_to_output={false,false}, num_partitions=4, frontend_attributes={fingerprint_before_lhs=\"d7b9bc40de002332dd665ff2ab537b76\"}\n\n%fused_multiply (param_0: f16[4,512,512]) -> f16[4,512,512] {\n  %param_0 = f16[4,512,512]{2,1,0} parameter(0)\n  %constant_4_1 = f16[] constant(-4.7684e-07)\n  %broadcast.8.1 = f16[4,512,512]{2,1,0} broadcast(f16[] %constant_4_1), dimensions={}, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/mul\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=484}\n  ROOT %multiply.5.1 = f16[4,512,512]{2,1,0} multiply(f16[4,512,512]{2,1,0} %param_0, f16[4,512,512]{2,1,0} %broadcast.8.1), metadata={op_name=\"pjit(custom_p_loss)/jit(main)/mul\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=484}\n}\n\n%region_0.9._custom_call_lowering_rule (Arg_0.10.0: f16[], Arg_1.11.0: f16[]) -> f16[] {\n  %Arg_1.11.0 = f16[] parameter(1)\n  %Arg_0.10.0 = f16[] parameter(0)\n  ROOT %add.2.0 = f16[] add(f16[] %Arg_0.10.0, f16[] %Arg_1.11.0), metadata={op_name=\"jit(main)/add\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=433}\n}\n\nENTRY %main.23_spmd (param.2: f16[4,512,512], param.1.0: f16[512,512]) -> (f16[4,512,512], f16[512,512]) {\n  %param.1.0 = f16[512,512]{1,0} parameter(1), sharding={replicated}\n  %param.2 = f16[4,512,512]{2,1,0} parameter(0), sharding={devices=[4,1,1]<=[4]}\n  %custom-call.3.0 = (f16[4,512,512]{2,1,0}, f32[4]{0}) custom-call(f16[4,512,512]{2,1,0} %param.2, f16[512,512]{1,0} %param.1.0), custom_call_target=\"rms_forward_affine_mixed_dtype\", operand_layout_constraints={f16[4,512,512]{2,1,0}, f16[512,512]{1,0}}, api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormFwdClass.partition at 0x7ff99e3980d0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormFwdClass.infer_sharding_from_operands at 0x7ff99e398040> decode_shardings=True in_tree=PyTreeDef((*, *)) out_tree=PyTreeDef((*, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=440}, backend_config=\"\\004\\000\\000\\000\\000\\000\\004\\000\\361h\\343\\210\\265\\370\\344>\\001\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000$V\\000\\000\"\n  %get-tuple-element.14 = f16[4,512,512]{2,1,0} get-tuple-element((f16[4,512,512]{2,1,0}, f32[4]{0}) %custom-call.3.0), index=0, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormFwdClass.partition at 0x7ff99e3980d0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormFwdClass.infer_sharding_from_operands at 0x7ff99e398040> decode_shardings=True in_tree=PyTreeDef((*, *)) out_tree=PyTreeDef((*, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=440}\n  %loop_multiply_fusion = f16[4,512,512]{2,1,0} fusion(f16[4,512,512]{2,1,0} %get-tuple-element.14), kind=kLoop, calls=%fused_multiply, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/mul\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=484}\n  %get-tuple-element.1.0 = f32[4]{0} get-tuple-element((f16[4,512,512]{2,1,0}, f32[4]{0}) %custom-call.3.0), index=1, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormFwdClass.partition at 0x7ff99e3980d0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormFwdClass.infer_sharding_from_operands at 0x7ff99e398040> decode_shardings=True in_tree=PyTreeDef((*, *)) out_tree=PyTreeDef((*, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=440}\n  %custom-call.5.0 = (f16[4,512,512]{2,1,0}, f16[512,512]{1,0}, f32[16,262144]{1,0}) custom-call(f16[4,512,512]{2,1,0} %loop_multiply_fusion, f32[4]{0} %get-tuple-element.1.0, f16[4,512,512]{2,1,0} %param.2, f16[512,512]{1,0} %param.1.0), custom_call_target=\"rms_backward_affine\", operand_layout_constraints={f16[4,512,512]{2,1,0}, f32[4]{0}, f16[4,512,512]{2,1,0}, f16[512,512]{1,0}}, api_version=API_VERSION_STATUS_RETURNING, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormBwdClass.partition at 0x7ff99e3985e0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550> decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=483}, backend_config=\"\\004\\000\\000\\000\\000\\000\\004\\000\\361h\\343\\210\\265\\370\\344>\\001\\000\\000\\000\\001\\000\\000\\000\\020\\000\\000\\000$V\\000\\000\"\n  %get-tuple-element.7.0 = f16[512,512]{1,0} get-tuple-element((f16[4,512,512]{2,1,0}, f16[512,512]{1,0}, f32[16,262144]{1,0}) %custom-call.5.0), index=1, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormBwdClass.partition at 0x7ff99e3985e0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550> decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=483}\n  %all-reduce-start = f16[512,512]{1,0} all-reduce-start(f16[512,512]{1,0} %get-tuple-element.7.0), channel_id=1, replica_groups={{0,1,2,3}}, use_global_device_ids=true, to_apply=%region_0.9._custom_call_lowering_rule, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormBwdClass.partition at 0x7ff99e3985e0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550> decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=483}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"collective_backend_config\":{\"is_sync\":true,\"no_parallel_custom_call\":false}}\n  %all-reduce-done = f16[512,512]{1,0} all-reduce-done(f16[512,512]{1,0} %all-reduce-start), metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormBwdClass.partition at 0x7ff99e3985e0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550> decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=483}\n  %get-tuple-element.12.0 = f16[4,512,512]{2,1,0} get-tuple-element((f16[4,512,512]{2,1,0}, f16[512,512]{1,0}, f32[16,262144]{1,0}) %custom-call.5.0), index=0, metadata={op_name=\"pjit(custom_p_loss)/jit(main)/custom_partitioning[partition=<function RmsNormBwdClass.partition at 0x7ff99e3985e0> propagate_user_sharding=None infer_sharding_from_operands=<function RmsNormBwdClass.infer_sharding_from_operands at 0x7ff99e398550> decode_shardings=True in_tree=PyTreeDef((*, *, *, *)) out_tree=PyTreeDef((*, *, *)) static_args=[1e-05]]\" source_file=\"/opt/jax/docs/Custom_Operation_for_GPUs.py\" source_line=483}\n  ROOT %tuple.1.0 = (f16[4,512,512]{2,1,0}, f16[512,512]{1,0}) tuple(f16[4,512,512]{2,1,0} %get-tuple-element.12.0, f16[512,512]{1,0} %all-reduce-done)\n} \n```", "```py\nTrue\nTrue \n```"]