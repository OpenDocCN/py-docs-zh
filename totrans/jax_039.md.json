["```py\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental import pallas as pl\n\ndef add_kernel(x_ref, y_ref, o_ref):\n  # In this code, `x_ref`, `y_ref` and `o_ref` are (8,)-shaped `Ref`s\n  x = x_ref[:]\n  y = y_ref[:]\n  o_ref[:] = x + y\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(add_kernel, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\nadd(x, y) \n```", "```py\ndef f(x_ref, o_ref):\n  # Using vanilla Python indexing\n  x = x_ref[0, 2:5, :]\n  # Or via Numpy advanced int indexing\n  o_ref[jnp.arange(3), :] = x\n\n# Note that in order to use NumPy advanced int indexing, you need to broadcast the indices against each other into the desired multidimensional shape:\ndef f(x_ref):\n  # Assume x_ref is (8, 4) and we want to read out a (2, 3) slice\n  x = x_ref[jnp.arange(2)[..., None], jnp.arange(3)[None, ...]] \n```", "```py\ndef f(x_ref, o_ref):\n  # Reading from memory via pallas.load\n  x = pl.load(x_ref, (0, slice(2, 5), slice(None)))\n  # Using integer indexing automatically broadcasts\n  x = pl.load(x_ref, (0, 2 + jnp.arange(3), slice(None)))\n  # You can also use `pl.dynamic_slice` (`pl.ds` for short) objects as well\n  pl.store(o_ref, (0, pl.ds(start=2, size=3), slice(None)), x) \n```", "```py\ndef f(x_ref, o_ref):\n  # Reading from memory via pallas.load\n  idx = jnp.arange(8)\n  mask = idx < 5\n  x = pl.load(x_ref, (idx,), mask=mask, other=float('-inf')) \n```", "```py\ndef f(x_ref, o_ref):\n  i = pl.program_id(axis=0)  # execution index in the first axis of the grid\n  o_ref[i] = jnp.exp(x_ref[i]) \n```", "```py\ndef pallas_call(\n    kernel: Callable,\n    in_specs: Sequence[Spec],\n    out_specs: Sequence[Spec],\n    out_shapes: Sequence[jax.ShapeDtypeStruct],\n    grid: Optional[Tuple[int, ...]] = None) -> Callable:\n  ... \n```", "```py\ndef pallas_call(kernel, in_specs, out_specs, out_shapes, grid):\n  def execute(*args):\n    outputs = map(empty_ref, out_shapes)\n    grid_indices = map(range, grid)\n    for indices in itertools.product(*grid_indices): # Could run in parallel!\n      local_inputs = [in_spec.transform(arg, indices) for arg, in_spec in\n                      zip(args, in_specs)]\n      local_outputs = [out_spec.transform(arg, indices) for arg, out_spec  in\n                       zip(outputs, out_specs)]\n      kernel(*local_inputs, *local_outputs) # writes to outputs\n  return execute \n```", "```py\nclass BlockSpec:\n  index_map: Callable[[Tuple[Int, ...]], Tuple[Int, ...]]\n  block_shape: Tuple[Optional[int], ...]\n\n  def transform(self, ref, *loop_indices):\n    block_indices = self.transform_function(loop_indices)\n    # Returns a view of `ref` starting at `block_indices` of shape self.block_shape\n    ... \n```", "```py\ndef make_kernel(eltwise_kernel):\n  def add(x_ref, y_ref, o_ref):\n    x = pl.load(x_ref, ())\n    y = pl.load(y_ref, ())\n    pl.store(o_ref, (), eltwise_kernel(x + y))\n  return add\n\nkernel1 = make_kernel(lambda x: x * 2)\nkernel2 = make_kernel(jnp.exp)\n\npl.pallas_call(kernel1, out_shape=x, grid=1)(1., 1.)\npl.pallas_call(kernel2, out_shape=x, grid=1)(1., 1.) \n```", "```py\ndef add_kernel(x_ref, y_ref, o_ref):\n  # In this code, `x_ref`, `y_ref` and `o_ref` are (2,)-shaped `Ref`s\n  x = x_ref[:]\n  y = y_ref[:]\n  o_ref[:] = x + y\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(\n    add_kernel,\n    out_shape=jax.ShapeDtypeStruct((8,), jnp.int32),\n    in_specs=[\n      pl.BlockSpec(lambda i: i, (2,)),\n      pl.BlockSpec(lambda i: i, (2,))\n    ],\n    out_specs=pl.BlockSpec(lambda i: i, (2,)),\n    grid=(4,))\nadd(x, y) \n```", "```py\ndef matmul_kernel(x_ref, y_ref, o_ref, *, activation, block_k):\n  acc = jnp.zeros((x_ref.shape[0], y_ref.shape[1]), jnp.float32)\n  for k in range(x_ref.shape[1] // block_k):\n    x = x_ref[:, k*block_k:(k+1)*block_k]\n    y = y_ref[k*block_k:(k+1)*block_k, :]\n    acc += x @ y\n  o_ref[:, :] = activation(acc).astype(o_ref.dtype)\n\nx, y = jnp.ones((512, 256)), jnp.ones((256, 1024))\nblock_shape = 128, 256, 128\n\n@partial(jax.jit, static_argnames=[\"block_shape\", \"activation\"])\ndef matmul(x, y, *, block_shape, activation):\n  block_m, block_n, block_k = block_shape\n  fused_matmul = pl.pallas_call(\n      partial(matmul_kernel, block_k=block_k, activation=activation),\n      out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1],), jnp.float32),\n      in_specs=[\n        pl.BlockSpec(lambda i, j: (i, 0), (block_m, x.shape[1])),\n        pl.BlockSpec(lambda i, j: (0, j), (y.shape[0], block_n))\n      ],\n      out_specs=pl.BlockSpec(lambda i, j: (i, j), (block_m, block_n)),\n      grid=(4, 4),\n  )\n  return fused_matmul(x, y)\n\nz = matmul(x, y, block_shape=block_shape, activation=jax.nn.gelu) \n```"]