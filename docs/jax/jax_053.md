# 自动微分手册

> 原文：[`jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html`](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)

![在 Colab 中打开](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb) ![在 Kaggle 中打开](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)

*alexbw@, mattjj@*

JAX 拥有非常通用的自动微分系统。在这本手册中，我们将介绍许多巧妙的自动微分思想，您可以根据自己的工作进行选择。

```py
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random

key = random.key(0) 
```

## 梯度

### 从`grad`开始

您可以使用`grad`对函数进行微分：

```py
grad_tanh = grad(jnp.tanh)
print(grad_tanh(2.0)) 
```

```py
0.070650816 
```

`grad`接受一个函数并返回一个函数。如果您有一个评估数学函数 \( f \) 的 Python 函数 `f`，那么 `grad(f)` 是一个评估数学函数 \( \nabla f \) 的 Python 函数。这意味着 `grad(f)(x)` 表示值 \( \nabla f(x) \)。

由于`grad`操作函数，您可以将其应用于其自身的输出以多次进行微分：

```py
print(grad(grad(jnp.tanh))(2.0))
print(grad(grad(grad(jnp.tanh)))(2.0)) 
```

```py
-0.13621868
0.25265405 
```

让我们看看如何在线性逻辑回归模型中使用`grad`计算梯度。首先，设置：

```py
def sigmoid(x):
    return 0.5 * (jnp.tanh(x / 2) + 1)

# Outputs probability of a label being true.
def predict(W, b, inputs):
    return sigmoid(jnp.dot(inputs, W) + b)

# Build a toy dataset.
inputs = jnp.array([[0.52, 1.12,  0.77],
                   [0.88, -1.08, 0.15],
                   [0.52, 0.06, -1.30],
                   [0.74, -2.49, 1.39]])
targets = jnp.array([True, True, False, True])

# Training loss is the negative log-likelihood of the training examples.
def loss(W, b):
    preds = predict(W, b, inputs)
    label_probs = preds * targets + (1 - preds) * (1 - targets)
    return -jnp.sum(jnp.log(label_probs))

# Initialize random model coefficients
key, W_key, b_key = random.split(key, 3)
W = random.normal(W_key, (3,))
b = random.normal(b_key, ()) 
```

使用`argnums`参数的`grad`函数来相对于位置参数微分函数。

```py
# Differentiate `loss` with respect to the first positional argument:
W_grad = grad(loss, argnums=0)(W, b)
print('W_grad', W_grad)

# Since argnums=0 is the default, this does the same thing:
W_grad = grad(loss)(W, b)
print('W_grad', W_grad)

# But we can choose different values too, and drop the keyword:
b_grad = grad(loss, 1)(W, b)
print('b_grad', b_grad)

# Including tuple values
W_grad, b_grad = grad(loss, (0, 1))(W, b)
print('W_grad', W_grad)
print('b_grad', b_grad) 
```

```py
W_grad [-0.16965583 -0.8774644  -1.4901346 ]
W_grad [-0.16965583 -0.8774644  -1.4901346 ]
b_grad -0.29227245
W_grad [-0.16965583 -0.8774644  -1.4901346 ]
b_grad -0.29227245 
```

此`grad` API 直接对应于 Spivak 经典著作*Calculus on Manifolds*（1965）中的优秀符号，也用于 Sussman 和 Wisdom 的*Structure and Interpretation of Classical Mechanics*（2015）及其*Functional Differential Geometry*（2013）。这两本书都是开放获取的。特别是参见*Functional Differential Geometry*的“序言”部分，以了解此符号的辩护。

当使用`argnums`参数时，如果`f`是一个用于计算数学函数 \( f \) 的 Python 函数，则 Python 表达式`grad(f, i)`用于评估 \( \partial_i f \) 的 Python 函数。

### 相对于嵌套列表、元组和字典进行微分

使用标准的 Python 容器进行微分是完全有效的，因此可以随意使用元组、列表和字典（以及任意嵌套）。

```py
def loss2(params_dict):
    preds = predict(params_dict['W'], params_dict['b'], inputs)
    label_probs = preds * targets + (1 - preds) * (1 - targets)
    return -jnp.sum(jnp.log(label_probs))

print(grad(loss2)({'W': W, 'b': b})) 
```

```py
{'W': Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32), 'b': Array(-0.29227245, dtype=float32)} 
```

您可以[注册您自己的容器类型](https://github.com/google/jax/issues/446#issuecomment-467105048)以便不仅与`grad`一起工作，还可以与所有 JAX 转换（`jit`、`vmap`等）一起工作。

### 使用`value_and_grad`评估函数及其梯度

另一个方便的函数是`value_and_grad`，可以高效地计算函数值及其梯度值：

```py
from jax import value_and_grad
loss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)
print('loss value', loss_value)
print('loss value', loss(W, b)) 
```

```py
loss value 3.0519385
loss value 3.0519385 
```

### 与数值差分进行对比

导数的一个很好的特性是它们很容易用有限差分进行检查：

```py
# Set a step size for finite differences calculations
eps = 1e-4

# Check b_grad with scalar finite differences
b_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps
print('b_grad_numerical', b_grad_numerical)
print('b_grad_autodiff', grad(loss, 1)(W, b))

# Check W_grad with finite differences in a random direction
key, subkey = random.split(key)
vec = random.normal(subkey, W.shape)
unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))
W_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps
print('W_dirderiv_numerical', W_grad_numerical)
print('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec)) 
```

```py
b_grad_numerical -0.29325485
b_grad_autodiff -0.29227245
W_dirderiv_numerical -0.2002716
W_dirderiv_autodiff -0.19909117 
```

JAX 提供了一个简单的便利函数，本质上执行相同的操作，但可以检查任何您喜欢的微分顺序：

```py
from jax.test_util import check_grads
check_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives 
```

### 使用 `grad`-of-`grad` 进行 Hessian 向量乘积

使用高阶 `grad` 可以构建一个 Hessian 向量乘积函数。 （稍后我们将编写一个更高效的实现，该实现混合了前向和反向模式，但这个实现将纯粹使用反向模式。）

在最小化平滑凸函数的[截断牛顿共轭梯度算法](https://en.wikipedia.org/wiki/Truncated_Newton_method)或研究神经网络训练目标的曲率（例如[1](https://arxiv.org/abs/1406.2572)，[2](https://arxiv.org/abs/1811.07062)，[3](https://arxiv.org/abs/1706.04454)，[4](https://arxiv.org/abs/1802.03451)）中，Hessian 向量乘积函数非常有用。

对于一个标量值函数 \( f : \mathbb{R}^n \to \mathbb{R} \)，具有连续的二阶导数（因此 Hessian 矩阵是对称的），点 \( x \in \mathbb{R}^n \) 处的 Hessian 被写为 \(\partial² f(x)\)。然后，Hessian 向量乘积函数能够评估

\(\qquad v \mapsto \partial² f(x) \cdot v\)

对于任意 \( v \in \mathbb{R}^n \)。

窍门在于不要实例化完整的 Hessian 矩阵：如果 \( n \) 很大，例如在神经网络的背景下可能是百万或十亿级别，那么可能无法存储。

幸运的是，`grad` 已经为我们提供了一种编写高效的 Hessian 向量乘积函数的方法。我们只需使用下面的身份证

\(\qquad \partial² f (x) v = \partial [x \mapsto \partial f(x) \cdot v] = \partial g(x)\)，

其中 \( g(x) = \partial f(x) \cdot v \) 是一个新的标量值函数，它将 \( f \) 在 \( x \) 处的梯度与向量 \( v \) 点乘。请注意，我们只对向量值参数的标量值函数进行微分，这正是我们知道 `grad` 高效的地方。

在 JAX 代码中，我们可以直接写成这样：

```py
def hvp(f, x, v):
    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x) 
```

这个例子表明，您可以自由使用词汇闭包，而 JAX 绝不会感到不安或困惑。

一旦我们看到如何计算密集的 Hessian 矩阵，我们将在几个单元格下检查此实现。我们还将编写一个更好的版本，该版本同时使用前向模式和反向模式。

### 使用 `jacfwd` 和 `jacrev` 计算 Jacobians 和 Hessians

您可以使用 `jacfwd` 和 `jacrev` 函数计算完整的 Jacobian 矩阵：

```py
from jax import jacfwd, jacrev

# Isolate the function from the weight matrix to the predictions
f = lambda W: predict(W, b, inputs)

J = jacfwd(f)(W)
print("jacfwd result, with shape", J.shape)
print(J)

J = jacrev(f)(W)
print("jacrev result, with shape", J.shape)
print(J) 
```

```py
jacfwd result, with shape (4, 3)
[[ 0.05981758  0.12883787  0.08857603]
 [ 0.04015916 -0.04928625  0.00684531]
 [ 0.12188288  0.01406341 -0.3047072 ]
 [ 0.00140431 -0.00472531  0.00263782]]
jacrev result, with shape (4, 3)
[[ 0.05981757  0.12883787  0.08857603]
 [ 0.04015916 -0.04928625  0.00684531]
 [ 0.12188289  0.01406341 -0.3047072 ]
 [ 0.00140431 -0.00472531  0.00263782]] 
```

这两个函数计算相同的值（直到机器数学），但它们在实现上有所不同：`jacfwd` 使用前向模式自动微分，对于“高”的 Jacobian 矩阵更有效，而 `jacrev` 使用反向模式，对于“宽”的 Jacobian 矩阵更有效。对于接近正方形的矩阵，`jacfwd` 可能比 `jacrev` 有优势。

您还可以在容器类型中使用 `jacfwd` 和 `jacrev`：

```py
def predict_dict(params, inputs):
    return predict(params['W'], params['b'], inputs)

J_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)
for k, v in J_dict.items():
    print("Jacobian from {} to logits is".format(k))
    print(v) 
```

```py
Jacobian from W to logits is
[[ 0.05981757  0.12883787  0.08857603]
 [ 0.04015916 -0.04928625  0.00684531]
 [ 0.12188289  0.01406341 -0.3047072 ]
 [ 0.00140431 -0.00472531  0.00263782]]
Jacobian from b to logits is
[0.11503381 0.04563541 0.23439017 0.00189771] 
```

关于前向模式和反向模式的更多细节，以及如何尽可能高效地实现 `jacfwd` 和 `jacrev`，请继续阅读！

使用两个这些函数的复合给我们一种计算密集的 Hessian 矩阵的方法：

```py
def hessian(f):
    return jacfwd(jacrev(f))

H = hessian(f)(W)
print("hessian, with shape", H.shape)
print(H) 
```

```py
hessian, with shape (4, 3, 3)
[[[ 0.02285465  0.04922541  0.03384247]
  [ 0.04922541  0.10602397  0.07289147]
  [ 0.03384247  0.07289147  0.05011288]]

 [[-0.03195215  0.03921401 -0.00544639]
  [ 0.03921401 -0.04812629  0.00668421]
  [-0.00544639  0.00668421 -0.00092836]]

 [[-0.01583708 -0.00182736  0.03959271]
  [-0.00182736 -0.00021085  0.00456839]
  [ 0.03959271  0.00456839 -0.09898177]]

 [[-0.00103524  0.00348343 -0.00194457]
  [ 0.00348343 -0.01172127  0.0065432 ]
  [-0.00194457  0.0065432  -0.00365263]]] 
```

这种形状是合理的：如果我们从一个函数 \(f : \mathbb{R}^n \to \mathbb{R}^m\) 开始，那么在点 \(x \in \mathbb{R}^n\) 我们期望得到以下形状

+   \(f(x) \in \mathbb{R}^m\)，在 \(x\) 处的 \(f\) 的值，

+   \(\partial f(x) \in \mathbb{R}^{m \times n}\)，在 \(x\) 处的雅可比矩阵，

+   \(\partial² f(x) \in \mathbb{R}^{m \times n \times n}\)，在 \(x\) 处的 Hessian 矩阵，

以及其他一些内容。

要实现 `hessian`，我们可以使用 `jacfwd(jacrev(f))` 或 `jacrev(jacfwd(f))` 或这两者的任何组合。但是前向超过反向通常是最有效的。这是因为在内部雅可比计算中，我们通常是在不同 iating 一个函数宽雅可比（也许像损失函数 \(f : \mathbb{R}^n \to \mathbb{R}\)），而在外部雅可比计算中，我们是在不同 iating 具有方雅可比的函数（因为 \(\nabla f : \mathbb{R}^n \to \mathbb{R}^n\)），这就是前向模式胜出的地方。

## 制造过程：两个基础的自动微分函数

### 雅可比-向量积（JVPs，也称为前向模式自动微分）

JAX 包括前向模式和反向模式自动微分的高效和通用实现。熟悉的 `grad` 函数建立在反向模式之上，但要解释两种模式的区别，以及每种模式何时有用，我们需要一些数学背景。

#### 数学中的雅可比向量积

在数学上，给定一个函数 \(f : \mathbb{R}^n \to \mathbb{R}^m\)，在输入点 \(x \in \mathbb{R}^n\) 处评估的雅可比矩阵 \(\partial f(x)\)，通常被视为一个 \(\mathbb{R}^m \times \mathbb{R}^n\) 中的矩阵：

\(\qquad \partial f(x) \in \mathbb{R}^{m \times n}\).

但我们也可以将 \(\partial f(x)\) 看作是一个线性映射，它将 \(f\) 的定义域在点 \(x\) 的切空间（即另一个 \(\mathbb{R}^n\) 的副本）映射到 \(f\) 的值域在点 \(f(x)\) 的切空间（一个 \(\mathbb{R}^m\) 的副本）：

\(\qquad \partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\).

此映射称为 \(f\) 在 \(x\) 处的[推前映射](https://en.wikipedia.org/wiki/Pushforward_(differential))。雅可比矩阵只是标准基中这个线性映射的矩阵。

如果我们不确定一个特定的输入点 \(x\)，那么我们可以将函数 \(\partial f\) 视为首先接受一个输入点并返回该输入点处的雅可比线性映射：

\(\qquad \partial f : \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m\)。

特别是，我们可以解开事物，这样给定输入点 \(x \in \mathbb{R}^n\) 和切向量 \(v \in \mathbb{R}^n\)，我们得到一个输出切向量在 \(\mathbb{R}^m\) 中。我们称这种映射，从 \((x, v)\) 对到输出切向量，为*雅可比向量积*，并将其写为

\(\qquad (x, v) \mapsto \partial f(x) v\)

#### JAX 代码中的雅可比向量积

回到 Python 代码中，JAX 的 `jvp` 函数模拟了这种转换。给定一个评估 \(f\) 的 Python 函数，JAX 的 `jvp` 是获取评估 \((x, v) \mapsto (f(x), \partial f(x) v)\) 的 Python 函数的一种方法。

```py
from jax import jvp

# Isolate the function from the weight matrix to the predictions
f = lambda W: predict(W, b, inputs)

key, subkey = random.split(key)
v = random.normal(subkey, W.shape)

# Push forward the vector `v` along `f` evaluated at `W`
y, u = jvp(f, (W,), (v,)) 
```

用类似 Haskell 的类型签名来说，我们可以写成

```py
jvp  ::  (a  ->  b)  ->  a  ->  T  a  ->  (b,  T  b) 
```

在这里，我们使用 `T a` 来表示 `a` 的切线空间的类型。简言之，`jvp` 接受一个类型为 `a -> b` 的函数作为参数，一个类型为 `a` 的值，以及一个类型为 `T a` 的切线向量值。它返回一个由类型为 `b` 的值和类型为 `T b` 的输出切线向量组成的对。

`jvp` 转换后的函数的评估方式与原函数类似，但与每个类型为 `a` 的原始值配对时，它会沿着类型为 `T a` 的切线值进行推进。对于原始函数将应用的每个原始数值操作，`jvp` 转换后的函数会执行一个“JVP 规则”，该规则同时在这些原始值上评估原始数值，并应用其 JVP。

该评估策略对计算复杂度有一些直接影响：由于我们在进行评估时同时评估 JVP，因此我们不需要为以后存储任何内容，因此内存成本与计算深度无关。此外，`jvp` 转换后的函数的 FLOP 成本约为评估函数的成本的 3 倍（例如对于评估原始函数的一个单位工作，如 `sin(x)`；一个单位用于线性化，如 `cos(x)`；和一个单位用于将线性化函数应用于向量，如 `cos_x * v`）。换句话说，对于固定的原始点 \(x\)，我们可以以大致相同的边际成本评估 \(v \mapsto \partial f(x) \cdot v\)，如同评估 \(f\) 一样。

那么内存复杂度听起来非常有说服力！那为什么我们在机器学习中很少见到正向模式呢？

要回答这个问题，首先考虑如何使用 JVP 构建完整的 Jacobian 矩阵。如果我们将 JVP 应用于一个单位切线向量，它会显示出我们输入的非零条目对应的 Jacobian 矩阵的一列。因此，我们可以逐列地构建完整的 Jacobian 矩阵，获取每列的成本大约与一个函数评估相同。对于具有“高”Jacobian 的函数来说，这将是高效的，但对于“宽”Jacobian 来说则效率低下。

如果你在机器学习中进行基于梯度的优化，你可能想要最小化一个从 \(\mathbb{R}^n\) 中的参数到 \(\mathbb{R}\) 中标量损失值的损失函数。这意味着这个函数的雅可比矩阵是一个非常宽的矩阵：\(\partial f(x) \in \mathbb{R}^{1 \times n}\)，我们通常将其视为梯度向量 \(\nabla f(x) \in \mathbb{R}^n\)。逐列构建这个矩阵，每次调用需要类似数量的浮点运算来评估原始函数，看起来确实效率低下！特别是对于训练神经网络，其中 \(f\) 是一个训练损失函数，而 \(n\) 可以是百万或十亿级别，这种方法根本不可扩展。

为了更好地处理这类函数，我们只需要使用反向模式。### 向量-雅可比积（VJPs，又称反向自动微分）

在前向模式中，我们得到了一个用于评估雅可比向量积的函数，然后我们可以使用它逐列构建雅可比矩阵；而反向模式则是一种获取用于评估向量-雅可比积（或等效地雅可比-转置向量积）的函数的方式，我们可以用它逐行构建雅可比矩阵。

#### 数学中的 VJPs

再次考虑一个函数 \(f : \mathbb{R}^n \to \mathbb{R}^m\)。从我们对 JVP 的表示开始，对于 VJP 的表示非常简单：

\(\qquad (x, v) \mapsto v \partial f(x)\),

其中 \(v\) 是在 \(x\) 处 \(f\) 的余切空间的元素（同构于另一个 \(\mathbb{R}^m\) 的副本）。在严格时，我们应该将 \(v\) 视为一个线性映射 \(v : \mathbb{R}^m \to \mathbb{R}\)，当我们写 \(v \partial f(x)\) 时，我们意味着函数复合 \(v \circ \partial f(x)\)，其中类型之间的对应关系是因为 \(\partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\)。但在通常情况下，我们可以将 \(v\) 与 \(\mathbb{R}^m\) 中的一个向量等同看待，并几乎可以互换使用，就像有时我们可以在“列向量”和“行向量”之间轻松切换而不加过多评论一样。

有了这个认识，我们可以将 VJP 的线性部分看作是 JVP 线性部分的转置（或共轭伴随）：

\(\qquad (x, v) \mapsto \partial f(x)^\mathsf{T} v\).

对于给定点 \(x\)，我们可以将签名写为

\(\qquad \partial f(x)^\mathsf{T} : \mathbb{R}^m \to \mathbb{R}^n\).

对应的余切空间映射通常称为\[拉回](https://en.wikipedia.org/wiki/Pullback_(differential_geometry)) \(f\) 在 \(x\) 处的。对我们而言，关键在于它从类似 \(f\) 输出的东西到类似 \(f\) 输入的东西，就像我们从一个转置线性函数所期望的那样。

#### JAX 代码中的 VJPs

从数学切换回 Python，JAX 函数 `vjp` 可以接受一个用于评估 \(f\) 的 Python 函数，并给我们返回一个用于评估 VJP \((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\) 的 Python 函数。

```py
from jax import vjp

# Isolate the function from the weight matrix to the predictions
f = lambda W: predict(W, b, inputs)

y, vjp_fun = vjp(f, W)

key, subkey = random.split(key)
u = random.normal(subkey, y.shape)

# Pull back the covector `u` along `f` evaluated at `W`
v = vjp_fun(u) 
```

就[类似 Haskell 类型签名](https://wiki.haskell.org/Type_signature)的形式来说，我们可以写成

```py
vjp  ::  (a  ->  b)  ->  a  ->  (b,  CT  b  ->  CT  a) 
```

在这里，我们使用 `CT a` 表示 `a` 的余切空间的类型。换句话说，`vjp` 接受类型为 `a -> b` 的函数和类型为 `a` 的点作为参数，并返回一个由类型为 `b` 的值和类型为 `CT b -> CT a` 的线性映射组成的对。

这很棒，因为它让我们一次一行地构建雅可比矩阵，并且评估 \((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\) 的 FLOP 成本仅约为评估 \(f\) 的三倍。特别是，如果我们想要函数 \(f : \mathbb{R}^n \to \mathbb{R}\) 的梯度，我们可以一次性完成。这就是 `grad` 对基于梯度的优化非常高效的原因，即使是对于数百万或数十亿个参数的神经网络训练损失函数这样的目标。

这里有一个成本：虽然 FLOP 友好，但内存随计算深度而增加。而且，该实现在传统上比前向模式更为复杂，但 JAX 对此有一些窍门（这是未来笔记本的故事！）。

关于反向模式的工作原理，可以查看[2017 年深度学习暑期学校的教程视频](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)。

### 使用 VJPs 的矢量值梯度

如果你对使用矢量值梯度（如 `tf.gradients`）感兴趣：

```py
from jax import vjp

def vgrad(f, x):
  y, vjp_fn = vjp(f, x)
  return vjp_fn(jnp.ones(y.shape))[0]

print(vgrad(lambda x: 3*x**2, jnp.ones((2, 2)))) 
```

```py
[[6\. 6.]
 [6\. 6.]] 
```

### 使用前向和反向模式的黑塞矢量积

在前面的部分中，我们仅使用反向模式实现了一个黑塞-矢量积函数（假设具有连续二阶导数）：

```py
def hvp(f, x, v):
    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x) 
```

这是高效的，但我们甚至可以更好地节省一些内存，通过使用前向模式和反向模式。

从数学上讲，给定一个要区分的函数 \(f : \mathbb{R}^n \to \mathbb{R}\)，要线性化函数的一个点 \(x \in \mathbb{R}^n\)，以及一个向量 \(v \in \mathbb{R}^n\)，我们想要的黑塞-矢量积函数是

\((x, v) \mapsto \partial² f(x) v\)

考虑助手函数 \(g : \mathbb{R}^n \to \mathbb{R}^n\) 定义为 \(f\) 的导数（或梯度），即 \(g(x) = \partial f(x)\)。我们所需的只是它的 JVP，因为这将给我们

\((x, v) \mapsto \partial g(x) v = \partial² f(x) v\).

我们几乎可以直接将其转换为代码：

```py
from jax import jvp, grad

# forward-over-reverse
def hvp(f, primals, tangents):
  return jvp(grad(f), primals, tangents)[1] 
```

更好的是，由于我们不需要直接调用 `jnp.dot`，这个 `hvp` 函数可以处理任何形状的数组以及任意的容器类型（如嵌套列表/字典/元组中存储的向量），甚至与`jax.numpy` 没有任何依赖。

这是如何使用它的示例：

```py
def f(X):
  return jnp.sum(jnp.tanh(X)**2)

key, subkey1, subkey2 = random.split(key, 3)
X = random.normal(subkey1, (30, 40))
V = random.normal(subkey2, (30, 40))

ans1 = hvp(f, (X,), (V,))
ans2 = jnp.tensordot(hessian(f)(X), V, 2)

print(jnp.allclose(ans1, ans2, 1e-4, 1e-4)) 
```

```py
True 
```

另一种你可能考虑写这个的方法是使用反向-前向模式：

```py
# reverse-over-forward
def hvp_revfwd(f, primals, tangents):
  g = lambda primals: jvp(f, primals, tangents)[1]
  return grad(g)(primals) 
```

不过，这不是很好，因为前向模式的开销比反向模式小，由于外部区分算子要区分比内部更大的计算，将前向模式保持在外部是最好的：

```py
# reverse-over-reverse, only works for single arguments
def hvp_revrev(f, primals, tangents):
  x, = primals
  v, = tangents
  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)

print("Forward over reverse")
%timeit -n10 -r3 hvp(f, (X,), (V,))
print("Reverse over forward")
%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))
print("Reverse over reverse")
%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))

print("Naive full Hessian materialization")
%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2) 
```

```py
Forward over reverse
4.74 ms ± 157 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)
Reverse over forward
9.46 ms ± 5.05 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)
Reverse over reverse
14.3 ms ± 7.71 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)
Naive full Hessian materialization
57.7 ms ± 1.32 ms per loop (mean ± std. dev. of 3 runs, 10 loops each) 
```

## 组成 VJP、JVP 和 `vmap`

### 雅可比-矩阵和矩阵-雅可比乘积

现在我们有`jvp`和`vjp`变换，它们为我们提供了推送或拉回单个向量的函数，我们可以使用 JAX 的`vmap` [变换](https://github.com/google/jax#auto-vectorization-with-vmap)一次推送和拉回整个基。特别是，我们可以用它来快速编写矩阵-雅可比和雅可比-矩阵乘积。

```py
# Isolate the function from the weight matrix to the predictions
f = lambda W: predict(W, b, inputs)

# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.
# First, use a list comprehension to loop over rows in the matrix M.
def loop_mjp(f, x, M):
    y, vjp_fun = vjp(f, x)
    return jnp.vstack([vjp_fun(mi) for mi in M])

# Now, use vmap to build a computation that does a single fast matrix-matrix
# multiply, rather than an outer loop over vector-matrix multiplies.
def vmap_mjp(f, x, M):
    y, vjp_fun = vjp(f, x)
    outs, = vmap(vjp_fun)(M)
    return outs

key = random.key(0)
num_covecs = 128
U = random.normal(key, (num_covecs,) + y.shape)

loop_vs = loop_mjp(f, W, M=U)
print('Non-vmapped Matrix-Jacobian product')
%timeit -n10 -r3 loop_mjp(f, W, M=U)

print('\nVmapped Matrix-Jacobian product')
vmap_vs = vmap_mjp(f, W, M=U)
%timeit -n10 -r3 vmap_mjp(f, W, M=U)

assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical' 
```

```py
Non-vmapped Matrix-Jacobian product
168 ms ± 260 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)

Vmapped Matrix-Jacobian product
6.39 ms ± 49.3 μs per loop (mean ± std. dev. of 3 runs, 10 loops each) 
```

```py
/tmp/ipykernel_1379/3769736790.py:8: DeprecationWarning: vstack requires ndarray or scalar arguments, got <class 'tuple'> at position 0\. In a future JAX release this will be an error.
  return jnp.vstack([vjp_fun(mi) for mi in M]) 
```

```py
def loop_jmp(f, W, M):
    # jvp immediately returns the primal and tangent values as a tuple,
    # so we'll compute and select the tangents in a list comprehension
    return jnp.vstack([jvp(f, (W,), (mi,))[1] for mi in M])

def vmap_jmp(f, W, M):
    _jvp = lambda s: jvp(f, (W,), (s,))[1]
    return vmap(_jvp)(M)

num_vecs = 128
S = random.normal(key, (num_vecs,) + W.shape)

loop_vs = loop_jmp(f, W, M=S)
print('Non-vmapped Jacobian-Matrix product')
%timeit -n10 -r3 loop_jmp(f, W, M=S)
vmap_vs = vmap_jmp(f, W, M=S)
print('\nVmapped Jacobian-Matrix product')
%timeit -n10 -r3 vmap_jmp(f, W, M=S)

assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Jacobian-Matrix products should be identical' 
```

```py
Non-vmapped Jacobian-Matrix product
290 ms ± 437 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)

Vmapped Jacobian-Matrix product
3.29 ms ± 22.5 μs per loop (mean ± std. dev. of 3 runs, 10 loops each) 
```

### `jacfwd`和`jacrev`的实现

现在我们已经看到了快速的雅可比-矩阵和矩阵-雅可比乘积，写出`jacfwd`和`jacrev`并不难。我们只需使用相同的技术一次推送或拉回整个标准基（等同于单位矩阵）。

```py
from jax import jacrev as builtin_jacrev

def our_jacrev(f):
    def jacfun(x):
        y, vjp_fun = vjp(f, x)
        # Use vmap to do a matrix-Jacobian product.
        # Here, the matrix is the Euclidean basis, so we get all
        # entries in the Jacobian at once. 
        J, = vmap(vjp_fun, in_axes=0)(jnp.eye(len(y)))
        return J
    return jacfun

assert jnp.allclose(builtin_jacrev(f)(W), our_jacrev(f)(W)), 'Incorrect reverse-mode Jacobian results!' 
```

```py
from jax import jacfwd as builtin_jacfwd

def our_jacfwd(f):
    def jacfun(x):
        _jvp = lambda s: jvp(f, (x,), (s,))[1]
        Jt =vmap(_jvp, in_axes=1)(jnp.eye(len(x)))
        return jnp.transpose(Jt)
    return jacfun

assert jnp.allclose(builtin_jacfwd(f)(W), our_jacfwd(f)(W)), 'Incorrect forward-mode Jacobian results!' 
```

有趣的是，[Autograd](https://github.com/hips/autograd)做不到这一点。我们在 Autograd 中反向模式`jacobian`的[实现](https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/differential_operators.py#L60)必须逐个向量地拉回，使用外层循环`map`。逐个向量地通过计算远不及使用`vmap`一次将所有内容批处理高效。

另一件 Autograd 做不到的事情是`jit`。有趣的是，无论您在要进行微分的函数中使用多少 Python 动态性，我们总是可以在计算的线性部分上使用`jit`。例如：

```py
def f(x):
    try:
        if x < 3:
            return 2 * x ** 3
        else:
            raise ValueError
    except ValueError:
        return jnp.pi * x

y, f_vjp = vjp(f, 4.)
print(jit(f_vjp)(1.)) 
```

```py
(Array(3.1415927, dtype=float32, weak_type=True),) 
```

## 复数和微分

JAX 在复数和微分方面表现出色。为了支持[全纯和非全纯微分](https://en.wikipedia.org/wiki/Holomorphic_function)，理解 JVP 和 VJP 很有帮助。

考虑一个复到复的函数 \(f: \mathbb{C} \to \mathbb{C}\) 并将其与相应的函数 \(g: \mathbb{R}² \to \mathbb{R}²\) 对应起来，

```py
def f(z):
  x, y = jnp.real(z), jnp.imag(z)
  return u(x, y) + v(x, y) * 1j

def g(x, y):
  return (u(x, y), v(x, y)) 
```

也就是说，我们分解了 \(f(z) = u(x, y) + v(x, y) i\) 其中 \(z = x + y i\)，并将 \(\mathbb{C}\) 与 \(\mathbb{R}²\) 对应起来得到了 \(g\)。

由于 \(g\) 只涉及实数输入和输出，我们已经知道如何为它编写雅可比-向量积，例如给定切向量 \((c, d) \in \mathbb{R}²\)，

\(\begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0 v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).

要获得应用于切向量 \(c + di \in \mathbb{C}\) 的原始函数 \(f\) 的 JVP，我们只需使用相同的定义，并将结果标识为另一个复数，

\(\partial f(x + y i)(c + d i) = \begin{matrix} \begin{bmatrix} 1 & i \end{bmatrix} \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0 v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix}\).

这就是我们对复到复函数 \(f\) 的 JVP 的定义！注意，无论 \(f\) 是否全纯，JVP 都是明确的。

这里是一个检查：

```py
def check(seed):
  key = random.key(seed)

  # random coeffs for u and v
  key, subkey = random.split(key)
  a, b, c, d = random.uniform(subkey, (4,))

  def fun(z):
    x, y = jnp.real(z), jnp.imag(z)
    return u(x, y) + v(x, y) * 1j

  def u(x, y):
    return a * x + b * y

  def v(x, y):
    return c * x + d * y

  # primal point
  key, subkey = random.split(key)
  x, y = random.uniform(subkey, (2,))
  z = x + y * 1j

  # tangent vector
  key, subkey = random.split(key)
  c, d = random.uniform(subkey, (2,))
  z_dot = c + d * 1j

  # check jvp
  _, ans = jvp(fun, (z,), (z_dot,))
  expected = (grad(u, 0)(x, y) * c +
              grad(u, 1)(x, y) * d +
              grad(v, 0)(x, y) * c * 1j+
              grad(v, 1)(x, y) * d * 1j)
  print(jnp.allclose(ans, expected)) 
```

```py
check(0)
check(1)
check(2) 
```

```py
True
True
True 
```

那么 VJP 呢？我们做了类似的事情：对于余切向量 \(c + di \in \mathbb{C}\)，我们将 \(f\) 的 VJP 定义为

\((c + di)^* \; \partial f(x + y i) = \begin{matrix} \begin{bmatrix} c & -d \end{bmatrix} \\ ~ \end{matrix} \begin{bmatrix} \partial_0 u(x, y) & \partial_1 u(x, y) \\ \partial_0 v(x, y) & \partial_1 v(x, y) \end{bmatrix} \begin{bmatrix} 1 \\ -i \end{bmatrix}\).

为什么要有负号？这些只是为了处理复共轭，以及我们正在处理余切向量的事实。

这里是 VJP 规则的检查：

```py
def check(seed):
  key = random.key(seed)

  # random coeffs for u and v
  key, subkey = random.split(key)
  a, b, c, d = random.uniform(subkey, (4,))

  def fun(z):
    x, y = jnp.real(z), jnp.imag(z)
    return u(x, y) + v(x, y) * 1j

  def u(x, y):
    return a * x + b * y

  def v(x, y):
    return c * x + d * y

  # primal point
  key, subkey = random.split(key)
  x, y = random.uniform(subkey, (2,))
  z = x + y * 1j

  # cotangent vector
  key, subkey = random.split(key)
  c, d = random.uniform(subkey, (2,))
  z_bar = jnp.array(c + d * 1j)  # for dtype control

  # check vjp
  _, fun_vjp = vjp(fun, z)
  ans, = fun_vjp(z_bar)
  expected = (grad(u, 0)(x, y) * c +
              grad(v, 0)(x, y) * (-d) +
              grad(u, 1)(x, y) * c * (-1j) +
              grad(v, 1)(x, y) * (-d) * (-1j))
  assert jnp.allclose(ans, expected, atol=1e-5, rtol=1e-5) 
```

```py
check(0)
check(1)
check(2) 
```

方便的包装器如`grad`、`jacfwd`和`jacrev`有什么作用？

对于\(\mathbb{R} \to \mathbb{R}\)函数，回想我们定义`grad(f)(x)`为`vjp(f, x)1`，这是因为将 VJP 应用于`1.0`值会显示梯度（即雅可比矩阵或导数）。对于\(\mathbb{C} \to \mathbb{R}\)函数，我们可以做同样的事情：我们仍然可以使用`1.0`作为余切向量，而我们得到的只是总结完整雅可比矩阵的一个复数结果：

```py
def f(z):
  x, y = jnp.real(z), jnp.imag(z)
  return x**2 + y**2

z = 3. + 4j
grad(f)(z) 
```

```py
Array(6.-8.j, dtype=complex64) 
```

对于一般的\(\mathbb{C} \to \mathbb{C}\)函数，雅可比矩阵有 4 个实值自由度（如上面的 2x2 雅可比矩阵），因此我们不能希望在一个复数中表示所有这些自由度。但对于全纯函数，我们可以！全纯函数恰好是一个\(\mathbb{C} \to \mathbb{C}\)函数，其导数可以表示为一个单一的复数。（[柯西-黎曼方程](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations)确保上述 2x2 雅可比矩阵在复平面内的作用具有复数乘法下的比例和旋转矩阵的特殊形式。）我们可以使用一个`vjp`调用并带有`1.0`的余切向量来揭示那一个复数。

因为这仅适用于全纯函数，为了使用这个技巧，我们需要向 JAX 保证我们的函数是全纯的；否则，在复数输出函数上使用`grad`时，JAX 会引发错误：

```py
def f(z):
  return jnp.sin(z)

z = 3. + 4j
grad(f, holomorphic=True)(z) 
```

```py
Array(-27.034945-3.8511531j, dtype=complex64, weak_type=True) 
```

`holomorphic=True`的承诺仅仅是在输出是复数值时禁用错误。当函数不是全纯时，我们仍然可以写`holomorphic=True`，但得到的答案将不表示完整的雅可比矩阵。相反，它将是在我们只丢弃输出的虚部的函数的雅可比矩阵。

```py
def f(z):
  return jnp.conjugate(z)

z = 3. + 4j
grad(f, holomorphic=True)(z)  # f is not actually holomorphic! 
```

```py
Array(1.-0.j, dtype=complex64, weak_type=True) 
```

在这里`grad`的工作有一些有用的结论：

1.  我们可以在全纯的\(\mathbb{C} \to \mathbb{C}\)函数上使用`grad`。

1.  我们可以使用`grad`来优化\(\mathbb{C} \to \mathbb{R}\)函数，例如复参数`x`的实值损失函数，通过朝着`grad(f)(x)`的共轭方向迈出步伐。

1.  如果我们有一个\(\mathbb{R} \to \mathbb{R}\)的函数，它恰好在内部使用一些复数运算（其中一些必须是非全纯的，例如在卷积中使用的 FFT），那么`grad`仍然有效，并且我们得到与仅使用实数值的实现相同的结果。

在任何情况下，JVPs 和 VJPs 都是明确的。如果我们想计算非全纯函数\(\mathbb{C} \to \mathbb{C}\)的完整 Jacobian 矩阵，我们可以用 JVPs 或 VJPs 来做到！

你应该期望复数在 JAX 中的任何地方都能正常工作。这里是通过复杂矩阵的 Cholesky 分解进行微分：

```py
A = jnp.array([[5.,    2.+3j,    5j],
              [2.-3j,   7.,  1.+7j],
              [-5j,  1.-7j,    12.]])

def f(X):
    L = jnp.linalg.cholesky(X)
    return jnp.sum((L - jnp.sin(L))**2)

grad(f, holomorphic=True)(A) 
```

```py
Array([[-0.7534186  +0.j       , -3.0509028 -10.940544j ,
         5.9896846  +3.5423026j],
       [-3.0509028 +10.940544j , -8.904491   +0.j       ,
        -5.1351523  -6.559373j ],
       [ 5.9896846  -3.5423026j, -5.1351523  +6.559373j ,
         0.01320427 +0.j       ]], dtype=complex64) 
```

## 更高级的自动微分

在这本笔记本中，我们通过一些简单的，然后逐渐复杂的应用中，使用 JAX 中的自动微分。我们希望现在您感觉在 JAX 中进行导数运算既简单又强大。

还有很多其他自动微分的技巧和功能。我们没有涵盖的主题，但希望在“高级自动微分手册”中进行涵盖：

+   高斯-牛顿向量乘积，一次线性化

+   自定义的 VJPs 和 JVPs

+   在固定点处高效地求导

+   使用随机的 Hessian-vector products 来估计 Hessian 的迹。

+   仅使用反向模式自动微分的前向模式自动微分。

+   对自定义数据类型进行导数计算。

+   检查点（二项式检查点用于高效的反向模式，而不是模型快照）。

+   优化 VJPs 通过 Jacobian 预积累。
