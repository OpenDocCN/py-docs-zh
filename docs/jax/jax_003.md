# 快速入门

> 原文：[`jax.readthedocs.io/en/latest/quickstart.html`](https://jax.readthedocs.io/en/latest/quickstart.html)

**JAX 是一个面向数组的数值计算库（*à la* [NumPy](https://numpy.org/)），具有自动微分和 JIT 编译功能，以支持高性能的机器学习研究**。

本文档提供了 JAX 主要功能的快速概述，让您可以快速开始使用 JAX：

+   JAX 提供了一个统一的类似于 NumPy 的接口，用于在 CPU、GPU 或 TPU 上运行的计算，在本地或分布式设置中。

+   JAX 通过 [Open XLA](https://github.com/openxla) 内置了即时编译（JIT）功能，这是一个开源的机器学习编译器生态系统。

+   JAX 函数支持通过其自动微分转换有效地评估梯度。

+   JAX 函数可以自动向量化，以有效地将它们映射到表示输入批次的数组上。

## 安装

可以直接从 [Python Package Index](https://pypi.org/project/jax/) 安装 JAX 用于 Linux、Windows 和 macOS 上的 CPU：

```py
pip install jax 
```

或者，对于 NVIDIA GPU：

```py
pip install -U "jax[cuda12]" 
```

如需更详细的特定平台安装信息，请查看安装 JAX。

## JAX 就像 NumPy 一样

大多数 JAX 的使用是通过熟悉的 `jax.numpy` API 进行的，通常在 `jnp` 别名下导入：

```py
import jax.numpy as jnp 
```

通过这个导入，您可以立即像使用典型的 NumPy 程序一样使用 JAX，包括使用 NumPy 风格的数组创建函数、Python 函数和操作符，以及数组属性和方法：

```py
def selu(x, alpha=1.67, lmbda=1.05):
  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)

x = jnp.arange(5.0)
print(selu(x)) 
```

```py
[0\.        1.05      2.1       3.1499999 4.2      ] 
```

一旦您开始深入研究，您会发现 JAX 数组和 NumPy 数组之间存在一些差异；这些差异在 [🔪 JAX - The Sharp Bits 🔪](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) 中进行了探讨。

## 使用`jax.jit()`进行即时编译

JAX 可以在 GPU 或 TPU 上透明运行（如果没有，则退回到 CPU）。然而，在上述示例中，JAX 是一次将核心分派到芯片上的操作。如果我们有一系列操作，我们可以使用 `jax.jit()` 函数将这些操作一起编译为 XLA。

我们可以使用 IPython 的 `%timeit` 快速测试我们的 `selu` 函数，使用 `block_until_ready()` 来考虑 JAX 的动态分派（请参阅异步分派）：

```py
from jax import random

key = random.key(1701)
x = random.normal(key, (1_000_000,))
%timeit selu(x).block_until_ready() 
```

```py
2.84 ms ± 9.23 μs per loop (mean ± std. dev. of 7 runs, 100 loops each) 
```

（请注意，我们已经使用 `jax.random` 生成了一些随机数；有关如何在 JAX 中生成随机数的详细信息，请查看伪随机数）。

我们可以使用 `jax.jit()` 转换来加速此函数的执行，该转换将在首次调用 `selu` 时进行 JIT 编译，并在此后进行缓存。

```py
from jax import jit

selu_jit = jit(selu)
_ = selu_jit(x)  # compiles on first call
%timeit selu_jit(x).block_until_ready() 
```

```py
844 μs ± 2.73 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) 
```

上述时间表示在 CPU 上执行，但同样的代码可以在 GPU 或 TPU 上运行，通常会有更大的加速效果。

欲了解更多关于 JAX 中 JIT 编译的信息，请查看即时编译。

## 使用 `jax.grad()` 计算导数

除了通过 JIT 编译转换函数外，JAX 还提供其他转换功能。其中一种转换是 `jax.grad()`，它执行[自动微分 (autodiff)](https://en.wikipedia.org/wiki/Automatic_differentiation)：

```py
from jax import grad

def sum_logistic(x):
  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))

x_small = jnp.arange(3.)
derivative_fn = grad(sum_logistic)
print(derivative_fn(x_small)) 
```

```py
[0.25       0.19661197 0.10499357] 
```

让我们用有限差分来验证我们的结果是否正确。

```py
def first_finite_differences(f, x, eps=1E-3):
  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)
                   for v in jnp.eye(len(x))])

print(first_finite_differences(sum_logistic, x_small)) 
```

```py
[0.24998187 0.1965761  0.10502338] 
```

`grad()` 和 `jit()` 转换可以任意组合并混合使用。在上面的示例中，我们对 `sum_logistic` 进行了 JIT 编译，然后取了它的导数。我们可以进一步进行：

```py
print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0)) 
```

```py
-0.0353256 
```

除了标量值函数外，`jax.jacobian()` 转换还可用于计算向量值函数的完整雅可比矩阵：

```py
from jax import jacobian
print(jacobian(jnp.exp)(x_small)) 
```

```py
[[1\.        0\.        0\.       ]
 [0\.        2.7182817 0\.       ]
 [0\.        0\.        7.389056 ]] 
```

对于更高级的自动微分操作，您可以使用 `jax.vjp()` 来进行反向模式向量-雅可比积分，以及使用 `jax.jvp()` 和 `jax.linearize()` 进行正向模式雅可比-向量积分。这两者可以任意组合，也可以与其他 JAX 转换组合使用。例如，`jax.jvp()` 和 `jax.vjp()` 用于定义正向模式 `jax.jacfwd()` 和反向模式 `jax.jacrev()`，用于计算正向和反向模式下的雅可比矩阵。以下是组合它们以有效计算完整 Hessian 矩阵的一种方法：

```py
from jax import jacfwd, jacrev
def hessian(fun):
  return jit(jacfwd(jacrev(fun)))
print(hessian(sum_logistic)(x_small)) 
```

```py
[[-0\.         -0\.         -0\.        ]
 [-0\.         -0.09085776 -0\.        ]
 [-0\.         -0\.         -0.07996249]] 
```

这种组合在实践中产生了高效的代码；这基本上是 JAX 内置的 `jax.hessian()` 函数的实现方式。

想了解更多关于 JAX 中的自动微分，请查看自动微分。

## 使用 `jax.vmap()` 进行自动向量化

另一个有用的转换是 `vmap()`，即向量化映射。它具有沿数组轴映射函数的熟悉语义，但与显式循环函数调用不同，它将函数转换为本地向量化版本，以获得更好的性能。与 `jit()` 组合时，它可以与手动重写函数以处理额外批处理维度的性能相媲美。

我们将处理一个简单的示例，并使用 `vmap()` 将矩阵-向量乘法提升为矩阵-矩阵乘法。虽然在这种特定情况下手动完成这一点很容易，但相同的技术也适用于更复杂的函数。

```py
key1, key2 = random.split(key)
mat = random.normal(key1, (150, 100))
batched_x = random.normal(key2, (10, 100))

def apply_matrix(x):
  return jnp.dot(mat, x) 
```

`apply_matrix` 函数将一个向量映射到另一个向量，但我们可能希望将其逐行应用于矩阵。在 Python 中，我们可以通过循环遍历批处理维度来实现这一点，但通常导致性能不佳。

```py
def naively_batched_apply_matrix(v_batched):
  return jnp.stack([apply_matrix(v) for v in v_batched])

print('Naively batched')
%timeit naively_batched_apply_matrix(batched_x).block_until_ready() 
```

```py
Naively batched
962 μs ± 1.54 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) 
```

熟悉 `jnp.dot` 函数的程序员可能会意识到，可以重写 `apply_matrix` 来避免显式循环，利用 `jnp.dot` 的内置批处理语义：

```py
import numpy as np

@jit
def batched_apply_matrix(batched_x):
  return jnp.dot(batched_x, mat.T)

np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),
                           batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)
print('Manually batched')
%timeit batched_apply_matrix(batched_x).block_until_ready() 
```

```py
Manually batched
14.3 μs ± 28.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) 
```

然而，随着函数变得更加复杂，这种手动批处理变得更加困难且容易出错。`vmap()` 转换旨在自动将函数转换为支持批处理的版本：

```py
from jax import vmap

@jit
def vmap_batched_apply_matrix(batched_x):
  return vmap(apply_matrix)(batched_x)

np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),
                           vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)
print('Auto-vectorized with vmap')
%timeit vmap_batched_apply_matrix(batched_x).block_until_ready() 
```

```py
Auto-vectorized with vmap
21.7 μs ± 98.7 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) 
```

正如您所预期的那样，`vmap()` 可以与 `jit()`、`grad()` 和任何其他 JAX 转换任意组合。

想了解更多关于 JAX 中的自动向量化，请查看自动向量化。

这只是 JAX 能做的一小部分。我们非常期待看到你用它做些什么！
