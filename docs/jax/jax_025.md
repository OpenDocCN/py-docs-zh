# GPU 性能提示

> 原文：[`jax.readthedocs.io/en/latest/gpu_performance_tips.html`](https://jax.readthedocs.io/en/latest/gpu_performance_tips.html)

本文档专注于神经网络工作负载的性能提示。

## 矩阵乘法精度

在像 Nvidia A100 一代或更高的最新 GPU 代中，将大多数计算以 `bfloat16` 精度执行可能是个好主意。例如，如果使用 [Flax](https://github.com/google/flax)，可以使用 `flax.linen.Dense(..., dtype=jax.numpy.bfloat16)` 实例化 `Dense` 层。以下是一些代码示例：

+   在 [Flax LM1B example](https://github.com/google/flax/tree/main/examples/lm1b) 中，`Dense` 模块也可以使用可配置的数据类型 [进行实例化](https://github.com/google/flax/blob/fd8fd76a4af5307a61f85bac98feab9b26d60db8/examples/lm1b/models.py#L188)，其 [默认值](https://github.com/google/flax/blob/fd8fd76a4af5307a61f85bac98feab9b26d60db8/examples/lm1b/configs/default.py#L112) 为 [bfloat16](https://github.com/google/flax/blob/c0087535d7f2e5bfcbf2a7be6825b9f5055a54c6/examples/lm1b/train.py#L431)。

+   在 [MaxText](https://github.com/google/maxtext) 中，`DenseGeneral` 模块也可用可配置的数据类型 [进行实例化](https://github.com/google/maxtext/blob/07dc6ce27ced1246407d0de311d4a0d6a9fd46d8/MaxText/layers.py#L592)，其 [默认值为 bfloat16](https://github.com/google/maxtext/blob/07dc6ce27ced1246407d0de311d4a0d6a9fd46d8/MaxText/configs/base.yml#L41)。

## XLA 性能标志

注意

JAX-Toolbox 还有一个关于 [NVIDIA XLA 性能 FLAGS](https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/docs/GPU_performance.md) 的页面。

XLA 标志的存在和确切行为可能取决于 `jaxlib` 版本。

截至 `jaxlib==0.4.18`（发布于 [2023 年 10 月 6 日](https://pypi.org/project/jaxlib/#history)），设置这些 XLA 标志可以提高性能。其中一些与多 GPU 之间的通信相关，因此仅在多设备运行计算时才相关，而其他一些与每个设备上的代码生成相关。

未来版本中可能会默认设置其中一些。

这些标志可以通过 `XLA_FLAGS` shell 环境变量进行设置。例如，我们可以将其添加到 Python 文件的顶部：

```py
import os
os.environ['XLA_FLAGS'] = (
    '--xla_gpu_enable_triton_softmax_fusion=true '
    '--xla_gpu_triton_gemm_any=True '
    '--xla_gpu_enable_async_collectives=true '
    '--xla_gpu_enable_latency_hiding_scheduler=true '
    '--xla_gpu_enable_highest_priority_async_stream=true '
) 
```

更多示例，请参阅 [XLA Flags recommended for Pax training on Nvidia GPUs](https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/rosetta/projects/pax/README.md#xla-flags)。

### 代码生成标志

+   **–xla_gpu_enable_triton_softmax_fusion** 此标志启用基于 Triton 代码生成支持的模式匹配自动 softmax 融合。默认值为 False。

+   **–xla_gpu_triton_gemm_any** 使用基于 Triton 的 GEMM（矩阵乘法）发射器支持的任何 GEMM。默认值为 False。

### 通信标志

+   **–xla_gpu_enable_async_collectives** 此标志启用诸如`AllReduce`、`AllGather`、`ReduceScatter`和`CollectivePermute`等集体操作以异步方式进行。异步通信可以将跨核心通信与计算重叠。默认值为 False。

+   **–xla_gpu_enable_latency_hiding_scheduler** 这个标志启用了延迟隐藏调度器，可以高效地将异步通信与计算重叠。默认值为 False。

+   **–xla_gpu_enable_pipelined_collectives** 在使用管道并行时，此标志允许将(i+1)层权重的`AllGather`与第 i 层的计算重叠。它还允许将(i+1)层权重的`Reduce`/`ReduceScatter`与第 i 层的计算重叠。默认值为 False。**在启用此标志时存在一些错误。**

+   **–xla_gpu_collective_permute_decomposer_threshold** 当执行[GSPMD pipelining](https://arxiv.org/abs/2105.04663)时，这个标志非常有用。设置一个非零的阈值会将`CollectivePermute`分解为`CollectivePermuteReceiveDone`和`CollectivePermuteSendDone`对，从而可以在每个对应的`ReceiveDone`/`SendDone`对之间执行计算，从而实现更多的重叠。默认阈值为 0，不进行分解。将其设置为大于 0 的阈值，例如`--xla_gpu_collective_permute_decomposer_threshold=1024`，可以启用此功能。

+   **–xla_gpu_all_gather_combine_threshold_bytes** **–xla_gpu_reduce_scatter_combine_threshold_bytes** **–xla_gpu_all_reduce_combine_threshold_bytes** 这些标志用于调整何时将多个小的`AllGather`/`ReduceScatter`/`AllReduce`组合成一个大的`AllGather`/`ReduceScatter`/`AllReduce`，以减少跨设备通信所花费的时间。例如，在基于 Transformer 的工作负载上，可以考虑将`AllGather`/`ReduceScatter`阈值调高，以至少组合一个 Transformer 层的权重`AllGather`/`ReduceScatter`。默认情况下，`combine_threshold_bytes`设置为 256。

## NCCL 标志

这些 Nvidia NCCL 标志值可能对在 Nvidia GPU 上进行单主机多设备计算有用：

```py
os.environ.update({
  "NCCL_LL128_BUFFSIZE": "-2",
  "NCCL_LL_BUFFSIZE": "-2",
   "NCCL_PROTO": "SIMPLE,LL,LL128",
 }) 
```

这些 NCCL 标志可以提高单主机通信速度。然而，这些标志对多主机通信似乎不太有用。

## 多进程

我们建议每个 GPU 使用一个进程，而不是每个节点使用一个进程。在某些情况下，这可以加速 jitted 计算。当在 SLURM 下运行时，`jax.distributed.initialize()` API 将自动理解此配置。然而，这只是一个经验法则，可能有必要在您的用例中测试每个 GPU 一个进程和每个节点一个进程的情况。
