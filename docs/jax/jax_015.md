# 有状态计算

> 原文：[`jax.readthedocs.io/en/latest/stateful-computations.html`](https://jax.readthedocs.io/en/latest/stateful-computations.html)

JAX 的转换（如`jit()`、`vmap()`、`grad()`）要求它们包装的函数是纯粹的：即，函数的输出仅依赖于输入，并且没有副作用，比如更新全局状态。您可以在[JAX sharp bits: Pure functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions)中找到关于这一点的讨论。

在机器学习的背景下，这种约束可能会带来一些挑战，因为状态可以以多种形式存在。例如：

+   模型参数，

+   优化器状态，以及

+   像[BatchNorm](https://zh.wikipedia.org/wiki/%E6%89%B9%E9%87%8F%E6%A0%87%E5%87%86)这样的有状态层。

本节提供了如何在 JAX 程序中正确处理状态的一些建议。

## 一个简单的例子：计数器

让我们首先看一个简单的有状态程序：一个计数器。

```py
import jax
import jax.numpy as jnp

class Counter:
  """A simple counter."""

  def __init__(self):
    self.n = 0

  def count(self) -> int:
  """Increments the counter and returns the new value."""
    self.n += 1
    return self.n

  def reset(self):
  """Resets the counter to zero."""
    self.n = 0

counter = Counter()

for _ in range(3):
  print(counter.count()) 
```

```py
1
2
3 
```

计数器的`n`属性在连续调用`count`时维护计数器的*状态*。调用`count`的副作用是修改它。

假设我们想要快速计数，所以我们即时编译`count`方法。（在这个例子中，这实际上不会以任何方式加快速度，由于很多原因，但把它看作是模型参数更新的玩具模型，`jit()`确实产生了巨大的影响）。

```py
counter.reset()
fast_count = jax.jit(counter.count)

for _ in range(3):
  print(fast_count()) 
```

```py
1
1
1 
```

哦不！我们的计数器不能工作了。这是因为

```py
self.n += 1 
```

在`count`中涉及副作用：它直接修改了输入的计数器，因此此函数不受`jit`支持。这样的副作用仅在首次跟踪函数时执行一次，后续调用将不会重复该副作用。那么，我们该如何修复它呢？

## 解决方案：显式状态

问题的一部分在于我们的计数器返回值不依赖于参数，这意味着编译输出中包含了一个常数。但它不应该是一个常数 - 它应该依赖于状态。那么，为什么我们不将状态作为一个参数呢？

```py
CounterState = int

class CounterV2:

  def count(self, n: CounterState) -> tuple[int, CounterState]:
    # You could just return n+1, but here we separate its role as 
    # the output and as the counter state for didactic purposes.
    return n+1, n+1

  def reset(self) -> CounterState:
    return 0

counter = CounterV2()
state = counter.reset()

for _ in range(3):
  value, state = counter.count(state)
  print(value) 
```

```py
1
2
3 
```

在这个`Counter`的新版本中，我们将`n`移动到`count`的参数中，并添加了另一个返回值，表示新的、更新的状态。现在，为了使用这个计数器，我们需要显式地跟踪状态。但作为回报，我们现在可以安全地使用`jax.jit`这个计数器：

```py
state = counter.reset()
fast_count = jax.jit(counter.count)

for _ in range(3):
  value, state = fast_count(state)
  print(value) 
```

```py
1
2
3 
```

## 一个一般的策略

我们可以将同样的过程应用到任何有状态方法中，将其转换为无状态方法。我们拿一个形式如下的类

```py
class StatefulClass

  state: State

  def stateful_method(*args, **kwargs) -> Output: 
```

并将其转换为以下形式的类

```py
class StatelessClass

  def stateless_method(state: State, *args, **kwargs) -> (Output, State): 
```

这是一个常见的[函数式编程](https://zh.wikipedia.org/wiki/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B)模式，本质上就是处理所有 JAX 程序中状态的方式。

注意，一旦我们按照这种方式重写它，类的必要性就不那么明显了。我们可以只保留`stateless_method`，因为类不再执行任何工作。这是因为，像我们刚刚应用的策略一样，面向对象编程（OOP）是帮助程序员理解程序状态的一种方式。

在我们的情况下，`CounterV2` 类只是一个名称空间，将所有使用 `CounterState` 的函数集中在一个位置。读者可以思考：将其保留为类是否有意义？

顺便说一句，你已经在 JAX 伪随机性 API 中看到了这种策略的示例，即 `jax.random`，在 :ref:`pseudorandom-numbers` 部分展示。与 Numpy 不同，后者使用隐式更新的有状态类管理随机状态，而 JAX 要求程序员直接使用随机生成器状态——PRNG 密钥。

## 简单的工作示例：线性回归

现在让我们将这种策略应用到一个简单的机器学习模型上：通过梯度下降进行线性回归。

这里，我们只处理一种状态：模型参数。但通常情况下，你会看到许多种状态在 JAX 函数中交替出现，比如优化器状态、批归一化的层统计数据等。

需要仔细查看的函数是 `update`。

```py
from typing import NamedTuple

class Params(NamedTuple):
  weight: jnp.ndarray
  bias: jnp.ndarray

def init(rng) -> Params:
  """Returns the initial model params."""
  weights_key, bias_key = jax.random.split(rng)
  weight = jax.random.normal(weights_key, ())
  bias = jax.random.normal(bias_key, ())
  return Params(weight, bias)

def loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:
  """Computes the least squares error of the model's predictions on x against y."""
  pred = params.weight * x + params.bias
  return jnp.mean((pred - y) ** 2)

LEARNING_RATE = 0.005

@jax.jit
def update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:
  """Performs one SGD update step on params using the given data."""
  grad = jax.grad(loss)(params, x, y)

  # If we were using Adam or another stateful optimizer,
  # we would also do something like
  #
  #   updates, new_optimizer_state = optimizer(grad, optimizer_state)
  # 
  # and then use `updates` instead of `grad` to actually update the params.
  # (And we'd include `new_optimizer_state` in the output, naturally.)

  new_params = jax.tree_map(
      lambda param, g: param - g * LEARNING_RATE, params, grad)

  return new_params 
```

注意，我们手动地将参数输入和输出到更新函数中。

```py
import matplotlib.pyplot as plt

rng = jax.random.key(42)

# Generate true data from y = w*x + b + noise
true_w, true_b = 2, -1
x_rng, noise_rng = jax.random.split(rng)
xs = jax.random.normal(x_rng, (128, 1))
noise = jax.random.normal(noise_rng, (128, 1)) * 0.5
ys = xs * true_w + true_b + noise

# Fit regression
params = init(rng)
for _ in range(1000):
  params = update(params, xs, ys)

plt.scatter(xs, ys)
plt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')
plt.legend(); 
```

```py
/tmp/ipykernel_2992/721844192.py:37: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).
  new_params = jax.tree_map( 
```

![_images/9d9c2471be1e4c9b8597cfff1433de0fe7ad2ef5b99cc6897ee153d7533d6521.png](img/5b4aeccf88cb97ddf4bb473ef4ec2d12.png)

## 进一步探讨

上述描述的策略是任何使用 `jit`、`vmap`、`grad` 等转换的 JAX 程序必须处理状态的方式。

如果只涉及两个参数，手动处理参数似乎还可以接受，但如果是有数十层的神经网络呢？你可能已经开始担心两件事情：

1.  我们是否应该手动初始化它们，基本上是在前向传播定义中已经编写过的内容？

1.  我们是否应该手动处理所有这些事情？

处理这些细节可能有些棘手，但有一些库的示例可以为您解决这些问题。请参阅[JAX 神经网络库](https://github.com/google/jax#neural-network-libraries)获取一些示例。
