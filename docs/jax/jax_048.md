# 在多主机和多进程环境中使用 JAX

> 原文：[`jax.readthedocs.io/en/latest/multi_process.html`](https://jax.readthedocs.io/en/latest/multi_process.html)

## 介绍

本指南解释了如何在 GPU 集群和[Cloud TPU](https://cloud.google.com/tpu) pod 等环境中使用 JAX，在这些环境中，加速器分布在多个 CPU 主机或 JAX 进程上。我们将这些称为“多进程”环境。

本指南专门介绍了如何在多进程设置中使用集体通信操作（例如 `jax.lax.psum()` ），尽管根据您的用例，其他通信方法也可能有用（例如 RPC，[mpi4jax](https://github.com/mpi4jax/mpi4jax)）。如果您尚未熟悉 JAX 的集体操作，建议从分片计算部分开始。在 JAX 的多进程环境中，重要的要求是加速器之间的直接通信链路，例如 Cloud TPU 的高速互连或[NCCL](https://developer.nvidia.com/nccl) 用于 GPU。这些链路允许集体操作在多个进程的加速器上高性能运行。

## 多进程编程模型

关键概念：

+   您必须在每个主机上至少运行一个 JAX 进程。

+   您应该使用 `jax.distributed.initialize()` 初始化集群。

+   每个进程都有一组独特的*本地*设备可以访问。*全局*设备是所有进程的所有设备集合。

+   使用标准的 JAX 并行 API，如 `jit()`（参见分片计算入门教程）和 `shard_map()`。jax.jit 仅接受全局形状的数组。shard_map 允许您按设备形状进行降级。

+   确保所有进程按照相同顺序运行相同的并行计算。

+   确保所有进程具有相同数量的本地设备。

+   确保所有设备相同（例如，全部为 V100 或全部为 H100）。

### 启动 JAX 进程

与其他分布式系统不同，其中单个控制节点管理多个工作节点，JAX 使用“多控制器”编程模型，其中每个 JAX Python 进程独立运行，有时称为单程序多数据（SPMD）模型。通常，在每个进程中运行相同的 JAX Python 程序，每个进程的执行之间只有轻微差异（例如，不同的进程将加载不同的输入数据）。此外，**您必须手动在每个主机上运行您的 JAX 程序！** JAX 不会从单个程序调用自动启动多个进程。

（对于多个进程的要求，这就是为什么本指南不作为笔记本提供的原因——我们目前没有好的方法来从单个笔记本管理多个 Python 进程。）

### 初始化集群

要初始化集群，您应该在每个进程的开始调用 `jax.distributed.initialize()`。`jax.distributed.initialize()` 必须在程序中的任何 JAX 计算执行之前早些时候调用。

API `jax.distributed.initialize()` 接受几个参数，即:

+   `coordinator_address`：集群中进程 0 的 IP 地址，以及该进程上可用的一个端口。进程 0 将启动一个通过该 IP 地址和端口暴露的 JAX 服务，集群中的其他进程将连接到该服务。

+   `coordinator_bind_address`：集群中进程 0 上的 JAX 服务将绑定到的 IP 地址和端口。默认情况下，它将使用与 `coordinator_address` 相同端口的所有可用接口进行绑定。

+   `num_processes`：集群中的进程数

+   `process_id`：本进程的 ID 号码，范围为`[0 .. num_processes)`。

+   `local_device_ids`：将当前进程的可见设备限制为 `local_device_ids`。

例如，在 GPU 上，典型用法如下：

```py
import jax

jax.distributed.initialize(coordinator_address="192.168.0.1:1234",
                           num_processes=2,
                           process_id=0) 
```

在 Cloud TPU、Slurm 和 Open MPI 环境中，你可以简单地调用 `jax.distributed.initialize()` 而无需参数。参数的默认值将自动选择。在使用 Slurm 和 Open MPI 运行 GPU 时，假定每个 GPU 启动一个进程，即每个进程只分配一个可见本地设备。否则假定每个主机启动一个进程，即每个进程将分配所有本地设备。只有当通过 `mpirun`/`mpiexec` 启动 JAX 进程时才会使用 Open MPI 自动初始化。

```py
import jax

jax.distributed.initialize() 
```

在当前 TPU 上，调用 `jax.distributed.initialize()` 目前是可选的，但建议使用，因为它启用了额外的检查点和健康检查功能。

### 本地与全局设备

在开始从您的程序中运行多进程计算之前，了解*本地*和*全局*设备之间的区别是很重要的。

**进程的*本地*设备是它可以直接寻址和启动计算的设备。** 例如，在 GPU 集群上，每个主机只能在直接连接的 GPU 上启动计算。在 Cloud TPU pod 上，每个主机只能在直接连接到该主机的 8 个 TPU 核心上启动计算（有关更多详情，请参阅[Cloud TPU 系统架构](https://cloud.google.com/tpu/docs/system-architecture)文档）。你可以通过 `jax.local_devices()` 查看进程的本地设备。

**全局设备是跨所有进程的设备。** 一个计算可以跨进程的设备并通过设备之间的直接通信链路执行集体操作，只要每个进程在其本地设备上启动计算即可。你可以通过 `jax.devices()` 查看所有可用的全局设备。一个进程的本地设备总是全局设备的一个子集。

### 运行多进程计算

那么，你到底如何运行涉及跨进程通信的计算呢？ **使用与单进程中相同的并行评估 API！**

例如，`shard_map()` 可以用于在多个进程间并行计算。（如果您还不熟悉如何使用 `shard_map` 在单个进程内的多个设备上运行，请参阅分片计算介绍教程。）从概念上讲，这可以被视为在跨主机分片的单个数组上运行 pmap，其中每个主机只“看到”其本地分片的输入和输出。

下面是多进程 pmap 的实际示例：

```py
# The following is run in parallel on each host on a GPU cluster or TPU pod slice.
>>> import jax
>>> jax.distributed.initialize()  # On GPU, see above for the necessary arguments.
>>> jax.device_count()  # total number of accelerator devices in the cluster
32
>>> jax.local_device_count()  # number of accelerator devices attached to this host
8
# The psum is performed over all mapped devices across the pod slice
>>> xs = jax.numpy.ones(jax.local_device_count())
>>> jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)
ShardedDeviceArray([32., 32., 32., 32., 32., 32., 32., 32.], dtype=float32) 
```

**非常重要的是，所有进程以相同的跨进程计算顺序运行。** 在每个进程中运行相同的 JAX Python 程序通常就足够了。尽管运行相同程序，但仍需注意可能导致不同顺序计算的一些常见陷阱：

+   将不同形状的输入传递给同一并行函数的进程可能导致挂起或不正确的返回值。只要它们在进程间产生相同形状的每设备数据分片，不同形状的输入是安全的；例如，传递不同的前导批次大小以在不同的本地设备数上运行是可以的，但是每个进程根据不同的最大示例长度填充其批次是不行的。

+   “最后一批”问题发生在并行函数在（训练）循环中调用时，其中一个或多个进程比其余进程更早退出循环。这将导致其余进程挂起，等待已经完成的进程开始计算。

+   基于集合的非确定性顺序的条件可能导致代码进程挂起。例如，在当前 Python 版本上遍历 `set` 或者 Python 3.7 之前的 `dict` [可能会导致不同进程的顺序不同](https://mail.python.org/pipermail/python-dev/2017-December/151283.html)，即使插入顺序相同也是如此。
