# 分布式数组和自动并行化

> 原文：[`jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html`](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)

![在 Colab 中打开](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.ipynb) ![在 Kaggle 中打开](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.ipynb)

本教程讨论了通过 `jax.Array` 实现的并行计算，这是 JAX v0.4.1 及更高版本中可用的统一数组对象模型。

```py
import os

import functools
from typing import Optional

import numpy as np

import jax
import jax.numpy as jnp 
```

⚠️ 警告：此笔记本需要 8 个设备才能运行。

```py
if len(jax.local_devices()) < 8:
  raise Exception("Notebook requires 8 devices to run") 
```

## 简介和一个快速示例

通过阅读这本教程笔记本，您将了解 `jax.Array`，一种用于表示数组的统一数据类型，即使物理存储跨越多个设备。您还将学习如何使用 `jax.Array` 与 `jax.jit` 结合，实现基于编译器的自动并行化。

在我们逐步思考之前，这里有一个快速示例。首先，我们将创建一个跨多个设备分片的 `jax.Array`：

```py
from jax.experimental import mesh_utils
from jax.sharding import PositionalSharding 
```

```py
# Create a Sharding object to distribute a value across devices:
sharding = PositionalSharding(mesh_utils.create_device_mesh((8,))) 
```

```py
# Create an array of random values:
x = jax.random.normal(jax.random.key(0), (8192, 8192))
# and use jax.device_put to distribute it across devices:
y = jax.device_put(x, sharding.reshape(4, 2))
jax.debug.visualize_array_sharding(y) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

接下来，我们将对其应用计算，并可视化结果值如何存储在多个设备上：

```py
z = jnp.sin(y)
jax.debug.visualize_array_sharding(z) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

`jnp.sin` 应用的评估已自动并行化，该应用跨存储输入值（和输出值）的设备：

```py
# `x` is present on a single device
%timeit -n 5 -r 5 jnp.sin(x).block_until_ready() 
```

```py
The slowest run took 13.32 times longer than the fastest. This could mean that an intermediate result is being cached 
5 loops, best of 5: 9.69 ms per loop 
```

```py
# `y` is sharded across 8 devices.
%timeit -n 5 -r 5 jnp.sin(y).block_until_ready() 
```

```py
5 loops, best of 5: 1.86 ms per loop 
```

现在让我们更详细地查看每个部分！

## `Sharding` 描述了如何将数组值布局在跨设备的内存中。

### Sharding 基础知识和 `PositionalSharding` 子类

要在多个设备上并行计算，我们首先必须在多个设备上布置输入数据。

在 JAX 中，`Sharding` 对象描述了分布式内存布局。它们可以与 `jax.device_put` 结合使用，生成具有分布式布局的值。

例如，这里是一个单设备 `Sharding` 的值：

```py
import jax
x = jax.random.normal(jax.random.key(0), (8192, 8192)) 
```

```py
jax.debug.visualize_array_sharding(x) 
```

```py
┌───────────────────────┐
│                       │
│                       │
│                       │
│                       │
│         TPU 0         │
│                       │
│                       │
│                       │
│                       │
└───────────────────────┘ 
```

在这里，我们使用 `jax.debug.visualize_array_sharding` 函数来展示内存中存储值 `x` 的位置。整个 `x` 存储在单个设备上，所以可视化效果相当无聊！

但是我们可以通过使用 `jax.device_put` 和 `Sharding` 对象将 `x` 分布在多个设备上。首先，我们使用 `mesh_utils.create_device_mesh` 制作一个 `Devices` 的 `numpy.ndarray`，该函数考虑了硬件拓扑以确定 `Device` 的顺序：

```py
from jax.experimental import mesh_utils
devices = mesh_utils.create_device_mesh((8,)) 
```

然后，我们创建一个 `PositionalSharding` 并与 `device_put` 一起使用：

```py
from jax.sharding import PositionalSharding

sharding = PositionalSharding(devices)

x = jax.device_put(x, sharding.reshape(8, 1))
jax.debug.visualize_array_sharding(x) 
```

```py
┌───────────────────────┐
│         TPU 0         │
├───────────────────────┤
│         TPU 1         │
├───────────────────────┤
│         TPU 2         │
├───────────────────────┤
│         TPU 3         │
├───────────────────────┤
│         TPU 6         │
├───────────────────────┤
│         TPU 7         │
├───────────────────────┤
│         TPU 4         │
├───────────────────────┤
│         TPU 5         │
└───────────────────────┘ 
```

这里的 `sharding` 是一个 `PositionalSharding`，它的作用类似于一个具有设备集合作为元素的数组：

```py
sharding 
```

```py
PositionalSharding([{TPU 0} {TPU 1} {TPU 2} {TPU 3} {TPU 6} {TPU 7} {TPU 4} {TPU 5}]) 
```

这里的设备编号不是按数字顺序排列的，因为网格反映了设备的基础环形拓扑结构。

通过编写 `PositionalSharding(ndarray_of_devices)`，我们确定了设备顺序和初始形状。然后我们可以对其进行重新形状化：

```py
sharding.reshape(8, 1) 
```

```py
PositionalSharding([[{TPU 0}]
                    [{TPU 1}]
                    [{TPU 2}]
                    [{TPU 3}]
                    [{TPU 6}]
                    [{TPU 7}]
                    [{TPU 4}]
                    [{TPU 5}]]) 
```

```py
sharding.reshape(4, 2) 
```

```py
PositionalSharding([[{TPU 0} {TPU 1}]
                    [{TPU 2} {TPU 3}]
                    [{TPU 6} {TPU 7}]
                    [{TPU 4} {TPU 5}]]) 
```

要使用`device_put`与数据数组`x`，我们可以将`sharding`重新形状为与`x.shape`*同余*的形状，这意味着具有与`x.shape`相同长度的形状，并且其中每个元素均匀地分割对应`x.shape`的元素：

```py
def is_congruent(x_shape: Sequence[int], sharding_shape: Sequence[int]) -> bool:
  return (len(x_shape) == len(sharding_shape) and
          all(d1 % d2 == 0 for d1, d2 in zip(x_shape, sharding_shape))) 
```

例如，我们可以将`sharding`重新形状为`(4, 2)`，然后在`device_put`中使用它：

```py
sharding = sharding.reshape(4, 2)
print(sharding) 
```

```py
PositionalSharding([[{TPU 0} {TPU 1}]
                    [{TPU 2} {TPU 3}]
                    [{TPU 6} {TPU 7}]
                    [{TPU 4} {TPU 5}]]) 
```

```py
y = jax.device_put(x, sharding)
jax.debug.visualize_array_sharding(y) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

这里的`y`代表与`x`相同的*值*，但其片段（即切片）存储在不同设备的内存中。

不同的`PositionalSharding`形状会导致结果的不同分布布局（即分片）：

```py
sharding = sharding.reshape(1, 8)
print(sharding) 
```

```py
PositionalSharding([[{TPU 0} {TPU 1} {TPU 2} {TPU 3} {TPU 6} {TPU 7} {TPU 4} {TPU 5}]]) 
```

```py
y = jax.device_put(x, sharding)
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│       │       │       │       │       │       │       │       │
│       │       │       │       │       │       │       │       │
│       │       │       │       │       │       │       │       │
│       │       │       │       │       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │
│       │       │       │       │       │       │       │       │
│       │       │       │       │       │       │       │       │
│       │       │       │       │       │       │       │       │
│       │       │       │       │       │       │       │       │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ 
```

在某些情况下，我们不只是想将`x`的每个切片存储在单个设备的内存中；我们可能希望在多个设备的内存中*复制*一些切片，即在多个设备的内存中存储切片的值。

使用`PositionalSharding`，我们可以通过调用 reducer 方法`replicate`来表达复制：

```py
sharding = sharding.reshape(4, 2)
print(sharding.replicate(axis=0, keepdims=True)) 
```

```py
PositionalSharding([[{TPU 0, 2, 4, 6} {TPU 1, 3, 5, 7}]]) 
```

```py
y = jax.device_put(x, sharding.replicate(axis=0, keepdims=True))
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────────┬───────────┐
│           │           │
│           │           │
│           │           │
│           │           │
│TPU 0,2,4,6│TPU 1,3,5,7│
│           │           │
│           │           │
│           │           │
│           │           │
└───────────┴───────────┘ 
```

这里的可视化显示了`x`沿其第二维以两种方式分片（而不沿第一维分片），每个片段都复制了四种方式（即存储在四个设备内存中）。

`replicate`方法类似于熟悉的 NumPy 数组缩减方法，如`.sum()`和`.prod()`。它沿着一个轴执行集合并操作。因此，如果`sharding`的形状为`(4, 2)`，那么`sharding.replicate(0, keepdims=True)`的形状为`(1, 2)`，`sharding.replicate(1, keepdims=True)`的形状为`(4, 1)`。与 NumPy 方法不同，`keepdims=True`实际上是默认的，因此减少的轴不会被压缩：

```py
print(sharding.replicate(0).shape)
print(sharding.replicate(1).shape) 
```

```py
(1, 2)
(4, 1) 
```

```py
y = jax.device_put(x, sharding.replicate(1))
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────────────────────┐
│        TPU 0,1        │
├───────────────────────┤
│        TPU 2,3        │
├───────────────────────┤
│        TPU 6,7        │
├───────────────────────┤
│        TPU 4,5        │
└───────────────────────┘ 
```

### `NamedSharding`提供了一种使用名称表达分片的方式。

到目前为止，我们已经使用了`PositionalSharding`，但还有其他表达分片的替代方法。实际上，`Sharding`是一个接口，任何实现该接口的类都可以与`device_put`等函数一起使用。

另一种方便的表达分片的方法是使用`NamedSharding`：

```py
from jax.sharding import Mesh
from jax.sharding import PartitionSpec
from jax.sharding import NamedSharding
from jax.experimental import mesh_utils

P = PartitionSpec

devices = mesh_utils.create_device_mesh((4, 2))
mesh = Mesh(devices, axis_names=('a', 'b'))
y = jax.device_put(x, NamedSharding(mesh, P('a', 'b')))
jax.debug.visualize_array_sharding(y) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

我们可以定义一个辅助函数使事情更简单：

```py
devices = mesh_utils.create_device_mesh((4, 2))
default_mesh = Mesh(devices, axis_names=('a', 'b'))

def mesh_sharding(
    pspec: PartitionSpec, mesh: Optional[Mesh] = None,
  ) -> NamedSharding:
  if mesh is None:
    mesh = default_mesh
  return NamedSharding(mesh, pspec) 
```

```py
y = jax.device_put(x, mesh_sharding(P('a', 'b')))
jax.debug.visualize_array_sharding(y) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

在这里，我们使用`P('a', 'b')`来表达`x`的第一和第二轴应该分片到设备网格轴`'a'`和`'b'`上。我们可以轻松切换到`P('b', 'a')`以在不同设备上分片`x`的轴：

```py
y = jax.device_put(x, mesh_sharding(P('b', 'a')))
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 2 │ TPU 6 │ TPU 4 │
│       │       │       │       │
│       │       │       │       │
├───────┼───────┼───────┼───────┤
│       │       │       │       │
│ TPU 1 │ TPU 3 │ TPU 7 │ TPU 5 │
│       │       │       │       │
│       │       │       │       │
└───────┴───────┴───────┴───────┘ 
```

```py
# This `None` means that `x` is not sharded on its second dimension,
# and since the Mesh axis name 'b' is not mentioned, shards are
# replicated across it.
y = jax.device_put(x, mesh_sharding(P('a', None)))
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────────────────────┐
│        TPU 0,1        │
├───────────────────────┤
│        TPU 2,3        │
├───────────────────────┤
│        TPU 6,7        │
├───────────────────────┤
│        TPU 4,5        │
└───────────────────────┘ 
```

这里，因为`P('a', None)`没有提及`Mesh`轴名`'b'`，我们在轴`'b'`上得到了复制。这里的`None`只是一个占位符，用于与值`x`的第二轴对齐，而不表示在任何网格轴上进行分片。（简写方式是，尾部的`None`可以省略，因此`P('a', None)`的意思与`P('a')`相同。但是明确说明并不会有害！）

要仅在`x`的第二轴上进行分片，我们可以在`PartitionSpec`中使用`None`占位符。

```py
y = jax.device_put(x, mesh_sharding(P(None, 'b')))
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────────┬───────────┐
│           │           │
│           │           │
│           │           │
│           │           │
│TPU 0,2,4,6│TPU 1,3,5,7│
│           │           │
│           │           │
│           │           │
│           │           │
└───────────┴───────────┘ 
```

```py
y = jax.device_put(x, mesh_sharding(P(None, 'a')))
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│       │       │       │       │
│       │       │       │       │
│       │       │       │       │
│TPU 0,1│TPU 2,3│TPU 6,7│TPU 4,5│
│       │       │       │       │
│       │       │       │       │
│       │       │       │       │
│       │       │       │       │
└───────┴───────┴───────┴───────┘ 
```

对于固定的网格，我们甚至可以将`x`的一个逻辑轴分割到多个设备网格轴上：

```py
y = jax.device_put(x, mesh_sharding(P(('a', 'b'), None)))
jax.debug.visualize_array_sharding(y) 
```

```py
┌───────────────────────┐
│         TPU 0         │
├───────────────────────┤
│         TPU 1         │
├───────────────────────┤
│         TPU 2         │
├───────────────────────┤
│         TPU 3         │
├───────────────────────┤
│         TPU 6         │
├───────────────────────┤
│         TPU 7         │
├───────────────────────┤
│         TPU 4         │
├───────────────────────┤
│         TPU 5         │
└───────────────────────┘ 
```

使用`NamedSharding`可以轻松定义一次设备网格并为其轴命名，然后只需在需要时在每个`device_put`的`PartitionSpec`中引用这些名称。

## 计算遵循数据分片并自动并行化

使用分片输入数据，编译器可以给我们并行计算。特别是，用 `jax.jit` 装饰的函数可以在分片数组上操作，而无需将数据复制到单个设备上。相反，计算遵循分片：基于输入数据的分片，编译器决定中间结果和输出值的分片，并并行评估它们，必要时甚至插入通信操作。

例如，最简单的计算是逐元素的：

```py
from jax.experimental import mesh_utils
from jax.sharding import PositionalSharding
sharding = PositionalSharding(mesh_utils.create_device_mesh((8,))) 
```

```py
x = jax.device_put(x, sharding.reshape(4, 2))
print('input sharding:')
jax.debug.visualize_array_sharding(x)

y = jnp.sin(x)
print('output sharding:')
jax.debug.visualize_array_sharding(y) 
```

```py
input sharding:
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘
output sharding:
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

这里对于逐元素操作 `jnp.sin`，编译器选择了输出分片与输入相同。此外，编译器自动并行化计算，因此每个设备都可以并行计算其输出片段。

换句话说，即使我们将 `jnp.sin` 的计算写成单台机器执行，编译器也会为我们拆分计算并在多个设备上执行。

我们不仅可以对逐元素操作执行相同操作。考虑使用分片输入的矩阵乘法：

```py
y = jax.device_put(x, sharding.reshape(4, 2).replicate(1))
z = jax.device_put(x, sharding.reshape(4, 2).replicate(0))
print('lhs sharding:')
jax.debug.visualize_array_sharding(y)
print('rhs sharding:')
jax.debug.visualize_array_sharding(z)

w = jnp.dot(y, z)
print('out sharding:')
jax.debug.visualize_array_sharding(w) 
```

```py
lhs sharding:
┌───────────────────────┐
│        TPU 0,1        │
├───────────────────────┤
│        TPU 2,3        │
├───────────────────────┤
│        TPU 6,7        │
├───────────────────────┤
│        TPU 4,5        │
└───────────────────────┘
rhs sharding:
┌───────────┬───────────┐
│           │           │
│           │           │
│           │           │
│           │           │
│TPU 0,2,4,6│TPU 1,3,5,7│
│           │           │
│           │           │
│           │           │
│           │           │
└───────────┴───────────┘
out sharding:
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

这里编译器选择了输出分片，以便最大化并行计算：无需通信，每个设备已经具有计算其输出分片所需的输入分片。

我们如何确保它实际上是并行运行的？我们可以进行简单的时间实验：

```py
x_single = jax.device_put(x, jax.devices()[0])
jax.debug.visualize_array_sharding(x_single) 
```

```py
┌───────────────────────┐
│                       │
│                       │
│                       │
│                       │
│         TPU 0         │
│                       │
│                       │
│                       │
│                       │
└───────────────────────┘ 
```

```py
np.allclose(jnp.dot(x_single, x_single),
            jnp.dot(y, z)) 
```

```py
True 
```

```py
%timeit -n 5 -r 5 jnp.dot(x_single, x_single).block_until_ready() 
```

```py
5 loops, best of 5: 19.3 ms per loop 
```

```py
%timeit -n 5 -r 5 jnp.dot(y, z).block_until_ready() 
```

```py
5 loops, best of 5: 3.25 ms per loop 
```

即使复制一个分片的 `Array`，也会产生具有输入分片的结果：

```py
w_copy = jnp.copy(w)
jax.debug.visualize_array_sharding(w_copy) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘ 
```

因此，当我们使用 `jax.device_put` 明确分片数据并对该数据应用函数时，编译器会尝试并行化计算并决定输出分片。这种对分片数据的策略是[JAX 遵循显式设备放置策略的泛化](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices)。

### 当明确分片不一致时，JAX 会报错

但是如果计算的两个参数在不同的设备组上明确放置，或者设备顺序不兼容，会发生错误：

```py
import textwrap
from termcolor import colored

def print_exception(e):
  name = colored(f'{type(e).__name__}', 'red')
  print(textwrap.fill(f'{name}: {str(e)}')) 
```

```py
sharding1 = PositionalSharding(jax.devices()[:4])
sharding2 = PositionalSharding(jax.devices()[4:])

y = jax.device_put(x, sharding1.reshape(2, 2))
z = jax.device_put(x, sharding2.reshape(2, 2))
try: y + z
except ValueError as e: print_exception(e) 
```

```py
ValueError: Devices of all `Array` inputs and outputs should
be the same. Got array device ids [0, 1, 2, 3] on platform TPU and
another array's device ids [4, 5, 6, 7] on platform TPU 
```

```py
devices = jax.devices()
permuted_devices = [devices[i] for i in [0, 1, 2, 3, 6, 7, 4, 5]]

sharding1 = PositionalSharding(devices)
sharding2 = PositionalSharding(permuted_devices)

y = jax.device_put(x, sharding1.reshape(4, 2))
z = jax.device_put(x, sharding2.reshape(4, 2))
try: y + z
except ValueError as e: print_exception(e) 
```

```py
ValueError: Devices of all `Array` inputs and outputs should
be the same. Got array device ids [0, 1, 2, 3, 4, 5, 6, 7] on platform
TPU and another array's device ids [0, 1, 2, 3, 6, 7, 4, 5] on
platform TPU 
```

我们说通过 `jax.device_put` 明确放置或分片的数组已经*锁定*在它们的设备上，因此不会自动移动。请查看 [设备放置常见问题解答](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices) 获取更多信息。

当数组没有使用 `jax.device_put` 明确放置或分片时，它们会放置在默认设备上并*未锁定*。与已锁定数组不同，未锁定数组可以自动移动和重新分片：也就是说，未锁定数组可以作为计算的参数，即使其他参数明确放置在不同的设备上。

例如，`jnp.zeros`、`jnp.arange` 和 `jnp.array` 的输出都是未锁定的：

```py
y = jax.device_put(x, sharding1.reshape(4, 2))
y + jnp.ones_like(y)
y + jnp.arange(y.size).reshape(y.shape)
print('no error!') 
```

```py
no error! 
```

## 限制在 `jit` 代码中的中间片段

虽然编译器将尝试决定函数的中间值和输出应如何分片，但我们还可以使用 `jax.lax.with_sharding_constraint` 来给它提供提示。使用 `jax.lax.with_sharding_constraint` 类似于 `jax.device_put`，不同之处在于我们在分阶段函数（即 `jit` 装饰的函数）内部使用它：

```py
sharding = PositionalSharding(mesh_utils.create_device_mesh((8,))) 
```

```py
x = jax.random.normal(jax.random.key(0), (8192, 8192))
x = jax.device_put(x, sharding.reshape(4, 2)) 
```

```py
@jax.jit
def f(x):
  x = x + 1
  y = jax.lax.with_sharding_constraint(x, sharding.reshape(2, 4))
  return y 
```

```py
jax.debug.visualize_array_sharding(x)
y = f(x)
jax.debug.visualize_array_sharding(y) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘
┌───────┬───────┬───────┬───────┐
│       │       │       │       │
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │
│       │       │       │       │
│       │       │       │       │
├───────┼───────┼───────┼───────┤
│       │       │       │       │
│ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │
│       │       │       │       │
│       │       │       │       │
└───────┴───────┴───────┴───────┘ 
```

```py
@jax.jit
def f(x):
  x = x + 1
  y = jax.lax.with_sharding_constraint(x, sharding.replicate())
  return y 
```

```py
jax.debug.visualize_array_sharding(x)
y = f(x)
jax.debug.visualize_array_sharding(y) 
```

```py
┌──────────┬──────────┐
│  TPU 0   │  TPU 1   │
├──────────┼──────────┤
│  TPU 2   │  TPU 3   │
├──────────┼──────────┤
│  TPU 6   │  TPU 7   │
├──────────┼──────────┤
│  TPU 4   │  TPU 5   │
└──────────┴──────────┘
┌───────────────────────┐
│                       │
│                       │
│                       │
│                       │
│  TPU 0,1,2,3,4,5,6,7  │
│                       │
│                       │
│                       │
│                       │
└───────────────────────┘ 
```

通过添加 `with_sharding_constraint`，我们限制了输出的分片。除了尊重特定中间变量的注释外，编译器还会使用注释来决定其他值的分片。

经常的好做法是注释计算的输出，例如根据值最终如何被使用来注释它们。

## 示例：神经网络

**⚠️ 警告：以下内容旨在简单演示使用 `jax.Array` 进行自动分片传播，但可能不反映实际示例的最佳实践。** 例如，实际示例可能需要更多使用 `with_sharding_constraint`。

我们可以利用 `jax.device_put` 和 `jax.jit` 的计算跟随分片特性来并行化神经网络中的计算。以下是基于这种基本神经网络的一些简单示例：

```py
import jax
import jax.numpy as jnp 
```

```py
def predict(params, inputs):
  for W, b in params:
    outputs = jnp.dot(inputs, W) + b
    inputs = jnp.maximum(outputs, 0)
  return outputs

def loss(params, batch):
  inputs, targets = batch
  predictions = predict(params, inputs)
  return jnp.mean(jnp.sum((predictions - targets)**2, axis=-1)) 
```

```py
loss_jit = jax.jit(loss)
gradfun = jax.jit(jax.grad(loss)) 
```

```py
def init_layer(key, n_in, n_out):
    k1, k2 = jax.random.split(key)
    W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)
    b = jax.random.normal(k2, (n_out,))
    return W, b

def init_model(key, layer_sizes, batch_size):
    key, *keys = jax.random.split(key, len(layer_sizes))
    params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))

    key, *keys = jax.random.split(key, 3)
    inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))
    targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))

    return params, (inputs, targets)

layer_sizes = [784, 8192, 8192, 8192, 10]
batch_size = 8192

params, batch = init_model(jax.random.key(0), layer_sizes, batch_size) 
```

### 8 路批数据并行

```py
sharding = PositionalSharding(jax.devices()).reshape(8, 1) 
```

```py
batch = jax.device_put(batch, sharding)
params = jax.device_put(params, sharding.replicate()) 
```

```py
loss_jit(params, batch) 
```

```py
Array(23.469475, dtype=float32) 
```

```py
step_size = 1e-5

for _ in range(30):
  grads = gradfun(params, batch)
  params = [(W - step_size * dW, b - step_size * db)
            for (W, b), (dW, db) in zip(params, grads)]

print(loss_jit(params, batch)) 
```

```py
10.760101 
```

```py
%timeit -n 5 -r 5 gradfun(params, batch)[0][0].block_until_ready() 
```

```py
5 loops, best of 5: 26.3 ms per loop 
```

```py
batch_single = jax.device_put(batch, jax.devices()[0])
params_single = jax.device_put(params, jax.devices()[0]) 
```

```py
%timeit -n 5 -r 5 gradfun(params_single, batch_single)[0][0].block_until_ready() 
```

```py
5 loops, best of 5: 122 ms per loop 
```

### 4 路批数据并行和 2 路模型张量并行

```py
sharding = sharding.reshape(4, 2) 
```

```py
batch = jax.device_put(batch, sharding.replicate(1))
jax.debug.visualize_array_sharding(batch[0])
jax.debug.visualize_array_sharding(batch[1]) 
```

```py
┌───────┐
│TPU 0,1│
├───────┤
│TPU 2,3│
├───────┤
│TPU 4,5│
├───────┤
│TPU 6,7│
└───────┘
┌───────┐
│TPU 0,1│
├───────┤
│TPU 2,3│
├───────┤
│TPU 4,5│
├───────┤
│TPU 6,7│
└───────┘ 
```

```py
(W1, b1), (W2, b2), (W3, b3), (W4, b4) = params

W1 = jax.device_put(W1, sharding.replicate())
b1 = jax.device_put(b1, sharding.replicate())

W2 = jax.device_put(W2, sharding.replicate(0))
b2 = jax.device_put(b2, sharding.replicate(0))

W3 = jax.device_put(W3, sharding.replicate(0).T)
b3 = jax.device_put(b3, sharding.replicate())

W4 = jax.device_put(W4, sharding.replicate())
b4 = jax.device_put(b4, sharding.replicate())

params = (W1, b1), (W2, b2), (W3, b3), (W4, b4) 
```

```py
jax.debug.visualize_array_sharding(W2) 
```

```py
┌───────────┬───────────┐
│           │           │
│           │           │
│           │           │
│           │           │
│TPU 0,2,4,6│TPU 1,3,5,7│
│           │           │
│           │           │
│           │           │
│           │           │
└───────────┴───────────┘ 
```

```py
jax.debug.visualize_array_sharding(W3) 
```

```py
┌───────────────────────┐
│                       │
│      TPU 0,2,4,6      │
│                       │
│                       │
├───────────────────────┤
│                       │
│      TPU 1,3,5,7      │
│                       │
│                       │
└───────────────────────┘ 
```

```py
print(loss_jit(params, batch)) 
```

```py
10.760103 
```

```py
step_size = 1e-5

for _ in range(30):
    grads = gradfun(params, batch)
    params = [(W - step_size * dW, b - step_size * db)
              for (W, b), (dW, db) in zip(params, grads)] 
```

```py
print(loss_jit(params, batch)) 
```

```py
10.752466 
```

```py
(W1, b1), (W2, b2), (W3, b3), (W4, b4) = params
jax.debug.visualize_array_sharding(W2)
jax.debug.visualize_array_sharding(W3) 
```

```py
┌───────────┬───────────┐
│           │           │
│           │           │
│           │           │
│           │           │
│TPU 0,2,4,6│TPU 1,3,5,7│
│           │           │
│           │           │
│           │           │
│           │           │
└───────────┴───────────┘
┌───────────────────────┐
│                       │
│      TPU 0,2,4,6      │
│                       │
│                       │
├───────────────────────┤
│                       │
│      TPU 1,3,5,7      │
│                       │
│                       │
└───────────────────────┘ 
```

```py
%timeit -n 10 -r 10 gradfun(params, batch)[0][0].block_until_ready() 
```

```py
10 loops, best of 10: 30.5 ms per loop 
```

## 锐利的部分

### 生成随机数

JAX 自带一个功能强大且确定性的 [随机数生成器](https://jax.readthedocs.io/en/latest/jep/263-prng.html)。它支持 [`jax.random` 模块](https://jax.readthedocs.io/en/latest/jax.random.html) 中的各种采样函数，如 `jax.random.uniform`。

JAX 的随机数是由基于计数器的 PRNG 生成的，因此原则上，随机数生成应该是对计数器值的纯映射。原则上，纯映射是一个可以轻松分片的操作。它不应需要跨设备通信，也不应需要设备间的冗余计算。

然而，由于历史原因，现有的稳定 RNG 实现并非自动可分片。

考虑以下示例，其中一个函数绘制随机均匀数并将其逐元素添加到输入中：

```py
@jax.jit
def f(key, x):
  numbers = jax.random.uniform(key, x.shape)
  return x + numbers

key = jax.random.key(42)
x_sharding = jax.sharding.PositionalSharding(jax.devices())
x = jax.device_put(jnp.arange(24), x_sharding) 
```

在分区输入上，函数 `f` 生成的输出也是分区的：

```py
jax.debug.visualize_array_sharding(f(key, x)) 
```

```py
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ 
```

但是，如果我们检查 `f` 在这个分区输入上的编译计算，我们会发现它确实涉及一些通信：

```py
f_exe = f.lower(key, x).compile()
print('Communicating?', 'collective-permute' in f_exe.as_text()) 
```

```py
Communicating? True 
```

解决这个问题的一种方法是使用实验性升级标志 `jax_threefry_partitionable` 配置 JAX。启用该标志后，编译计算中的“集体排列”操作现在已经消失：

```py
jax.config.update('jax_threefry_partitionable', True)
f_exe = f.lower(key, x).compile()
print('Communicating?', 'collective-permute' in f_exe.as_text()) 
```

```py
Communicating? False 
```

输出仍然是分区的：

```py
jax.debug.visualize_array_sharding(f(key, x)) 
```

```py
┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐
│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 4 │ TPU 5 │ TPU 6 │ TPU 7 │
└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ 
```

然而，`jax_threefry_partitionable` 选项的一个注意事项是，即使是由相同随机密钥生成的，*使用该标志设置后生成的随机值可能与未设置标志时不同*：

```py
jax.config.update('jax_threefry_partitionable', False)
print('Stable:')
print(f(key, x))
print()

jax.config.update('jax_threefry_partitionable', True)
print('Partitionable:')
print(f(key, x)) 
```

```py
Stable:
[ 0.72503686  1.8532515   2.983416    3.083253    4.0332246   5.4782867
  6.1720605   7.6900277   8.602836    9.810046   10.861367   11.907651
 12.330483   13.456195   14.808557   15.960099   16.067581   17.739723
 18.335474   19.46401    20.390276   21.116539   22.858128   23.223194  ]

Partitionable:
[ 0.48870957  1.6797972   2.6162715   3.561016    4.4506445   5.585866
  6.0748096   7.775133    8.698959    9.818634   10.350306   11.87282
 12.925881   13.86013    14.477554   15.818481   16.711355   17.586697
 18.073738   19.777622   20.404566   21.119123   22.026257   23.63918   ] 
```

在 `jax_threefry_partitionable` 模式下，JAX 的 PRNG 保持确定性，但其实现是新的（并且正在开发中）。为给定密钥生成的随机值在特定的 JAX 版本（或 `main` 分支上的特定提交）中将保持相同，但在不同版本之间可能会有所变化。
